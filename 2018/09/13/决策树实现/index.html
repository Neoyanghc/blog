<!DOCTYPE html>
<html lang="zh">
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
<title>决策树实现 - Davidham&#39;s blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



    <meta name="description" content="最近给本科生当助教，出了一道实现决策树的题，还有一个预剪枝的题，自己也顺便实现一下。">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树实现">
<meta property="og:url" content="https://davidham3.github.io/2018/09/13/决策树实现/index.html">
<meta property="og:site_name" content="Davidham&#39;s blog">
<meta property="og:description" content="最近给本科生当助教，出了一道实现决策树的题，还有一个预剪枝的题，自己也顺便实现一下。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://davidham3.github.io/blog/images/og_image.png">
<meta property="og:updated_time" content="2019-04-25T11:05:13.384Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="决策树实现">
<meta name="twitter:description" content="最近给本科生当助教，出了一道实现决策树的题，还有一个预剪枝的题，自己也顺便实现一下。">
<meta name="twitter:image" content="https://davidham3.github.io/blog/images/og_image.png">







<link rel="icon" href="/blog/images/favicon.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
    
    <style>body>.footer,body>.navbar,body>.section{opacity:0}</style>
    

    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">
    

    
    

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">


    
    
    
    

<link rel="stylesheet" href="/blog/css/back-to-top.css">


    
    

    
    
    
    

    
    
<link rel="stylesheet" href="/blog/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

    
    
    

    
    
    


<link rel="stylesheet" href="/blog/css/style.css">
</head>
<body class="is-3-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/blog/">
            
                Davidham&#39;s blog
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item" href="/blog/">Home</a>
                
                <a class="navbar-item" href="/blog/archives">Archives</a>
                
                <a class="navbar-item" href="/blog/categories">Categories</a>
                
                <a class="navbar-item" href="/blog/tags">Tags</a>
                
                <a class="navbar-item" href="/blog/about">About</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                
                <a class="navbar-item search" title="Search" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-6-widescreen has-order-2 column-main"><div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-09-13T13:40:03.000Z">2018-09-13</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    an hour read (About 13367 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                决策树实现
            
        </h1>
        <div class="content">
            <p>最近给本科生当助教，出了一道实现决策树的题，还有一个预剪枝的题，自己也顺便实现一下。</p>
<a id="more"></a>
<p>我实现的这个决策树主要是参照了C4.5算法。没加入剪枝。实现的其实很简单，只针对离散特征，做了一个二叉决策树。也就是将所有特征先做one-hot，这样所有的特征都变成0和1了，然后对其进行二分。</p>
<p>原理其实很简单，选择一种划分指标，遍历所有的特征，找到最优划分特征，然后分割训练集，从剩余特征中删除当前的最优特征，然后分左子树和右子树递归地继续创建结点即可。无非是递归的终止条件，递归的终止条件有三点：</p>
<ol>
<li>如果当前结点内所有的样本同属一类，则直接做叶子结点</li>
<li>如果当前深度达到最大深度，直接做叶子结点</li>
<li>如果无剩余特征可供划分，直接做叶子节点</li>
</ol>
<h1 id="第三题：实现决策树"><a href="#第三题：实现决策树" class="headerlink" title="第三题：实现决策树"></a>第三题：实现决策树</h1><p>实验内容：<br>使用LendingClub Safe Loans数据集：</p>
<ol>
<li>实现信息增益、信息增益率、基尼指数三种划分标准</li>
<li>使用给定的训练集完成三种决策树的训练过程</li>
<li>计算三种决策树在最大深度为10时在训练集和测试集上的精度，查准率，查全率，F1值</li>
</ol>
<p>在这部分，我们会实现一个很简单的二叉决策树</p>
<h2 id="1-读取数据"><a href="#1-读取数据" class="headerlink" title="1. 读取数据"></a>1. 读取数据</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 导入类库</span></span><br><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> json</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 导入数据</span></span><br><span class="line">loans = pd.read_csv(<span class="hljs-string">'data/lendingclub/lending-club-data.csv'</span>, low_memory=<span class="hljs-keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>数据中有两列是我们想预测的指标，一项是safe_loans，一项是bad_loans，分别表示正例和负例，我们对其进行处理，将正例的safe_loans设为1，负例设为-1，删除bad_loans这列</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 对数据进行预处理，将safe_loans作为标记</span></span><br><span class="line">loans[<span class="hljs-string">'safe_loans'</span>] = loans[<span class="hljs-string">'bad_loans'</span>].apply(<span class="hljs-keyword">lambda</span> x : +<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x==<span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">-1</span>)</span><br><span class="line"><span class="hljs-keyword">del</span> loans[<span class="hljs-string">'bad_loans'</span>]</span><br></pre></td></tr></table></figure>
<p>我们只使用grade, term, home_ownership, emp_length这四列作为特征，safe_loans作为标记，只保留loans中的这五列</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">features = [<span class="hljs-string">'grade'</span>,              <span class="hljs-comment"># grade of the loan</span></span><br><span class="line">            <span class="hljs-string">'term'</span>,               <span class="hljs-comment"># the term of the loan</span></span><br><span class="line">            <span class="hljs-string">'home_ownership'</span>,     <span class="hljs-comment"># home_ownership status: own, mortgage or rent</span></span><br><span class="line">            <span class="hljs-string">'emp_length'</span>,         <span class="hljs-comment"># number of years of employment</span></span><br><span class="line">           ]</span><br><span class="line">target = <span class="hljs-string">'safe_loans'</span></span><br><span class="line">loans = loans[features + [target]]</span><br></pre></td></tr></table></figure>
<h2 id="2-划分训练集和测试集"><a href="#2-划分训练集和测试集" class="headerlink" title="2. 划分训练集和测试集"></a>2. 划分训练集和测试集</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.utils <span class="hljs-keyword">import</span> shuffle</span><br><span class="line">loans = shuffle(loans, random_state = <span class="hljs-number">34</span>)</span><br><span class="line"></span><br><span class="line">split_line = int(len(loans) * <span class="hljs-number">0.6</span>)</span><br><span class="line">train_data = loans.iloc[: split_line]</span><br><span class="line">test_data = loans.iloc[split_line:]</span><br></pre></td></tr></table></figure>
<h2 id="3-特征预处理"><a href="#3-特征预处理" class="headerlink" title="3. 特征预处理"></a>3. 特征预处理</h2><p>可以看到所有的特征都是离散类型的特征，需要对数据进行预处理，使用one-hot编码对其进行处理。</p>
<p>one-hot编码的思想就是将离散特征变成向量，假设特征$A$有三种取值$\lbrace a, b, c\rbrace$，这三种取值等价，如果我们使用1,2,3三个数字表示这三种取值，那么在计算时就会产生偏差，有一些涉及距离度量的算法会认为，2和1离得近，3和1离得远，但这三个值应该是等价的，这种表示方法会造成模型在判断上出现偏差。解决方案就是使用一个三维向量表示他们，用$[1, 0, 0]$表示a，$[0, 1, 0]$表示b，$[0, 0, 1]$表示c，这样三个向量之间的距离就都是相等的了，任意两个向量在欧式空间的距离都是$\sqrt{2}$。这就是one-hot编码是思想。</p>
<p>pandas中使用get_dummies生成one-hot向量</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">one_hot_encoding</span><span class="hljs-params">(data, features_categorical)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    Parameter</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    data: pd.DataFrame</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    features_categorical: list(str)</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 对所有的离散特征遍历</span></span><br><span class="line">    <span class="hljs-keyword">for</span> cat <span class="hljs-keyword">in</span> features_categorical:</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 对这列进行one-hot编码，前缀为这个变量名</span></span><br><span class="line">        one_encoding = pd.get_dummies(data[cat], prefix = cat)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 将生成的one-hot编码与之前的dataframe拼接起来</span></span><br><span class="line">        data = pd.concat([data, one_encoding],axis=<span class="hljs-number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 删除掉原始的这列离散特征</span></span><br><span class="line">        <span class="hljs-keyword">del</span> data[cat]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> data</span><br></pre></td></tr></table></figure>
<p>首先对训练集生成one-hot向量，然后对测试集生成one-hot向量，这里需要注意的是，如果训练集中，特征$A$的取值为$\lbrace a, b, c\rbrace$，这样我们生成的特征就有三列，分别为$A_a$, $A_b$, $A_c$，然后我们使用这个训练集训练模型，模型就只会考虑这三个特征，在测试集中如果有一个样本的特征$A$的值为$d$，那它的$A_a$，$A_b$，$A_c$就都为0，我们不去考虑$A_d$，因为这个特征在训练模型的时候是不存在的。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data = one_hot_encoding(train_data, features)</span><br></pre></td></tr></table></figure>
<p>获取所有特征的名字</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">one_hot_features = train_data.columns.tolist()</span><br><span class="line">one_hot_features.remove(target)</span><br><span class="line">one_hot_features</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;grade_A&apos;,
 &apos;grade_B&apos;,
 &apos;grade_C&apos;,
 &apos;grade_D&apos;,
 &apos;grade_E&apos;,
 &apos;grade_F&apos;,
 &apos;grade_G&apos;,
 &apos;term_ 36 months&apos;,
 &apos;term_ 60 months&apos;,
 &apos;home_ownership_MORTGAGE&apos;,
 &apos;home_ownership_OTHER&apos;,
 &apos;home_ownership_OWN&apos;,
 &apos;home_ownership_RENT&apos;,
 &apos;emp_length_1 year&apos;,
 &apos;emp_length_10+ years&apos;,
 &apos;emp_length_2 years&apos;,
 &apos;emp_length_3 years&apos;,
 &apos;emp_length_4 years&apos;,
 &apos;emp_length_5 years&apos;,
 &apos;emp_length_6 years&apos;,
 &apos;emp_length_7 years&apos;,
 &apos;emp_length_8 years&apos;,
 &apos;emp_length_9 years&apos;,
 &apos;emp_length_&lt; 1 year&apos;]
</code></pre><p>接下来是对测试集进行one_hot编码，但只要保留出现在one_hot_features中的特征即可·</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_data_tmp = one_hot_encoding(test_data, features)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 创建一个空的DataFrame</span></span><br><span class="line">test_data = pd.DataFrame(columns = train_data.columns)</span><br><span class="line"><span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> train_data.columns:</span><br><span class="line">    <span class="hljs-comment"># 如果训练集中当前特征在test_data_tmp中出现了，将其复制到test_data中</span></span><br><span class="line">    <span class="hljs-keyword">if</span> feature <span class="hljs-keyword">in</span> test_data_tmp.columns:</span><br><span class="line">        test_data[feature] = test_data_tmp[feature].copy()</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-comment"># 否则就用全为0的列去替代</span></span><br><span class="line">        test_data[feature] = np.zeros(test_data_tmp.shape[<span class="hljs-number">0</span>], dtype = <span class="hljs-string">'uint8'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.shape</span><br></pre></td></tr></table></figure>
<pre><code>(73564, 25)
</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_data.shape</span><br></pre></td></tr></table></figure>
<pre><code>(49043, 25)
</code></pre><p>训练集有37224个样本，测试集有9284个样本，<strong>处理完后，所有的特征都是0和1，标记是1和-1</strong>，以上就是数据预处理流程</p>
<h2 id="4-实现3种特征划分准则"><a href="#4-实现3种特征划分准则" class="headerlink" title="4. 实现3种特征划分准则"></a>4. 实现3种特征划分准则</h2><p>决策树中有很多常用的特征划分方法，比如信息增益、信息增益率、基尼指数</p>
<p>我们需要实现一个函数，它的作用是，给定决策树的某个结点内的所有样本的标记，让它计算出对应划分指标的值是多少</p>
<p>接下来我们会实现上述三种划分指标</p>
<p><strong>这里我们约定，将所有特征取值为0的样本，划分到左子树，特征取值为1的样本，划分到右子树</strong></p>
<h3 id="4-1-信息增益"><a href="#4-1-信息增益" class="headerlink" title="4.1 信息增益"></a>4.1 信息增益</h3><p>信息熵：<br>$$<br>\mathrm{Ent}(D) = - \sum^{\vert \mathcal{Y} \vert}_{k = 1} p_k \mathrm{log}_2 p_k<br>$$</p>
<p>信息增益：<br>$$<br>\mathrm{Gain}(D, a) = \mathrm{Ent}(D) - \sum^{V}_{v=1} \frac{\vert D^v \vert}{\vert D \vert} \mathrm{Ent}(D^v)<br>$$</p>
<p>计算信息熵时约定：若$p = 0$，则$p \log_2p = 0$</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">information_entropy</span><span class="hljs-params">(labels_in_node)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    求当前结点的信息熵</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameter</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    labels_in_node: np.ndarray, 如[-1, 1, -1, 1, 1]</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    float: information entropy</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 统计样本总个数</span></span><br><span class="line">    num_of_samples = labels_in_node.shape[<span class="hljs-number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> num_of_samples == <span class="hljs-number">0</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计出标记为1的个数</span></span><br><span class="line">    num_of_positive = len(labels_in_node[labels_in_node == <span class="hljs-number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计出标记为-1的个数</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    num_of_negative = len(labels_in_node[labels_in_node == <span class="hljs-number">-1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计正例的概率</span></span><br><span class="line">    prob_positive = num_of_positive / num_of_samples</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计负例的概率</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    prob_negative = num_of_negative / num_of_samples</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> prob_positive == <span class="hljs-number">0</span>:</span><br><span class="line">        positive_part = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        positive_part = prob_positive * np.log2(prob_positive)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> prob_negative == <span class="hljs-number">0</span>:</span><br><span class="line">        negative_part = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        negative_part = prob_negative * np.log2(prob_negative)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> - ( positive_part + negative_part )</span><br></pre></td></tr></table></figure>
<p>下面是6个测试样例</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 信息熵测试样例1</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])</span><br><span class="line">print(information_entropy(example_labels)) <span class="hljs-comment"># 0.97095</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 信息熵测试样例2</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])</span><br><span class="line">print(information_entropy(example_labels)) <span class="hljs-comment"># 0.86312</span></span><br><span class="line">    </span><br><span class="line"><span class="hljs-comment"># 信息熵测试样例3</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])</span><br><span class="line">print(information_entropy(example_labels)) <span class="hljs-comment"># 0.86312</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 信息熵测试样例4</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">-1</span>] * <span class="hljs-number">9</span> + [<span class="hljs-number">1</span>] * <span class="hljs-number">8</span>)</span><br><span class="line">print(information_entropy(example_labels)) <span class="hljs-comment"># 0.99750</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 信息熵测试样例5</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">1</span>] * <span class="hljs-number">8</span>)</span><br><span class="line">print(information_entropy(example_labels)) <span class="hljs-comment"># 0</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 信息熵测试样例6</span></span><br><span class="line">example_labels = np.array([])</span><br><span class="line">print(information_entropy(example_labels)) <span class="hljs-comment"># 0</span></span><br></pre></td></tr></table></figure>
<pre><code>0.970950594455
0.863120568567
0.863120568567
0.997502546369
-0.0
0
</code></pre><p>接下来完成计算所有特征的信息增益的函数</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_information_gains</span><span class="hljs-params">(data, features, target, annotate = False)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    计算所有特征的信息增益</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameter</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">        data: pd.DataFrame，传入的样本，带有特征和标记的dataframe</span></span><br><span class="line"><span class="hljs-string">        </span></span><br><span class="line"><span class="hljs-string">        features: list(str)，特征名组成的list</span></span><br><span class="line"><span class="hljs-string">        </span></span><br><span class="line"><span class="hljs-string">        target: str, 标记(label)的名字</span></span><br><span class="line"><span class="hljs-string">        </span></span><br><span class="line"><span class="hljs-string">        annotate, boolean，是否打印所有特征的信息增益值，默认为False</span></span><br><span class="line"><span class="hljs-string">        </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">        information_gains: dict, key: str, 特征名</span></span><br><span class="line"><span class="hljs-string">                                 value: float，信息增益</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 我们将使用每个特征划分的信息增益值存储在这个dict中</span></span><br><span class="line">    <span class="hljs-comment"># 键是特征名，值是信息增益值</span></span><br><span class="line">    information_gains = dict()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 对所有的特征进行遍历，使用当前的划分方法对每个特征进行计算</span></span><br><span class="line">    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features:</span><br><span class="line">        <span class="hljs-comment"># 左子树保证所有的样本的这个特征取值为0</span></span><br><span class="line">        left_split_target = data[data[feature] == <span class="hljs-number">0</span>][target]</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 右子树保证所有的样本的这个特征取值为1</span></span><br><span class="line">        right_split_target =  data[data[feature] == <span class="hljs-number">1</span>][target]</span><br><span class="line">            </span><br><span class="line">        <span class="hljs-comment"># 计算左子树的信息熵</span></span><br><span class="line">        left_entropy = information_entropy(left_split_target)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算左子树的权重</span></span><br><span class="line">        left_weight = len(left_split_target) / (len(left_split_target) + len(right_split_target))</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># 计算右子树的信息熵</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        right_entropy = information_entropy(right_split_target)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算右子树的权重</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        right_weight = len(right_split_target) / (len(left_split_target) + len(right_split_target))</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算当前结点的信息熵</span></span><br><span class="line">        current_entropy = information_entropy(data[target])</span><br><span class="line">            </span><br><span class="line">        <span class="hljs-comment"># 计算使用当前特征划分的信息增益</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        gain = current_entropy - (left_weight * left_entropy + right_weight * right_entropy)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 将特征名与增益值以键值对的形式存储在information_gains中</span></span><br><span class="line">        information_gains[feature] = gain</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-keyword">if</span> annotate:</span><br><span class="line">            print(<span class="hljs-string">" "</span>, feature, gain)</span><br><span class="line">            </span><br><span class="line">    <span class="hljs-keyword">return</span> information_gains</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 信息增益测试样例1</span></span><br><span class="line">print(compute_information_gains(train_data, one_hot_features, target)[<span class="hljs-string">'grade_A'</span>]) <span class="hljs-comment"># 0.01759</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 信息增益测试样例2</span></span><br><span class="line">print(compute_information_gains(train_data, one_hot_features, target)[<span class="hljs-string">'term_ 60 months'</span>]) <span class="hljs-comment"># 0.01429</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 信息增益测试样例3</span></span><br><span class="line">print(compute_information_gains(train_data, one_hot_features, target)[<span class="hljs-string">'grade_B'</span>]) <span class="hljs-comment"># 0.00370</span></span><br></pre></td></tr></table></figure>
<pre><code>0.0175919801789
0.0142918503294
0.00370492003453
</code></pre><h3 id="4-2-信息增益率"><a href="#4-2-信息增益率" class="headerlink" title="4.2 信息增益率"></a>4.2 信息增益率</h3><p>信息增益率：</p>
<p>$$<br>\mathrm{Gain_ratio}(D, a) = \frac{\mathrm{Gain}(D, a)}{\mathrm{IV}(a)}<br>$$</p>
<p>其中</p>
<p>$$<br>\mathrm{IV}(a) = - \sum^V_{v=1} \frac{\vert D^v \vert}{\vert D \vert} \log_2 \frac{\vert D^v \vert}{\vert D \vert}<br>$$</p>
<p>完成计算所有特征信息增益率的函数</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_information_gain_ratios</span><span class="hljs-params">(data, features, target, annotate = False)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    计算所有特征的信息增益率并保存起来</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameter</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    data: pd.DataFrame, 带有特征和标记的数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    features: list(str)，特征名组成的list</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    target: str， 特征的名字</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    annotate: boolean, default False，是否打印注释</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    gain_ratios: dict, key: str, 特征名</span></span><br><span class="line"><span class="hljs-string">                       value: float，信息增益率</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    gain_ratios = dict()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 对所有的特征进行遍历，使用当前的划分方法对每个特征进行计算</span></span><br><span class="line">    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features:</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 左子树保证所有的样本的这个特征取值为0</span></span><br><span class="line">        left_split_target = data[data[feature] == <span class="hljs-number">0</span>][target]</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 右子树保证所有的样本的这个特征取值为1</span></span><br><span class="line">        right_split_target =  data[data[feature] == <span class="hljs-number">1</span>][target]</span><br><span class="line">            </span><br><span class="line">        <span class="hljs-comment"># 计算左子树的信息熵</span></span><br><span class="line">        left_entropy = information_entropy(left_split_target)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算左子树的权重</span></span><br><span class="line">        left_weight = len(left_split_target) / (len(left_split_target) + len(right_split_target))</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># 计算右子树的信息熵</span></span><br><span class="line">        right_entropy = information_entropy(right_split_target)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算右子树的权重</span></span><br><span class="line">        right_weight = len(right_split_target) / (len(left_split_target) + len(right_split_target))</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算当前结点的信息熵</span></span><br><span class="line">        current_entropy = information_entropy(data[target])</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算当前结点的信息增益</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        gain = current_entropy - (left_weight * left_entropy + right_weight * right_entropy)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算左子树的IV</span></span><br><span class="line">        <span class="hljs-keyword">if</span> left_weight == <span class="hljs-number">0</span>:</span><br><span class="line">            left_IV = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">            left_IV = left_weight * np.log2(left_weight)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算右子树的IV</span></span><br><span class="line">        <span class="hljs-keyword">if</span> right_weight == <span class="hljs-number">0</span>:</span><br><span class="line">            right_IV = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">            right_IV = right_weight * np.log2(right_weight)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># IV 等于所有子树IV之和的相反数</span></span><br><span class="line">        IV = - (left_IV + right_IV)</span><br><span class="line">            </span><br><span class="line">        <span class="hljs-comment"># 计算使用当前特征划分的信息增益率</span></span><br><span class="line">        <span class="hljs-comment"># 这里为了防止IV是0，导致除法得到np.inf，在分母加了一个很小的小数</span></span><br><span class="line">        gain_ratio = gain / (IV + np.finfo(np.longdouble).eps)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 信息增益率的存储</span></span><br><span class="line">        gain_ratios[feature] = gain_ratio</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-keyword">if</span> annotate:</span><br><span class="line">            print(<span class="hljs-string">" "</span>, feature, gain_ratio)</span><br><span class="line">            </span><br><span class="line">    <span class="hljs-keyword">return</span> gain_ratios</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 信息增益率测试样例1</span></span><br><span class="line">print(compute_information_gain_ratios(train_data, one_hot_features, target)[<span class="hljs-string">'grade_A'</span>]) <span class="hljs-comment"># 0.02573</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 信息增益率测试样例2</span></span><br><span class="line">print(compute_information_gain_ratios(train_data, one_hot_features, target)[<span class="hljs-string">'grade_B'</span>]) <span class="hljs-comment"># 0.00417</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 信息增益率测试样例3</span></span><br><span class="line">print(compute_information_gain_ratios(train_data, one_hot_features, target)[<span class="hljs-string">'term_ 60 months'</span>]) <span class="hljs-comment"># 0.01970</span></span><br></pre></td></tr></table></figure>
<pre><code>0.025734780668
0.00417549506943
0.0197093627186
</code></pre><h3 id="4-3-基尼指数"><a href="#4-3-基尼指数" class="headerlink" title="4.3 基尼指数"></a>4.3 基尼指数</h3><p>数据集$D$的基尼值：</p>
<p>$$<br>\begin{aligned}<br>\mathrm{Gini}(D) &amp; = \sum^{\vert \mathcal{Y} \vert}_{k=1} \sum_{k’ \neq k} p_k p_{k’}\\<br>&amp; = 1 - \sum^{\vert \mathcal{Y} \vert}_{k=1} p^2_k.<br>\end{aligned}<br>$$</p>
<p>属性$a$的基尼指数：</p>
<p>$$<br>\mathrm{Gini_index}(D, a) = \sum^V_{v = 1} \frac{\vert D^v \vert}{\vert D \vert} \mathrm{Gini}(D^v)<br>$$</p>
<p>完成数据集基尼值的计算</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gini</span><span class="hljs-params">(labels_in_node)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    计算一个结点内样本的基尼指数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Paramters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    label_in_data: np.ndarray, 样本的标记，如[-1, -1, 1, 1, 1]</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ---------</span></span><br><span class="line"><span class="hljs-string">    gini: float，基尼指数</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计样本总个数</span></span><br><span class="line">    num_of_samples = labels_in_node.shape[<span class="hljs-number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> num_of_samples == <span class="hljs-number">0</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计出1的个数</span></span><br><span class="line">    num_of_positive = len(labels_in_node[labels_in_node == <span class="hljs-number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计出-1的个数</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    num_of_negative = len(labels_in_node[labels_in_node == <span class="hljs-number">-1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计正例的概率</span></span><br><span class="line">    prob_positive = num_of_positive / num_of_samples</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计负例的概率</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    prob_negative = num_of_negative / num_of_samples</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 计算基尼值</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    gini = <span class="hljs-number">1</span> - (prob_positive ** <span class="hljs-number">2</span> + prob_negative ** <span class="hljs-number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> gini</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 基尼值测试样例1</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])</span><br><span class="line">print(gini(example_labels)) <span class="hljs-comment"># 0.48</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 基尼值测试样例2</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])</span><br><span class="line">print(gini(example_labels)) <span class="hljs-comment"># 0.40816</span></span><br><span class="line">    </span><br><span class="line"><span class="hljs-comment"># 基尼值测试样例3</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])</span><br><span class="line">print(gini(example_labels)) <span class="hljs-comment"># 0.40816</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 基尼值测试样例4</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">-1</span>] * <span class="hljs-number">9</span> + [<span class="hljs-number">1</span>] * <span class="hljs-number">8</span>)</span><br><span class="line">print(gini(example_labels)) <span class="hljs-comment"># 0.49827</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 基尼值测试样例5</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">1</span>] * <span class="hljs-number">8</span>)</span><br><span class="line">print(gini(example_labels)) <span class="hljs-comment"># 0</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 基尼值测试样例6</span></span><br><span class="line">example_labels = np.array([])</span><br><span class="line">print(gini(example_labels)) <span class="hljs-comment"># 0</span></span><br></pre></td></tr></table></figure>
<pre><code>0.48
0.40816326530612246
0.40816326530612246
0.4982698961937716
0.0
0
</code></pre><p>然后计算所有特征的基尼指数</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_gini_indices</span><span class="hljs-params">(data, features, target, annotate = False)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    计算使用各个特征进行划分时，各特征的基尼指数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameter</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    data: pd.DataFrame, 带有特征和标记的数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    features: list(str)，特征名组成的list</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    target: str， 特征的名字</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    annotate: boolean, default False，是否打印注释</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    gini_indices: dict, key: str, 特征名</span></span><br><span class="line"><span class="hljs-string">                       value: float，基尼指数</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    gini_indices = dict()</span><br><span class="line">    <span class="hljs-comment"># 对所有的特征进行遍历，使用当前的划分方法对每个特征进行计算</span></span><br><span class="line">    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features:</span><br><span class="line">        <span class="hljs-comment"># 左子树保证所有的样本的这个特征取值为0</span></span><br><span class="line">        left_split_target = data[data[feature] == <span class="hljs-number">0</span>][target]</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 右子树保证所有的样本的这个特征取值为1</span></span><br><span class="line">        right_split_target =  data[data[feature] == <span class="hljs-number">1</span>][target]</span><br><span class="line">            </span><br><span class="line">        <span class="hljs-comment"># 计算左子树的基尼值</span></span><br><span class="line">        left_gini = gini(left_split_target)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算左子树的权重</span></span><br><span class="line">        left_weight = len(left_split_target) / (len(left_split_target) + len(right_split_target))</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># 计算右子树的基尼值</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        right_gini = gini(right_split_target)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算右子树的权重</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        right_weight = len(right_split_target) / (len(left_split_target) + len(right_split_target))</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算当前结点的基尼指数</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        gini_index = left_weight * left_gini + right_weight * right_gini</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 存储</span></span><br><span class="line">        gini_indices[feature] = gini_index</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-keyword">if</span> annotate:</span><br><span class="line">            print(<span class="hljs-string">" "</span>, feature, gini_index)</span><br><span class="line">            </span><br><span class="line">    <span class="hljs-keyword">return</span> gini_indices</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 基尼指数测试样例1</span></span><br><span class="line">print(compute_gini_indices(train_data, one_hot_features, target)[<span class="hljs-string">'grade_A'</span>]) <span class="hljs-comment"># 0.30095</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 基尼指数测试样例2</span></span><br><span class="line">print(compute_gini_indices(train_data, one_hot_features, target)[<span class="hljs-string">'grade_B'</span>]) <span class="hljs-comment"># 0.30568</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 基尼指数测试样例3</span></span><br><span class="line">print(compute_gini_indices(train_data, one_hot_features, target)[<span class="hljs-string">'term_ 36 months'</span>]) <span class="hljs-comment"># 0.30055</span></span><br></pre></td></tr></table></figure>
<pre><code>0.3009520964964362
0.3056855375882364
0.30055418611740065
</code></pre><h2 id="5-完成最优特征的选择"><a href="#5-完成最优特征的选择" class="headerlink" title="5. 完成最优特征的选择"></a>5. 完成最优特征的选择</h2><p>到此，我们完成了三种划分策略的实现，接下来就是完成获取最优特征的函数</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">best_splitting_feature</span><span class="hljs-params">(data, features, target, criterion = <span class="hljs-string">'gini'</span>, annotate = False)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    给定划分方法和数据，找到最优的划分特征</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    data: pd.DataFrame, 带有特征和标记的数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    features: list(str)，特征名组成的list</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    target: str， 特征的名字</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    criterion: str, 使用哪种指标，三种选项: 'information_gain', 'gain_ratio', 'gini'</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    annotate: boolean, default False，是否打印注释</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    best_feature: str, 最佳的划分特征的名字</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> criterion == <span class="hljs-string">'information_gain'</span>:</span><br><span class="line">        <span class="hljs-keyword">if</span> annotate:</span><br><span class="line">            print(<span class="hljs-string">'using information gain'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 得到当前所有特征的信息增益</span></span><br><span class="line">        information_gains = compute_information_gains(data, features, target, annotate)</span><br><span class="line">    </span><br><span class="line">        <span class="hljs-comment"># information_gains是一个dict类型的对象，我们要找值最大的那个元素的键是谁</span></span><br><span class="line">        <span class="hljs-comment"># 可以通过这种方式直接获取这个键</span></span><br><span class="line">        <span class="hljs-comment"># 根据这些特征和他们的信息增益，找到最佳的划分特征</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        best_feature = max(information_gains.items(), key = <span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>])[<span class="hljs-number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-keyword">return</span> best_feature</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">elif</span> criterion == <span class="hljs-string">'gain_ratio'</span>:</span><br><span class="line">        <span class="hljs-keyword">if</span> annotate:</span><br><span class="line">            print(<span class="hljs-string">'using information gain ratio'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 得到当前所有特征的信息增益率</span></span><br><span class="line">        gain_ratios = compute_information_gain_ratios(data, features, target, annotate)</span><br><span class="line">    </span><br><span class="line">        <span class="hljs-comment"># 根据这些特征和他们的信息增益率，找到最佳的划分特征</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        best_feature = max(gain_ratios.items(), key = <span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>])[<span class="hljs-number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">return</span> best_feature</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">elif</span> criterion == <span class="hljs-string">'gini'</span>:</span><br><span class="line">        <span class="hljs-keyword">if</span> annotate:</span><br><span class="line">            print(<span class="hljs-string">'using gini'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 得到当前所有特征的基尼指数</span></span><br><span class="line">        gini_indices = compute_gini_indices(data, features, target, annotate)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 根据这些特征和他们的基尼指数，找到最佳的划分特征</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        best_feature = min(gini_indices.items(), key = <span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>])[<span class="hljs-number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">return</span> best_feature</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">"传入的criterion不合规!"</span>, criterion)</span><br></pre></td></tr></table></figure>
<h2 id="6-判断结点内样本的类别是否为同一类"><a href="#6-判断结点内样本的类别是否为同一类" class="headerlink" title="6. 判断结点内样本的类别是否为同一类"></a>6. 判断结点内样本的类别是否为同一类</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">intermediate_node_num_mistakes</span><span class="hljs-params">(labels_in_node)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    求树的结点中，样本数少的那个类的样本有多少，比如输入是[1, 1, -1, -1, 1]，返回2</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameter</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    labels_in_node: np.ndarray, pd.Series</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    int：个数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 如果传入的array为空，返回0</span></span><br><span class="line">    <span class="hljs-keyword">if</span> len(labels_in_node) == <span class="hljs-number">0</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计1的个数</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    num_of_one = len(labels_in_node[labels_in_node == <span class="hljs-number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计-1的个数</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    num_of_minus_one = len(labels_in_node[labels_in_node == <span class="hljs-number">-1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> num_of_one <span class="hljs-keyword">if</span> num_of_minus_one &gt; num_of_one <span class="hljs-keyword">else</span> num_of_minus_one</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例1</span></span><br><span class="line">print(intermediate_node_num_mistakes(np.array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>]))) <span class="hljs-comment"># 2</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 测试样例2</span></span><br><span class="line">print(intermediate_node_num_mistakes(np.array([]))) <span class="hljs-comment"># 0</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 测试样例3</span></span><br><span class="line">print(intermediate_node_num_mistakes(np.array([<span class="hljs-number">1</span>]))) <span class="hljs-comment"># 0</span></span><br></pre></td></tr></table></figure>
<pre><code>2
0
0
</code></pre><h2 id="7-创建叶子结点"><a href="#7-创建叶子结点" class="headerlink" title="7. 创建叶子结点"></a>7. 创建叶子结点</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_leaf</span><span class="hljs-params">(target_values)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    计算出当前叶子结点的标记是什么，并且将叶子结点信息保存在一个dict中</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameter:</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    target_values: pd.Series, 当前叶子结点内样本的标记</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    leaf: dict，表示一个叶结点，</span></span><br><span class="line"><span class="hljs-string">            leaf['splitting_features'], None，叶结点不需要划分特征</span></span><br><span class="line"><span class="hljs-string">            leaf['left'], None，叶结点没有左子树</span></span><br><span class="line"><span class="hljs-string">            leaf['right'], None，叶结点没有右子树</span></span><br><span class="line"><span class="hljs-string">            leaf['is_leaf'], True, 是否是叶子结点</span></span><br><span class="line"><span class="hljs-string">            leaf['prediction'], int, 表示该叶子结点的预测值</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 创建叶子结点</span></span><br><span class="line">    leaf = &#123;<span class="hljs-string">'splitting_feature'</span> : <span class="hljs-keyword">None</span>,</span><br><span class="line">            <span class="hljs-string">'left'</span> : <span class="hljs-keyword">None</span>,</span><br><span class="line">            <span class="hljs-string">'right'</span> : <span class="hljs-keyword">None</span>,</span><br><span class="line">            <span class="hljs-string">'is_leaf'</span>: <span class="hljs-keyword">True</span>&#125;</span><br><span class="line">   </span><br><span class="line">    <span class="hljs-comment"># 数结点内-1和+1的个数</span></span><br><span class="line">    num_ones = len(target_values[target_values == +<span class="hljs-number">1</span>])</span><br><span class="line">    num_minus_ones = len(target_values[target_values == <span class="hljs-number">-1</span>])    </span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 叶子结点的标记使用少数服从多数的原则，为样本数多的那类的标记，保存在 leaf['prediction']</span></span><br><span class="line">    <span class="hljs-keyword">if</span> num_ones &gt; num_minus_ones:</span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        leaf[<span class="hljs-string">'prediction'</span>] = <span class="hljs-number">1</span></span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        leaf[<span class="hljs-string">'prediction'</span>] = <span class="hljs-number">-1</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 返回叶子结点</span></span><br><span class="line">    <span class="hljs-keyword">return</span> leaf</span><br></pre></td></tr></table></figure>
<h2 id="8-递归地创建决策树"><a href="#8-递归地创建决策树" class="headerlink" title="8. 递归地创建决策树"></a>8. 递归地创建决策树</h2><p>递归的创建决策树<br>递归算法终止的三个条件：</p>
<ol>
<li>如果结点内所有的样本的标记都相同，该结点就不需要再继续划分，直接做叶子结点即可</li>
<li>如果结点所有的特征都已经在之前使用过了，在当前结点无剩余特征可供划分样本，该结点直接做叶子结点</li>
<li>如果当前结点的深度已经达到了我们限制的树的最大深度，直接做叶子结点</li>
</ol>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">decision_tree_create</span><span class="hljs-params">(data, features, target, criterion = <span class="hljs-string">'gini'</span>, current_depth = <span class="hljs-number">0</span>, max_depth = <span class="hljs-number">10</span>, annotate = False)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    Parameter:</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    data: pd.DataFrame, 数据</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    features: iterable, 特征组成的可迭代对象，比如一个list</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    target: str, 标记的名字</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    criterion: 'str', 特征划分方法，只支持三种：'information_gain', 'gain_ratio', 'gini'</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    current_depth: int, 当前深度，递归的时候需要记录</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    max_depth: int, 树的最大深度，我们设定的树的最大深度，达到最大深度需要终止递归</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    dict, dict['is_leaf']          : False, 当前顶点不是叶子结点</span></span><br><span class="line"><span class="hljs-string">          dict['prediction']       : None, 不是叶子结点就没有预测值</span></span><br><span class="line"><span class="hljs-string">          dict['splitting_feature']: splitting_feature, 当前结点是使用哪个特征进行划分的</span></span><br><span class="line"><span class="hljs-string">          dict['left']             : dict</span></span><br><span class="line"><span class="hljs-string">          dict['right']            : dict</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> criterion <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">'information_gain'</span>, <span class="hljs-string">'gain_ratio'</span>, <span class="hljs-string">'gini'</span>]:</span><br><span class="line">        <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">"传入的criterion不合规!"</span>, criterion)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 复制一份特征，存储起来，每使用一个特征进行划分，我们就删除一个</span></span><br><span class="line">    remaining_features = features[:]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 取出标记值</span></span><br><span class="line">    target_values = data[target]</span><br><span class="line">    print(<span class="hljs-string">"-"</span> * <span class="hljs-number">50</span>)</span><br><span class="line">    print(<span class="hljs-string">"Subtree, depth = %s (%s data points)."</span> % (current_depth, len(target_values)))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 终止条件1</span></span><br><span class="line">    <span class="hljs-comment"># 如果当前结点内所有样本同属一类，即这个结点中，各类别样本数最小的那个等于0</span></span><br><span class="line">    <span class="hljs-comment"># 使用前面写的intermediate_node_num_mistakes来完成这个判断</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    <span class="hljs-keyword">if</span> intermediate_node_num_mistakes(target_values) == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"Stopping condition 1 reached."</span>)</span><br><span class="line">        <span class="hljs-keyword">return</span> create_leaf(target_values)   <span class="hljs-comment"># 创建叶子节点</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 终止条件2</span></span><br><span class="line">    <span class="hljs-comment"># 如果已经没有剩余的特征可供分割，即remaining_features为空</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    <span class="hljs-keyword">if</span> len(remaining_features) == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"Stopping condition 2 reached."</span>)</span><br><span class="line">        <span class="hljs-keyword">return</span> create_leaf(target_values)   <span class="hljs-comment"># 创建叶子节点</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 终止条件3</span></span><br><span class="line">    <span class="hljs-comment"># 如果已经到达了我们要求的最大深度，即当前深度达到了最大深度</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    <span class="hljs-keyword">if</span> current_depth &gt;= max_depth:</span><br><span class="line">        print(<span class="hljs-string">"Reached maximum depth. Stopping for now."</span>)</span><br><span class="line">        <span class="hljs-keyword">return</span> create_leaf(target_values)   <span class="hljs-comment"># 创建叶子节点</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 找到最优划分特征</span></span><br><span class="line">    <span class="hljs-comment"># 使用best_splitting_feature这个函数</span></span><br><span class="line">    <span class="hljs-comment">## YOUR CODE HERE</span></span><br><span class="line">    splitting_feature = best_splitting_feature(data, features, target, criterion, annotate)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 使用我们找到的最优特征将数据划分成两份</span></span><br><span class="line">    <span class="hljs-comment"># 左子树的数据</span></span><br><span class="line">    left_split = data[data[splitting_feature] == <span class="hljs-number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 右子树的数据</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    right_split = data[data[splitting_feature] == <span class="hljs-number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 现在已经完成划分，我们要从剩余特征中删除掉当前这个特征</span></span><br><span class="line">    remaining_features.remove(splitting_feature)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 打印当前划分使用的特征，打印左子树样本个数，右子树样本个数</span></span><br><span class="line">    print(<span class="hljs-string">"Split on feature %s. (%s, %s)"</span> % (\</span><br><span class="line">                      splitting_feature, len(left_split), len(right_split)))</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 如果使用当前的特征，将所有的样本都划分到一棵子树中，那么就直接将这棵子树变成叶子节点</span></span><br><span class="line">    <span class="hljs-comment"># 判断左子树是不是“完美”的</span></span><br><span class="line">    <span class="hljs-keyword">if</span> len(left_split) == len(data):</span><br><span class="line">        print(<span class="hljs-string">"Creating leaf node."</span>)</span><br><span class="line">        <span class="hljs-keyword">return</span> create_leaf(left_split[target])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 判断右子树是不是“完美”的</span></span><br><span class="line">    <span class="hljs-keyword">if</span> len(right_split) == len(data):</span><br><span class="line">        print(<span class="hljs-string">"Creating right node."</span>)</span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        <span class="hljs-keyword">return</span> create_leaf(right_split[target])</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 递归地创建左子树</span></span><br><span class="line">    left_tree = decision_tree_create(left_split, remaining_features, target, criterion, current_depth + <span class="hljs-number">1</span>, max_depth, annotate)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 递归地创建右子树</span></span><br><span class="line">    <span class="hljs-comment">## YOUR CODE HERE</span></span><br><span class="line">    right_tree = decision_tree_create(right_split, remaining_features, target, criterion, current_depth + <span class="hljs-number">1</span>, max_depth, annotate) </span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 返回树的非叶子结点</span></span><br><span class="line">    <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">'is_leaf'</span>          : <span class="hljs-keyword">False</span>, </span><br><span class="line">            <span class="hljs-string">'prediction'</span>       : <span class="hljs-keyword">None</span>,</span><br><span class="line">            <span class="hljs-string">'splitting_feature'</span>: splitting_feature,</span><br><span class="line">            <span class="hljs-string">'left'</span>             : left_tree, </span><br><span class="line">            <span class="hljs-string">'right'</span>            : right_tree&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my_decision_tree = decision_tree_create(train_data, one_hot_features, target, <span class="hljs-string">'gini'</span>, max_depth = <span class="hljs-number">6</span>, annotate = <span class="hljs-keyword">False</span>)</span><br></pre></td></tr></table></figure>
<pre><code>--------------------------------------------------
Subtree, depth = 0 (73564 data points).
Split on feature term_ 36 months. (14831, 58733)
--------------------------------------------------
Subtree, depth = 1 (14831 data points).
Split on feature grade_F. (13003, 1828)
--------------------------------------------------
Subtree, depth = 2 (13003 data points).
Split on feature grade_E. (9818, 3185)
--------------------------------------------------
Subtree, depth = 3 (9818 data points).
Split on feature home_ownership_RENT. (6796, 3022)
--------------------------------------------------
Subtree, depth = 4 (6796 data points).
Split on feature grade_G. (6507, 289)
--------------------------------------------------
Subtree, depth = 5 (6507 data points).
Split on feature grade_D. (4368, 2139)
--------------------------------------------------
Subtree, depth = 6 (4368 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (2139 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (289 data points).
Split on feature home_ownership_OWN. (249, 40)
--------------------------------------------------
Subtree, depth = 6 (249 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (40 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 4 (3022 data points).
Split on feature grade_G. (2827, 195)
--------------------------------------------------
Subtree, depth = 5 (2827 data points).
Split on feature grade_D. (1651, 1176)
--------------------------------------------------
Subtree, depth = 6 (1651 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (1176 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (195 data points).
Split on feature emp_length_2 years. (176, 19)
--------------------------------------------------
Subtree, depth = 6 (176 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (19 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 3 (3185 data points).
Split on feature home_ownership_RENT. (1980, 1205)
--------------------------------------------------
Subtree, depth = 4 (1980 data points).
Split on feature emp_length_3 years. (1828, 152)
--------------------------------------------------
Subtree, depth = 5 (1828 data points).
Split on feature emp_length_10+ years. (1057, 771)
--------------------------------------------------
Subtree, depth = 6 (1057 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (771 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (152 data points).
Split on feature home_ownership_OTHER. (151, 1)
--------------------------------------------------
Subtree, depth = 6 (151 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (1 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 4 (1205 data points).
Split on feature emp_length_1 year. (1124, 81)
--------------------------------------------------
Subtree, depth = 5 (1124 data points).
Split on feature emp_length_8 years. (1073, 51)
--------------------------------------------------
Subtree, depth = 6 (1073 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (51 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (81 data points).
Split on feature grade_A. (81, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 2 (1828 data points).
Split on feature home_ownership_RENT. (1030, 798)
--------------------------------------------------
Subtree, depth = 3 (1030 data points).
Split on feature emp_length_3 years. (957, 73)
--------------------------------------------------
Subtree, depth = 4 (957 data points).
Split on feature emp_length_2 years. (886, 71)
--------------------------------------------------
Subtree, depth = 5 (886 data points).
Split on feature home_ownership_OTHER. (884, 2)
--------------------------------------------------
Subtree, depth = 6 (884 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (2 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 5 (71 data points).
Split on feature home_ownership_MORTGAGE. (12, 59)
--------------------------------------------------
Subtree, depth = 6 (12 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (59 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 4 (73 data points).
Split on feature home_ownership_MORTGAGE. (13, 60)
--------------------------------------------------
Subtree, depth = 5 (13 data points).
Split on feature grade_A. (13, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 5 (60 data points).
Split on feature grade_A. (60, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 3 (798 data points).
Split on feature emp_length_7 years. (740, 58)
--------------------------------------------------
Subtree, depth = 4 (740 data points).
Split on feature emp_length_3 years. (673, 67)
--------------------------------------------------
Subtree, depth = 5 (673 data points).
Split on feature emp_length_9 years. (646, 27)
--------------------------------------------------
Subtree, depth = 6 (646 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (27 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (67 data points).
Split on feature grade_A. (67, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 4 (58 data points).
Split on feature grade_A. (58, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 1 (58733 data points).
Split on feature grade_A. (45632, 13101)
--------------------------------------------------
Subtree, depth = 2 (45632 data points).
Split on feature grade_B. (25130, 20502)
--------------------------------------------------
Subtree, depth = 3 (25130 data points).
Split on feature grade_C. (11066, 14064)
--------------------------------------------------
Subtree, depth = 4 (11066 data points).
Split on feature home_ownership_MORTGAGE. (6987, 4079)
--------------------------------------------------
Subtree, depth = 5 (6987 data points).
Split on feature grade_F. (6650, 337)
--------------------------------------------------
Subtree, depth = 6 (6650 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (337 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (4079 data points).
Split on feature grade_G. (4003, 76)
--------------------------------------------------
Subtree, depth = 6 (4003 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (76 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 4 (14064 data points).
Split on feature home_ownership_MORTGAGE. (8209, 5855)
--------------------------------------------------
Subtree, depth = 5 (8209 data points).
Split on feature emp_length_2 years. (7209, 1000)
--------------------------------------------------
Subtree, depth = 6 (7209 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (1000 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (5855 data points).
Split on feature emp_length_10+ years. (3802, 2053)
--------------------------------------------------
Subtree, depth = 6 (3802 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (2053 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 3 (20502 data points).
Split on feature home_ownership_MORTGAGE. (10775, 9727)
--------------------------------------------------
Subtree, depth = 4 (10775 data points).
Split on feature home_ownership_OTHER. (10741, 34)
--------------------------------------------------
Subtree, depth = 5 (10741 data points).
Split on feature emp_length_1 year. (9754, 987)
--------------------------------------------------
Subtree, depth = 6 (9754 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (987 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (34 data points).
Split on feature emp_length_10+ years. (27, 7)
--------------------------------------------------
Subtree, depth = 6 (27 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (7 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 4 (9727 data points).
Split on feature emp_length_&lt; 1 year. (9186, 541)
--------------------------------------------------
Subtree, depth = 5 (9186 data points).
Split on feature emp_length_9 years. (8807, 379)
--------------------------------------------------
Subtree, depth = 6 (8807 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (379 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (541 data points).
Split on feature grade_C. (541, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 2 (13101 data points).
Split on feature home_ownership_MORTGAGE. (5830, 7271)
--------------------------------------------------
Subtree, depth = 3 (5830 data points).
Split on feature emp_length_3 years. (5283, 547)
--------------------------------------------------
Subtree, depth = 4 (5283 data points).
Split on feature emp_length_1 year. (4705, 578)
--------------------------------------------------
Subtree, depth = 5 (4705 data points).
Split on feature emp_length_7 years. (4467, 238)
--------------------------------------------------
Subtree, depth = 6 (4467 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (238 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (578 data points).
Split on feature home_ownership_OTHER. (576, 2)
--------------------------------------------------
Subtree, depth = 6 (576 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (2 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 4 (547 data points).
Split on feature home_ownership_OTHER. (545, 2)
--------------------------------------------------
Subtree, depth = 5 (545 data points).
Split on feature home_ownership_OWN. (468, 77)
--------------------------------------------------
Subtree, depth = 6 (468 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (77 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (2 data points).
Split on feature grade_B. (2, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 3 (7271 data points).
Split on feature emp_length_2 years. (6702, 569)
--------------------------------------------------
Subtree, depth = 4 (6702 data points).
Split on feature emp_length_4 years. (6234, 468)
--------------------------------------------------
Subtree, depth = 5 (6234 data points).
Split on feature emp_length_3 years. (5689, 545)
--------------------------------------------------
Subtree, depth = 6 (5689 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (545 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (468 data points).
Split on feature grade_B. (468, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 4 (569 data points).
Split on feature grade_B. (569, 0)
Creating leaf node.
</code></pre><p>现在，模型就训练好了</p>
<h2 id="9-预测"><a href="#9-预测" class="headerlink" title="9. 预测"></a>9. 预测</h2><p>接下来我们需要完成预测函数</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">classify</span><span class="hljs-params">(tree, x, annotate = False)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    递归的进行预测，一次只能预测一个样本</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    tree: dict</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    x: pd.Series，样本</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    x: pd.DataFrame, 待预测的样本</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    annotate, boolean, 是否显示注释</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    返回预测的标记</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-keyword">if</span> tree[<span class="hljs-string">'is_leaf'</span>]:</span><br><span class="line">        <span class="hljs-keyword">if</span> annotate:</span><br><span class="line">            <span class="hljs-keyword">print</span> (<span class="hljs-string">"At leaf, predicting %s"</span> % tree[<span class="hljs-string">'prediction'</span>])</span><br><span class="line">        <span class="hljs-keyword">return</span> tree[<span class="hljs-string">'prediction'</span>]</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        split_feature_value = x[tree[<span class="hljs-string">'splitting_feature'</span>]]</span><br><span class="line">        <span class="hljs-keyword">if</span> annotate:</span><br><span class="line">             <span class="hljs-keyword">print</span> (<span class="hljs-string">"Split on %s = %s"</span> % (tree[<span class="hljs-string">'splitting_feature'</span>], split_feature_value))</span><br><span class="line">        <span class="hljs-keyword">if</span> split_feature_value == <span class="hljs-number">0</span>:</span><br><span class="line">            <span class="hljs-keyword">return</span> classify(tree[<span class="hljs-string">'left'</span>], x, annotate)</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            <span class="hljs-keyword">return</span> classify(tree[<span class="hljs-string">'right'</span>], x, annotate)</span><br></pre></td></tr></table></figure>
<p>我们取测试集第一个样本来测试</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_sample = test_data.iloc[<span class="hljs-number">0</span>]</span><br><span class="line">print(test_sample)</span><br></pre></td></tr></table></figure>
<pre><code>safe_loans                 1
grade_A                    0
grade_B                    0
grade_C                    0
grade_D                    0
grade_E                    1
grade_F                    0
grade_G                    0
term_ 36 months            1
term_ 60 months            0
home_ownership_MORTGAGE    1
home_ownership_OTHER       0
home_ownership_OWN         0
home_ownership_RENT        0
emp_length_1 year          0
emp_length_10+ years       0
emp_length_2 years         1
emp_length_3 years         0
emp_length_4 years         0
emp_length_5 years         0
emp_length_6 years         0
emp_length_7 years         0
emp_length_8 years         0
emp_length_9 years         0
emp_length_&lt; 1 year        0
Name: 37225, dtype: int64
</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="hljs-string">'True class: %s '</span> % (test_sample[<span class="hljs-string">'safe_loans'</span>]))</span><br><span class="line">print(<span class="hljs-string">'Predicted class: %s '</span> % classify(my_decision_tree, test_sample))</span><br></pre></td></tr></table></figure>
<pre><code>True class: 1 
Predicted class: 1 
</code></pre><p>打印出使用决策树判断的过程</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classify(my_decision_tree, test_sample, annotate=<span class="hljs-keyword">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Split on term_ 36 months = 1
Split on grade_A = 0
Split on grade_B = 0
Split on grade_C = 0
Split on home_ownership_MORTGAGE = 1
Split on grade_G = 0
At leaf, predicting 1





1
</code></pre><h2 id="10-在测试集上对我们的模型进行评估"><a href="#10-在测试集上对我们的模型进行评估" class="headerlink" title="10. 在测试集上对我们的模型进行评估"></a>10. 在测试集上对我们的模型进行评估</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score</span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> precision_score</span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> recall_score</span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> f1_score</span><br></pre></td></tr></table></figure>
<p>先来编写一个批量预测的函数，传入的是整个测试集那样的pd.DataFrame，这个函数返回一个np.ndarray，存储模型的预测结果</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(tree, data)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    按行遍历data，对每个样本进行预测，将值存储起来，最后返回np.ndarray</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameter</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    tree, dict, 模型</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    data, pd.DataFrame, 数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    predictions, np.ndarray, 模型对这些样本的预测结果</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    predictions = np.zeros(len(data))</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(data)):</span><br><span class="line">        predictions[i] = classify(tree, data.iloc[i])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> predictions</span><br></pre></td></tr></table></figure>
<h2 id="11-请你计算使用不同评价指标得到模型的四项指标的值，填写在下方表格内"><a href="#11-请你计算使用不同评价指标得到模型的四项指标的值，填写在下方表格内" class="headerlink" title="11. 请你计算使用不同评价指标得到模型的四项指标的值，填写在下方表格内"></a>11. 请你计算使用不同评价指标得到模型的四项指标的值，填写在下方表格内</h2><p><strong>树的最大深度为6</strong></p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># YOUR CODE HERE</span></span><br></pre></td></tr></table></figure>
<p>树的最大深度为6  </p>
<h6 id="双击此处编写"><a href="#双击此处编写" class="headerlink" title="双击此处编写"></a>双击此处编写</h6><table>
<thead>
<tr>
<th>划分标准</th>
<th>精度</th>
<th>查准率</th>
<th>查全率</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>信息增益</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<td>信息增益率</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<td>基尼指数</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
</tbody>
</table>
<h2 id="12-扩展：使用Echarts绘制决策树"><a href="#12-扩展：使用Echarts绘制决策树" class="headerlink" title="12. 扩展：使用Echarts绘制决策树"></a>12. 扩展：使用Echarts绘制决策树</h2><p>我们可以使用echarts绘制出我们训练的决策树，这时候可以利用pyecharts这个库<br><a href="http://pyecharts.org/#/" target="_blank" rel="noopener">pyecharts</a><br>pyecharts可以与jupyter notebook无缝衔接，直接在notebook中绘制图表。<br><strong>提醒：pyecharts还未支持jupyter lab</strong></p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 导入树形图</span></span><br><span class="line"><span class="hljs-keyword">from</span> pyecharts <span class="hljs-keyword">import</span> Tree</span><br></pre></td></tr></table></figure>
<p>echarts中的树形图要求我们提供一组这样的数据<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        value: 1212,    # 数值</span><br><span class="line">        # 子节点</span><br><span class="line">        children: [</span><br><span class="line">            &#123;</span><br><span class="line">                # 子节点数值</span><br><span class="line">                value: 2323,</span><br><span class="line">                # 子节点名</span><br><span class="line">                name: &apos;description of this node&apos;,</span><br><span class="line">                children: [...],</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                value: 4545,</span><br><span class="line">                name: &apos;description of this node&apos;,</span><br><span class="line">                children: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        value: 5656,</span><br><span class="line">                        name: &apos;description of this node&apos;,</span><br><span class="line">                        children: [...]</span><br><span class="line">                    &#125;,</span><br><span class="line">                    ...</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    ...</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p>
<p>关于pyecharts中的树形图的文档地址:<a href="http://pyecharts.org/#/zh-cn/charts?id=tree%EF%BC%88%E6%A0%91%E5%9B%BE%EF%BC%89" target="_blank" rel="noopener">pyecharts Tree</a></p>
<p>其实和我们训练得到的树结构类似，只不过每个结点有个”name”属性，表示这个结点的名字，”value”表示它的值，”children”是一个list，里面还有这样的dict，我们可以写一个递归的函数完成这种数据的生成</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_echarts_data</span><span class="hljs-params">(tree)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 当前顶点的dict</span></span><br><span class="line">    value = dict()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 如果传入的tree已经是叶子结点了</span></span><br><span class="line">    <span class="hljs-keyword">if</span> tree[<span class="hljs-string">'is_leaf'</span>] == <span class="hljs-keyword">True</span>:</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 它的value就设置为预测的标记</span></span><br><span class="line">        value[<span class="hljs-string">'value'</span>] = tree[<span class="hljs-string">'prediction'</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 它的名字就叫"label: 标记"</span></span><br><span class="line">        value[<span class="hljs-string">'name'</span>] = <span class="hljs-string">'label: %s'</span>%(tree[<span class="hljs-string">'prediction'</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 直接返回这个dict即可</span></span><br><span class="line">        <span class="hljs-keyword">return</span> value</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 如果传入的tree不是叶子结点，名字就叫当前这个顶点的划分特征，子树是一个list</span></span><br><span class="line">    <span class="hljs-comment"># 分别增加左子树和右子树到children中</span></span><br><span class="line">    value[<span class="hljs-string">'name'</span>] = tree[<span class="hljs-string">'splitting_feature'</span>]</span><br><span class="line">    value[<span class="hljs-string">'children'</span>] = [generate_echarts_data(tree[<span class="hljs-string">'left'</span>]), generate_echarts_data(tree[<span class="hljs-string">'right'</span>])]</span><br><span class="line">    <span class="hljs-keyword">return</span> value</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = generate_echarts_data(my_decision_tree)</span><br></pre></td></tr></table></figure>
<p>使用下面的代码进行绘制，绘制完成后，树的结点是可以点击的，点击后会展开它的子树</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tree = Tree(width=<span class="hljs-number">800</span>, height=<span class="hljs-number">400</span>)</span><br><span class="line">tree.add(<span class="hljs-string">""</span>,</span><br><span class="line">         [data],</span><br><span class="line">         tree_collapse_interval=<span class="hljs-number">5</span>,</span><br><span class="line">         tree_top=<span class="hljs-string">"15%"</span>,</span><br><span class="line">         tree_right=<span class="hljs-string">"20%"</span>,</span><br><span class="line">         tree_symbol = <span class="hljs-string">'rect'</span>,</span><br><span class="line">         tree_symbol_size = <span class="hljs-number">20</span>,</span><br><span class="line">         )</span><br><span class="line">tree.render()</span><br><span class="line">tree</span><br></pre></td></tr></table></figure>
<h1 id="第四题：预剪枝"><a href="#第四题：预剪枝" class="headerlink" title="第四题：预剪枝"></a>第四题：预剪枝</h1><h2 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h2><ol>
<li>实现使用信息增益率划分的预剪枝</li>
</ol>
<p>我们会以信息增益率作为划分准则，构造带有预剪枝的二叉决策树<br>使用的数据和第三题一样，剪枝需要使用验证集，所以数据划分策略会和第三题不同</p>
<h2 id="1-读取数据-1"><a href="#1-读取数据-1" class="headerlink" title="1. 读取数据"></a>1. 读取数据</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 导入类库</span></span><br><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 导入数据</span></span><br><span class="line">loans = pd.read_csv(<span class="hljs-string">'data/lendingclub/lending-club-data.csv'</span>, low_memory=<span class="hljs-keyword">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 对数据进行预处理，将safe_loans作为标记</span></span><br><span class="line">loans[<span class="hljs-string">'safe_loans'</span>] = loans[<span class="hljs-string">'bad_loans'</span>].apply(<span class="hljs-keyword">lambda</span> x : +<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x==<span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">-1</span>)</span><br><span class="line"><span class="hljs-keyword">del</span> loans[<span class="hljs-string">'bad_loans'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">features = [<span class="hljs-string">'grade'</span>,              <span class="hljs-comment"># grade of the loan</span></span><br><span class="line">            <span class="hljs-string">'term'</span>,               <span class="hljs-comment"># the term of the loan</span></span><br><span class="line">            <span class="hljs-string">'home_ownership'</span>,     <span class="hljs-comment"># home_ownership status: own, mortgage or rent</span></span><br><span class="line">            <span class="hljs-string">'emp_length'</span>,         <span class="hljs-comment"># number of years of employment</span></span><br><span class="line">           ]</span><br><span class="line">target = <span class="hljs-string">'safe_loans'</span></span><br><span class="line">loans = loans[features + [target]]</span><br></pre></td></tr></table></figure>
<h2 id="2-划分训练集和测试集-1"><a href="#2-划分训练集和测试集-1" class="headerlink" title="2. 划分训练集和测试集"></a>2. 划分训练集和测试集</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.utils <span class="hljs-keyword">import</span> shuffle</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loans = shuffle(loans, random_state = <span class="hljs-number">34</span>)</span><br></pre></td></tr></table></figure>
<p>我们使用数据的60%做训练集，20%做验证集，20%做测试集</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">split_line1 = int(len(loans) * <span class="hljs-number">0.6</span>)</span><br><span class="line">split_line2 = int(len(loans) * <span class="hljs-number">0.8</span>)</span><br><span class="line">train_data = loans.iloc[: split_line1]</span><br><span class="line">validation_data = loans.iloc[split_line1: split_line2]</span><br><span class="line">test_data = loans.iloc[split_line2:]</span><br></pre></td></tr></table></figure>
<h2 id="3-特征预处理-1"><a href="#3-特征预处理-1" class="headerlink" title="3. 特征预处理"></a>3. 特征预处理</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">one_hot_encoding</span><span class="hljs-params">(data, features_categorical)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    Parameter</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    data: pd.DataFrame</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    features_categorical: list(str)</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 对所有的离散特征遍历</span></span><br><span class="line">    <span class="hljs-keyword">for</span> cat <span class="hljs-keyword">in</span> features_categorical:</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 对这列进行one-hot编码，前缀为这个变量名</span></span><br><span class="line">        one_encoding = pd.get_dummies(data[cat], prefix = cat)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 将生成的one-hot编码与之前的dataframe拼接起来</span></span><br><span class="line">        data = pd.concat([data, one_encoding],axis=<span class="hljs-number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 删除掉原始的这列离散特征</span></span><br><span class="line">        <span class="hljs-keyword">del</span> data[cat]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> data</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data = one_hot_encoding(train_data, features)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">one_hot_features = train_data.columns.tolist()</span><br><span class="line">one_hot_features.remove(target)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">validation_tmp = one_hot_encoding(validation_data, features)</span><br><span class="line">validation_data = pd.DataFrame(columns = train_data.columns)</span><br><span class="line"><span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> train_data.columns:</span><br><span class="line">    <span class="hljs-keyword">if</span> feature <span class="hljs-keyword">in</span> validation_tmp:</span><br><span class="line">        validation_data[feature] = validation_tmp[feature].copy()</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        validation_data[feature] = np.zeros(len(validation_tmp), dtype = <span class="hljs-string">'uint8'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test_data_tmp = one_hot_encoding(test_data, features)</span><br><span class="line">test_data = pd.DataFrame(columns = train_data.columns)</span><br><span class="line"><span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> train_data.columns:</span><br><span class="line">    <span class="hljs-keyword">if</span> feature <span class="hljs-keyword">in</span> test_data_tmp.columns:</span><br><span class="line">        test_data[feature] = test_data_tmp[feature].copy()</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        test_data[feature] = np.zeros(test_data_tmp.shape[<span class="hljs-number">0</span>], dtype = <span class="hljs-string">'uint8'</span>)</span><br></pre></td></tr></table></figure>
<p>打印一下3个数据集的shape</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(train_data.shape, validation_data.shape, test_data.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(73564, 25) (24521, 25) (24522, 25)
</code></pre><h1 id="4-实现信息增益率的计算"><a href="#4-实现信息增益率的计算" class="headerlink" title="4. 实现信息增益率的计算"></a>4. 实现信息增益率的计算</h1><p>信息熵：<br>$$<br>\mathrm{Ent}(D) = - \sum^{\vert \mathcal{Y} \vert}_{k = 1} p_k \mathrm{log}_2 p_k<br>$$</p>
<p>信息增益：<br>$$<br>\mathrm{Gain}(D, a) = \mathrm{Ent}(D) - \sum^{V}_{v=1} \frac{\vert D^v \vert}{\vert D \vert} \mathrm{Ent}(D^v)<br>$$</p>
<p>信息增益率：</p>
<p>$$<br>\mathrm{Gain_ratio}(D, a) = \frac{\mathrm{Gain}(D, a)}{\mathrm{IV}(a)}<br>$$</p>
<p>其中</p>
<p>$$<br>\mathrm{IV}(a) = - \sum^V_{v=1} \frac{\vert D^v \vert}{\vert D \vert} \log_2 \frac{\vert D^v \vert}{\vert D \vert}<br>$$</p>
<p>计算信息熵时约定：若$p = 0$，则$p \log_2p = 0$</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">information_entropy</span><span class="hljs-params">(labels_in_node)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    求当前结点的信息熵</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameter</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    labels_in_node: np.ndarray, 如[-1, 1, -1, 1, 1]</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    float: information entropy</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计样本总个数</span></span><br><span class="line">    num_of_samples = labels_in_node.shape[<span class="hljs-number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> num_of_samples == <span class="hljs-number">0</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计出标记为1的个数</span></span><br><span class="line">    num_of_positive = len(labels_in_node[labels_in_node == <span class="hljs-number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计出标记为-1的个数</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    num_of_negative = len(labels_in_node[labels_in_node == <span class="hljs-number">-1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计正例的概率</span></span><br><span class="line">    prob_positive = num_of_positive / num_of_samples</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计负例的概率</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    prob_negative = num_of_negative / num_of_samples</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> prob_positive == <span class="hljs-number">0</span>:</span><br><span class="line">        positive_part = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        positive_part = prob_positive * np.log2(prob_positive)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> prob_negative == <span class="hljs-number">0</span>:</span><br><span class="line">        negative_part = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        negative_part = prob_negative * np.log2(prob_negative)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> - ( positive_part + negative_part )</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 信息熵测试样例1</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])</span><br><span class="line">print(information_entropy(example_labels)) <span class="hljs-comment"># 0.97095</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 信息熵测试样例2</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])</span><br><span class="line">print(information_entropy(example_labels)) <span class="hljs-comment"># 0.86312</span></span><br><span class="line">    </span><br><span class="line"><span class="hljs-comment"># 信息熵测试样例3</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])</span><br><span class="line">print(information_entropy(example_labels)) <span class="hljs-comment"># 0.86312</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 信息熵测试样例4</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">-1</span>] * <span class="hljs-number">9</span> + [<span class="hljs-number">1</span>] * <span class="hljs-number">8</span>)</span><br><span class="line">print(information_entropy(example_labels)) <span class="hljs-comment"># 0.99750</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 信息熵测试样例5</span></span><br><span class="line">example_labels = np.array([<span class="hljs-number">1</span>] * <span class="hljs-number">8</span>)</span><br><span class="line">print(information_entropy(example_labels)) <span class="hljs-comment"># 0</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 信息熵测试样例6</span></span><br><span class="line">example_labels = np.array([])</span><br><span class="line">print(information_entropy(example_labels)) <span class="hljs-comment"># 0</span></span><br></pre></td></tr></table></figure>
<pre><code>0.970950594455
0.863120568567
0.863120568567
0.997502546369
-0.0
0
</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_information_gain_ratios</span><span class="hljs-params">(data, features, target, annotate = False)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    计算所有特征的信息增益率并保存起来</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameter</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    data: pd.DataFrame, 带有特征和标记的数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    features: list(str)，特征名组成的list</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    target: str， 特征的名字</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    annotate: boolean, default False，是否打印注释</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    gain_ratios: dict, key: str, 特征名</span></span><br><span class="line"><span class="hljs-string">                       value: float，信息增益率</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    gain_ratios = dict()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 对所有的特征进行遍历，使用当前的划分方法对每个特征进行计算</span></span><br><span class="line">    <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features:</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 左子树保证所有的样本的这个特征取值为0</span></span><br><span class="line">        left_split_target = data[data[feature] == <span class="hljs-number">0</span>][target]</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 右子树保证所有的样本的这个特征取值为1</span></span><br><span class="line">        right_split_target =  data[data[feature] == <span class="hljs-number">1</span>][target]</span><br><span class="line">            </span><br><span class="line">        <span class="hljs-comment"># 计算左子树的信息熵</span></span><br><span class="line">        left_entropy = information_entropy(left_split_target)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算左子树的权重</span></span><br><span class="line">        left_weight = len(left_split_target) / (len(left_split_target) + len(right_split_target))</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># 计算右子树的信息熵</span></span><br><span class="line">        right_entropy = information_entropy(right_split_target)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算右子树的权重</span></span><br><span class="line">        right_weight = len(right_split_target) / (len(left_split_target) + len(right_split_target))</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算当前结点的信息熵</span></span><br><span class="line">        current_entropy = information_entropy(data[target])</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算当前结点的信息增益</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        gain = current_entropy - (left_weight * left_entropy + right_weight * right_entropy)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算左子树的IV</span></span><br><span class="line">        <span class="hljs-keyword">if</span> left_weight == <span class="hljs-number">0</span>:</span><br><span class="line">            left_IV = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">            left_IV = left_weight * np.log2(left_weight)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算右子树的IV</span></span><br><span class="line">        <span class="hljs-keyword">if</span> right_weight == <span class="hljs-number">0</span>:</span><br><span class="line">            right_IV = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">            right_IV = right_weight * np.log2(right_weight)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># IV 等于所有子树IV之和的相反数</span></span><br><span class="line">        IV = - (left_IV + right_IV)</span><br><span class="line">            </span><br><span class="line">        <span class="hljs-comment"># 计算使用当前特征划分的信息增益率</span></span><br><span class="line">        <span class="hljs-comment"># 这里为了防止IV是0，导致除法得到np.inf，在分母加了一个很小的小数</span></span><br><span class="line">        gain_ratio = gain / (IV + np.finfo(np.longdouble).eps)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 信息增益率的存储</span></span><br><span class="line">        gain_ratios[feature] = gain_ratio</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-keyword">if</span> annotate:</span><br><span class="line">            print(<span class="hljs-string">" "</span>, feature, gain_ratio)</span><br><span class="line">            </span><br><span class="line">    <span class="hljs-keyword">return</span> gain_ratios</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 信息增益率测试样例1</span></span><br><span class="line">print(compute_information_gain_ratios(train_data, one_hot_features, target)[<span class="hljs-string">'grade_A'</span>]) <span class="hljs-comment"># 0.02573</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 信息增益率测试样例2</span></span><br><span class="line">print(compute_information_gain_ratios(train_data, one_hot_features, target)[<span class="hljs-string">'grade_B'</span>]) <span class="hljs-comment"># 0.00418</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 信息增益率测试样例3</span></span><br><span class="line">print(compute_information_gain_ratios(train_data, one_hot_features, target)[<span class="hljs-string">'term_ 60 months'</span>]) <span class="hljs-comment"># 0.01971</span></span><br></pre></td></tr></table></figure>
<pre><code>0.025734780668
0.00417549506943
0.0197093627186
</code></pre><h2 id="5-完成最优特征的选择-1"><a href="#5-完成最优特征的选择-1" class="headerlink" title="5. 完成最优特征的选择"></a>5. 完成最优特征的选择</h2><p>这里我们没有实现信息增益和基尼指数的最优特征求解，感兴趣的同学可以按上一题实现</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">best_splitting_feature</span><span class="hljs-params">(data, features, target, criterion = <span class="hljs-string">'gain_ratios'</span>, annotate = False)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    给定划分方法和数据，找到最优的划分特征</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    data: pd.DataFrame, 带有特征和标记的数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    features: list(str)，特征名组成的list</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    target: str， 特征的名字</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    criterion: str, 使用哪种指标，三种选项: 'information_gain', 'gain_ratio', 'gini'</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    annotate: boolean, default False，是否打印注释</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    best_feature: str, 最佳的划分特征的名字</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-keyword">if</span> criterion == <span class="hljs-string">'information_gain'</span>:</span><br><span class="line">        <span class="hljs-keyword">if</span> annotate:</span><br><span class="line">            print(<span class="hljs-string">'using information gain'</span>)</span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">elif</span> criterion == <span class="hljs-string">'gain_ratio'</span>:</span><br><span class="line">        <span class="hljs-keyword">if</span> annotate:</span><br><span class="line">            print(<span class="hljs-string">'using information gain ratio'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 得到当前所有特征的信息增益率</span></span><br><span class="line">        gain_ratios = compute_information_gain_ratios(data, features, target, annotate)</span><br><span class="line">    </span><br><span class="line">        <span class="hljs-comment"># 根据这些特征和他们的信息增益率，找到最佳的划分特征</span></span><br><span class="line">        best_feature = max(gain_ratios.items(), key = <span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>])[<span class="hljs-number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">return</span> best_feature</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">elif</span> criterion == <span class="hljs-string">'gini'</span>:</span><br><span class="line">        <span class="hljs-keyword">if</span> annotate:</span><br><span class="line">            print(<span class="hljs-string">'using gini'</span>)</span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-keyword">None</span></span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">"传入的criterion不合规!"</span>, criterion)</span><br></pre></td></tr></table></figure>
<h2 id="6-判断结点内样本的类别是否为同一类-1"><a href="#6-判断结点内样本的类别是否为同一类-1" class="headerlink" title="6. 判断结点内样本的类别是否为同一类"></a>6. 判断结点内样本的类别是否为同一类</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">intermediate_node_num_mistakes</span><span class="hljs-params">(labels_in_node)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    求树的结点中，样本数少的那个类的样本有多少，比如输入是[1, 1, -1, -1, 1]，返回2</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameter</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    labels_in_node: np.ndarray, pd.Series</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    int：个数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 如果传入的array为空，返回0</span></span><br><span class="line">    <span class="hljs-keyword">if</span> len(labels_in_node) == <span class="hljs-number">0</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计1的个数</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    num_of_one = len(labels_in_node[labels_in_node == <span class="hljs-number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计-1的个数</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    num_of_minus_one = len(labels_in_node[labels_in_node == <span class="hljs-number">-1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> num_of_one <span class="hljs-keyword">if</span> num_of_minus_one &gt; num_of_one <span class="hljs-keyword">else</span> num_of_minus_one</span><br></pre></td></tr></table></figure>
<h1 id="7-创建叶子结点-1"><a href="#7-创建叶子结点-1" class="headerlink" title="7. 创建叶子结点"></a>7. 创建叶子结点</h1><p>先编写一个辅助函数majority_class，求树的结点中，样本数多的那个类是什么</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">majority_class</span><span class="hljs-params">(labels_in_node)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">        求树的结点中，样本数多的那个类是什么</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 如果传入的array为空，返回0</span></span><br><span class="line">    <span class="hljs-keyword">if</span> len(labels_in_node) == <span class="hljs-number">0</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计1的个数</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    num_of_one = len(labels_in_node[labels_in_node == <span class="hljs-number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 统计-1的个数</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    num_of_minus_one = len(labels_in_node[labels_in_node == <span class="hljs-number">-1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> num_of_minus_one &lt; num_of_one <span class="hljs-keyword">else</span> <span class="hljs-number">-1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_leaf</span><span class="hljs-params">(target_values)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    计算出当前叶子结点的标记是什么，并且将叶子结点信息保存在一个dict中</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameter:</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    target_values: pd.Series, 当前叶子结点内样本的标记</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    leaf: dict，表示一个叶结点，</span></span><br><span class="line"><span class="hljs-string">            leaf['splitting_features'], None，叶结点不需要划分特征</span></span><br><span class="line"><span class="hljs-string">            leaf['left'], None，叶结点没有左子树</span></span><br><span class="line"><span class="hljs-string">            leaf['right'], None，叶结点没有右子树</span></span><br><span class="line"><span class="hljs-string">            leaf['is_leaf'], True, 是否是叶子结点</span></span><br><span class="line"><span class="hljs-string">            leaf['prediction'], int, 表示该叶子结点的预测值</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 创建叶子结点</span></span><br><span class="line">    leaf = &#123;<span class="hljs-string">'splitting_feature'</span> : <span class="hljs-keyword">None</span>,</span><br><span class="line">            <span class="hljs-string">'left'</span> : <span class="hljs-keyword">None</span>,</span><br><span class="line">            <span class="hljs-string">'right'</span> : <span class="hljs-keyword">None</span>,</span><br><span class="line">            <span class="hljs-string">'is_leaf'</span>: <span class="hljs-keyword">True</span>&#125;</span><br><span class="line">   </span><br><span class="line">    <span class="hljs-comment"># 数结点内-1和+1的个数</span></span><br><span class="line">    num_ones = len(target_values[target_values == +<span class="hljs-number">1</span>])</span><br><span class="line">    num_minus_ones = len(target_values[target_values == <span class="hljs-number">-1</span>])    </span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 叶子结点的标记使用少数服从多数的原则，为样本数多的那类的标记，保存在 leaf['prediction']</span></span><br><span class="line">    leaf[<span class="hljs-string">'prediction'</span>] = majority_class(target_values)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 返回叶子结点</span></span><br><span class="line">    <span class="hljs-keyword">return</span> leaf</span><br></pre></td></tr></table></figure>
<h2 id="8-递归地创建决策树-1"><a href="#8-递归地创建决策树-1" class="headerlink" title="8. 递归地创建决策树"></a>8. 递归地创建决策树</h2><p>递归的创建决策树<br>决策树终止的三个条件：</p>
<ol>
<li>如果结点内所有的样本的标记都相同，该结点就不需要再继续划分，直接做叶子结点即可</li>
<li>如果结点所有的特征都已经在之前使用过了，在当前结点无剩余特征可供划分样本，该结点直接做叶子结点</li>
<li>如果当前结点的深度已经达到了我们限制的树的最大深度，直接做叶子结点</li>
</ol>
<p>对于预剪枝来说，实质上是增加了第四个终止条件：  </p>
<ol start="4">
<li>如果当前结点划分后，模型的泛化能力没有提升，则不进行划分</li>
</ol>
<p>如何判断泛化能力有没有提升？我们需要使用验证集<br>就像使用训练集递归地划分数据一样，我们在递归地构造决策树时，也需要递归地将验证集进行划分，计算决策树在验证集上的精度</p>
<p>因为我们是递归地对决策树进行划分，所以每次计算验证集上精度是否提升时，也只是针对当前结点内的样本，因为是否对当前结点内的样本进行划分，不会影响它的兄弟结点及兄弟结点的子结点的精度</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">decision_tree_create</span><span class="hljs-params">(training_data, validation_data, features, target, criterion = <span class="hljs-string">'gain_ratios'</span>, pre_pruning = False, current_depth = <span class="hljs-number">0</span>, max_depth = <span class="hljs-number">10</span>, annotate = False)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    Parameter:</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    trianing_data: pd.DataFrame, 数据</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    features: iterable, 特征组成的可迭代对象</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    target: str, 标记的名字</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    criterion: 'str', 特征划分方法</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    current_depth: int, 当前深度</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    max_depth: int, 树的最大深度</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    dict, dict['is_leaf']          : False, 当前顶点不是叶子结点</span></span><br><span class="line"><span class="hljs-string">          dict['prediction']       : None, 不是叶子结点就没有预测值</span></span><br><span class="line"><span class="hljs-string">          dict['splitting_feature']: splitting_feature, 当前结点是使用哪个特征进行划分的</span></span><br><span class="line"><span class="hljs-string">          dict['left']             : dict</span></span><br><span class="line"><span class="hljs-string">          dict['right']            : dict</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-keyword">if</span> criterion <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">'information_gain'</span>, <span class="hljs-string">'gain_ratio'</span>, <span class="hljs-string">'gini'</span>]:</span><br><span class="line">        <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">"传入的criterion不合规!"</span>, criterion)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 复制一份特征，存储起来，每使用一个特征进行划分，我们就删除一个</span></span><br><span class="line">    remaining_features = features[:]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 取出标记值</span></span><br><span class="line">    target_values = training_data[target]</span><br><span class="line">    validation_values = validation_data[target]</span><br><span class="line">    print(<span class="hljs-string">"-"</span> * <span class="hljs-number">50</span>)</span><br><span class="line">    print(<span class="hljs-string">"Subtree, depth = %s (%s data points)."</span> % (current_depth, len(target_values)))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 终止条件1</span></span><br><span class="line">    <span class="hljs-comment"># 如果当前结点内所有样本同属一类，即这个结点中，各类别样本数最小的那个等于0</span></span><br><span class="line">    <span class="hljs-comment"># 使用前面写的intermediate_node_num_mistakes来完成这个判断</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    <span class="hljs-keyword">if</span> intermediate_node_num_mistakes(target_values) == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"Stopping condition 1 reached."</span>)</span><br><span class="line">        <span class="hljs-keyword">return</span> create_leaf(target_values)   <span class="hljs-comment"># 创建叶子结点</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 终止条件2</span></span><br><span class="line">    <span class="hljs-comment"># 如果已经没有剩余的特征可供分割</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    <span class="hljs-keyword">if</span> len(remaining_features) == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"Stopping condition 2 reached."</span>)</span><br><span class="line">        <span class="hljs-keyword">return</span> create_leaf(target_values)   <span class="hljs-comment"># 创建叶子结点</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 终止条件3</span></span><br><span class="line">    <span class="hljs-comment"># 如果已经到达了我们要求的最大深度</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    <span class="hljs-keyword">if</span> current_depth &gt;= max_depth:</span><br><span class="line">        print(<span class="hljs-string">"Reached maximum depth. Stopping for now."</span>)</span><br><span class="line">        <span class="hljs-keyword">return</span> create_leaf(target_values)   <span class="hljs-comment"># 创建叶子结点</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 找到最优划分特征</span></span><br><span class="line">    <span class="hljs-comment"># 使用best_splitting_feature这个函数</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    splitting_feature = best_splitting_feature(training_data, features, target, criterion, annotate)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 使用我们找到的最优特征将数据划分成两份</span></span><br><span class="line">    <span class="hljs-comment"># 左子树的数据</span></span><br><span class="line">    left_split = training_data[training_data[splitting_feature] == <span class="hljs-number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 右子树的数据</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    right_split = training_data[training_data[splitting_feature] == <span class="hljs-number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 使用这个最优特征对验证集进行划分</span></span><br><span class="line">    validation_left_split = validation_data[validation_data[splitting_feature] == <span class="hljs-number">0</span>]</span><br><span class="line">    validation_right_split = validation_data[validation_data[splitting_feature] == <span class="hljs-number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 如果使用预剪枝，需要判断在验证集上的精度是否提升了</span></span><br><span class="line">    <span class="hljs-keyword">if</span> pre_pruning:</span><br><span class="line">        <span class="hljs-comment"># 首先计算不划分的时候的在验证集上的精度，也就是当前结点为叶子结点</span></span><br><span class="line">        <span class="hljs-comment"># 统计当前结点样本中，样本数多的那个类(majority class)</span></span><br><span class="line">        true_class = majority_class(target_values)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># 判断验证集在不划分时的精度，分母加eps是因为，有可能在划分的时候，验证集的样本数为0</span></span><br><span class="line">        acc_without_splitting = len(validation_values[validation_values == true_class]) / (len(validation_values) + np.finfo(np.longdouble).eps)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># 对当前结点进行划分，统计划分后，左子树的majority class</span></span><br><span class="line">        left_true_class = majority_class(left_split[target])</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># 对当前结点进行划分，统计右子树的majority class</span></span><br><span class="line">        right_true_class = majority_class(right_split[target])</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># 统计验证集左子树中有多少样本是左子树的majority class</span></span><br><span class="line">        vali_left_num_of_majority = len(validation_left_split[validation_left_split[target] == left_true_class])</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># 统计验证集右子树中有多少样本是右子树的majority class</span></span><br><span class="line">        vali_right_num_of_majority = len(validation_right_split[validation_right_split[target] == right_true_class])</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># 计算划分后的精度</span></span><br><span class="line">        acc_with_splitting = (vali_left_num_of_majority + vali_right_num_of_majority) / (len(validation_data) + np.finfo(np.longdouble).eps)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">if</span> annotate == <span class="hljs-keyword">True</span>:</span><br><span class="line">            print(<span class="hljs-string">'acc before splitting: %.3f'</span>%(acc_without_splitting))</span><br><span class="line">            print(<span class="hljs-string">'acc after splitting: %.3f'</span>%(acc_with_splitting))</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># 如果划分后的精度小于等于划分前的精度，那就不划分，当前结点直接变成叶子结点</span></span><br><span class="line">        <span class="hljs-comment"># 否则继续划分，创建左右子树</span></span><br><span class="line">        <span class="hljs-keyword">if</span> acc_with_splitting &lt; acc_without_splitting:</span><br><span class="line">            print(<span class="hljs-string">'Pre-Pruning'</span>)</span><br><span class="line">            <span class="hljs-keyword">return</span> create_leaf(target_values)   <span class="hljs-comment"># 创建叶子结点</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 从剩余特征中删除掉当前这个特征</span></span><br><span class="line">    remaining_features.remove(splitting_feature)</span><br><span class="line">    </span><br><span class="line">    print(<span class="hljs-string">"Split on feature %s. (%s, %s)"</span> % (\</span><br><span class="line">                      splitting_feature, len(left_split), len(right_split)))</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 如果使用当前的特征，将所有的样本都划分到一棵子树中，那么就直接将这棵子树变成叶子结点</span></span><br><span class="line">    <span class="hljs-comment"># 判断左子树是不是“完美”的</span></span><br><span class="line">    <span class="hljs-keyword">if</span> len(left_split) == len(training_data):</span><br><span class="line">        print(<span class="hljs-string">"Creating leaf node."</span>)</span><br><span class="line">        <span class="hljs-keyword">return</span> create_leaf(left_split[target])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 判断右子树是不是“完美”的</span></span><br><span class="line">    <span class="hljs-keyword">if</span> len(right_split) == len(training_data):</span><br><span class="line">        print(<span class="hljs-string">"Creating right node."</span>)</span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        <span class="hljs-keyword">return</span> create_leaf(right_split[target])</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 递归地创建左子树，需要传入验证集的左子树</span></span><br><span class="line">    left_tree = decision_tree_create(left_split, validation_left_split, remaining_features, target, criterion, pre_pruning, current_depth + <span class="hljs-number">1</span>, max_depth, annotate)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 递归地创建右子树，需要传入验证集的右子树</span></span><br><span class="line">    <span class="hljs-comment">## YOUR CODE HERE</span></span><br><span class="line">    right_tree = decision_tree_create(right_split, validation_right_split, remaining_features, target, criterion, pre_pruning, current_depth + <span class="hljs-number">1</span>, max_depth, annotate) </span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">'is_leaf'</span>          : <span class="hljs-keyword">False</span>, </span><br><span class="line">            <span class="hljs-string">'prediction'</span>       : <span class="hljs-keyword">None</span>,</span><br><span class="line">            <span class="hljs-string">'splitting_feature'</span>: splitting_feature,</span><br><span class="line">            <span class="hljs-string">'left'</span>             : left_tree, </span><br><span class="line">            <span class="hljs-string">'right'</span>            : right_tree&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree_without_pre_pruning = decision_tree_create(train_data, validation_data, one_hot_features, target, criterion = <span class="hljs-string">'gain_ratio'</span>, pre_pruning = <span class="hljs-keyword">False</span>, current_depth = <span class="hljs-number">0</span>, max_depth = <span class="hljs-number">6</span>, annotate = <span class="hljs-keyword">False</span>)</span><br></pre></td></tr></table></figure>
<pre><code>--------------------------------------------------
Subtree, depth = 0 (73564 data points).
Split on feature grade_F. (71229, 2335)
--------------------------------------------------
Subtree, depth = 1 (71229 data points).
Split on feature grade_A. (57869, 13360)
--------------------------------------------------
Subtree, depth = 2 (57869 data points).
Split on feature grade_G. (57232, 637)
--------------------------------------------------
Subtree, depth = 3 (57232 data points).
Split on feature grade_E. (51828, 5404)
--------------------------------------------------
Subtree, depth = 4 (51828 data points).
Split on feature grade_D. (40326, 11502)
--------------------------------------------------
Subtree, depth = 5 (40326 data points).
Split on feature term_ 36 months. (5760, 34566)
--------------------------------------------------
Subtree, depth = 6 (5760 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (34566 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (11502 data points).
Split on feature term_ 36 months. (3315, 8187)
--------------------------------------------------
Subtree, depth = 6 (3315 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (8187 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 4 (5404 data points).
Split on feature term_ 36 months. (3185, 2219)
--------------------------------------------------
Subtree, depth = 5 (3185 data points).
Split on feature home_ownership_OTHER. (3184, 1)
--------------------------------------------------
Subtree, depth = 6 (3184 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (1 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 5 (2219 data points).
Split on feature emp_length_1 year. (2011, 208)
--------------------------------------------------
Subtree, depth = 6 (2011 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (208 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 3 (637 data points).
Split on feature emp_length_3 years. (590, 47)
--------------------------------------------------
Subtree, depth = 4 (590 data points).
Split on feature emp_length_2 years. (541, 49)
--------------------------------------------------
Subtree, depth = 5 (541 data points).
Split on feature home_ownership_OWN. (495, 46)
--------------------------------------------------
Subtree, depth = 6 (495 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (46 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (49 data points).
Split on feature term_ 36 months. (32, 17)
--------------------------------------------------
Subtree, depth = 6 (32 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (17 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 4 (47 data points).
Split on feature home_ownership_OTHER. (46, 1)
--------------------------------------------------
Subtree, depth = 5 (46 data points).
Split on feature home_ownership_OWN. (44, 2)
--------------------------------------------------
Subtree, depth = 6 (44 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (2 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (1 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 2 (13360 data points).
Split on feature term_ 36 months. (259, 13101)
--------------------------------------------------
Subtree, depth = 3 (259 data points).
Split on feature emp_length_9 years. (252, 7)
--------------------------------------------------
Subtree, depth = 4 (252 data points).
Split on feature home_ownership_RENT. (202, 50)
--------------------------------------------------
Subtree, depth = 5 (202 data points).
Split on feature emp_length_8 years. (192, 10)
--------------------------------------------------
Subtree, depth = 6 (192 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (10 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 5 (50 data points).
Split on feature emp_length_4 years. (48, 2)
--------------------------------------------------
Subtree, depth = 6 (48 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (2 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 4 (7 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 3 (13101 data points).
Split on feature home_ownership_MORTGAGE. (5830, 7271)
--------------------------------------------------
Subtree, depth = 4 (5830 data points).
Split on feature emp_length_7 years. (5592, 238)
--------------------------------------------------
Subtree, depth = 5 (5592 data points).
Split on feature emp_length_3 years. (5045, 547)
--------------------------------------------------
Subtree, depth = 6 (5045 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (547 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (238 data points).
Split on feature home_ownership_OWN. (184, 54)
--------------------------------------------------
Subtree, depth = 6 (184 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (54 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 4 (7271 data points).
Split on feature emp_length_2 years. (6702, 569)
--------------------------------------------------
Subtree, depth = 5 (6702 data points).
Split on feature emp_length_4 years. (6234, 468)
--------------------------------------------------
Subtree, depth = 6 (6234 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (468 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (569 data points).
Split on feature grade_B. (569, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 1 (2335 data points).
Split on feature emp_length_7 years. (2197, 138)
--------------------------------------------------
Subtree, depth = 2 (2197 data points).
Split on feature term_ 36 months. (1719, 478)
--------------------------------------------------
Subtree, depth = 3 (1719 data points).
Split on feature home_ownership_OTHER. (1717, 2)
--------------------------------------------------
Subtree, depth = 4 (1717 data points).
Split on feature emp_length_3 years. (1577, 140)
--------------------------------------------------
Subtree, depth = 5 (1577 data points).
Split on feature home_ownership_RENT. (904, 673)
--------------------------------------------------
Subtree, depth = 6 (904 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (673 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (140 data points).
Split on feature home_ownership_RENT. (73, 67)
--------------------------------------------------
Subtree, depth = 6 (73 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (67 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 4 (2 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 3 (478 data points).
Split on feature emp_length_8 years. (460, 18)
--------------------------------------------------
Subtree, depth = 4 (460 data points).
Split on feature emp_length_4 years. (433, 27)
--------------------------------------------------
Subtree, depth = 5 (433 data points).
Split on feature home_ownership_MORTGAGE. (287, 146)
--------------------------------------------------
Subtree, depth = 6 (287 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (146 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (27 data points).
Split on feature home_ownership_OWN. (25, 2)
--------------------------------------------------
Subtree, depth = 6 (25 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (2 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 4 (18 data points).
Split on feature home_ownership_OWN. (17, 1)
--------------------------------------------------
Subtree, depth = 5 (17 data points).
Split on feature home_ownership_MORTGAGE. (11, 6)
--------------------------------------------------
Subtree, depth = 6 (11 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (6 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (1 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 2 (138 data points).
Split on feature term_ 36 months. (109, 29)
--------------------------------------------------
Subtree, depth = 3 (109 data points).
Split on feature home_ownership_RENT. (51, 58)
--------------------------------------------------
Subtree, depth = 4 (51 data points).
Split on feature home_ownership_MORTGAGE. (8, 43)
--------------------------------------------------
Subtree, depth = 5 (8 data points).
Split on feature grade_A. (8, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 5 (43 data points).
Split on feature grade_A. (43, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 4 (58 data points).
Split on feature grade_A. (58, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 3 (29 data points).
Split on feature home_ownership_OWN. (25, 4)
--------------------------------------------------
Subtree, depth = 4 (25 data points).
Split on feature home_ownership_MORTGAGE. (13, 12)
--------------------------------------------------
Subtree, depth = 5 (13 data points).
Split on feature grade_A. (13, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 5 (12 data points).
Split on feature grade_A. (12, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 4 (4 data points).
Split on feature grade_A. (4, 0)
Creating leaf node.
</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree_with_pre_pruning = decision_tree_create(train_data, validation_data, one_hot_features, target, criterion = <span class="hljs-string">"gain_ratio"</span>, pre_pruning = <span class="hljs-keyword">True</span>, current_depth = <span class="hljs-number">0</span>, max_depth = <span class="hljs-number">6</span>, annotate = <span class="hljs-keyword">False</span>)</span><br></pre></td></tr></table></figure>
<pre><code>--------------------------------------------------
Subtree, depth = 0 (73564 data points).
Split on feature grade_F. (71229, 2335)
--------------------------------------------------
Subtree, depth = 1 (71229 data points).
Split on feature grade_A. (57869, 13360)
--------------------------------------------------
Subtree, depth = 2 (57869 data points).
Split on feature grade_G. (57232, 637)
--------------------------------------------------
Subtree, depth = 3 (57232 data points).
Split on feature grade_E. (51828, 5404)
--------------------------------------------------
Subtree, depth = 4 (51828 data points).
Split on feature grade_D. (40326, 11502)
--------------------------------------------------
Subtree, depth = 5 (40326 data points).
Split on feature term_ 36 months. (5760, 34566)
--------------------------------------------------
Subtree, depth = 6 (5760 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (34566 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (11502 data points).
Split on feature term_ 36 months. (3315, 8187)
--------------------------------------------------
Subtree, depth = 6 (3315 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (8187 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 4 (5404 data points).
Split on feature term_ 36 months. (3185, 2219)
--------------------------------------------------
Subtree, depth = 5 (3185 data points).
Split on feature home_ownership_OTHER. (3184, 1)
--------------------------------------------------
Subtree, depth = 6 (3184 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (1 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 5 (2219 data points).
Split on feature emp_length_1 year. (2011, 208)
--------------------------------------------------
Subtree, depth = 6 (2011 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (208 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 3 (637 data points).
Split on feature emp_length_3 years. (590, 47)
--------------------------------------------------
Subtree, depth = 4 (590 data points).
Split on feature emp_length_2 years. (541, 49)
--------------------------------------------------
Subtree, depth = 5 (541 data points).
Pre-Pruning
--------------------------------------------------
Subtree, depth = 5 (49 data points).
Split on feature term_ 36 months. (32, 17)
--------------------------------------------------
Subtree, depth = 6 (32 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (17 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 4 (47 data points).
Split on feature home_ownership_OTHER. (46, 1)
--------------------------------------------------
Subtree, depth = 5 (46 data points).
Split on feature home_ownership_OWN. (44, 2)
--------------------------------------------------
Subtree, depth = 6 (44 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (2 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (1 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 2 (13360 data points).
Split on feature term_ 36 months. (259, 13101)
--------------------------------------------------
Subtree, depth = 3 (259 data points).
Split on feature emp_length_9 years. (252, 7)
--------------------------------------------------
Subtree, depth = 4 (252 data points).
Split on feature home_ownership_RENT. (202, 50)
--------------------------------------------------
Subtree, depth = 5 (202 data points).
Split on feature emp_length_8 years. (192, 10)
--------------------------------------------------
Subtree, depth = 6 (192 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (10 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 5 (50 data points).
Split on feature emp_length_4 years. (48, 2)
--------------------------------------------------
Subtree, depth = 6 (48 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (2 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 4 (7 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 3 (13101 data points).
Split on feature home_ownership_MORTGAGE. (5830, 7271)
--------------------------------------------------
Subtree, depth = 4 (5830 data points).
Split on feature emp_length_7 years. (5592, 238)
--------------------------------------------------
Subtree, depth = 5 (5592 data points).
Split on feature emp_length_3 years. (5045, 547)
--------------------------------------------------
Subtree, depth = 6 (5045 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (547 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (238 data points).
Split on feature home_ownership_OWN. (184, 54)
--------------------------------------------------
Subtree, depth = 6 (184 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (54 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 4 (7271 data points).
Split on feature emp_length_2 years. (6702, 569)
--------------------------------------------------
Subtree, depth = 5 (6702 data points).
Split on feature emp_length_4 years. (6234, 468)
--------------------------------------------------
Subtree, depth = 6 (6234 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (468 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (569 data points).
Split on feature grade_B. (569, 0)
Creating leaf node.
--------------------------------------------------
Subtree, depth = 1 (2335 data points).
Split on feature emp_length_7 years. (2197, 138)
--------------------------------------------------
Subtree, depth = 2 (2197 data points).
Split on feature term_ 36 months. (1719, 478)
--------------------------------------------------
Subtree, depth = 3 (1719 data points).
Split on feature home_ownership_OTHER. (1717, 2)
--------------------------------------------------
Subtree, depth = 4 (1717 data points).
Split on feature emp_length_3 years. (1577, 140)
--------------------------------------------------
Subtree, depth = 5 (1577 data points).
Split on feature home_ownership_RENT. (904, 673)
--------------------------------------------------
Subtree, depth = 6 (904 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (673 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (140 data points).
Split on feature home_ownership_RENT. (73, 67)
--------------------------------------------------
Subtree, depth = 6 (73 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (67 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 4 (2 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 3 (478 data points).
Split on feature emp_length_8 years. (460, 18)
--------------------------------------------------
Subtree, depth = 4 (460 data points).
Pre-Pruning
--------------------------------------------------
Subtree, depth = 4 (18 data points).
Split on feature home_ownership_OWN. (17, 1)
--------------------------------------------------
Subtree, depth = 5 (17 data points).
Split on feature home_ownership_MORTGAGE. (11, 6)
--------------------------------------------------
Subtree, depth = 6 (11 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 6 (6 data points).
Reached maximum depth. Stopping for now.
--------------------------------------------------
Subtree, depth = 5 (1 data points).
Stopping condition 1 reached.
--------------------------------------------------
Subtree, depth = 2 (138 data points).
Pre-Pruning
</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">classify</span><span class="hljs-params">(tree, x, annotate = False)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    递归的进行预测，一次只能预测一个样本</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    tree: dict</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    x: pd.Series，样本</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    x: pd.DataFrame, 待预测的样本</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    annotate, boolean, 是否显示注释</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    返回预测的标记</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-keyword">if</span> tree[<span class="hljs-string">'is_leaf'</span>]:</span><br><span class="line">        <span class="hljs-keyword">if</span> annotate:</span><br><span class="line">            <span class="hljs-keyword">print</span> (<span class="hljs-string">"At leaf, predicting %s"</span> % tree[<span class="hljs-string">'prediction'</span>])</span><br><span class="line">        <span class="hljs-keyword">return</span> tree[<span class="hljs-string">'prediction'</span>]</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        split_feature_value = x[tree[<span class="hljs-string">'splitting_feature'</span>]]</span><br><span class="line">        <span class="hljs-keyword">if</span> annotate:</span><br><span class="line">             <span class="hljs-keyword">print</span> (<span class="hljs-string">"Split on %s = %s"</span> % (tree[<span class="hljs-string">'splitting_feature'</span>], split_feature_value))</span><br><span class="line">        <span class="hljs-keyword">if</span> split_feature_value == <span class="hljs-number">0</span>:</span><br><span class="line">            <span class="hljs-keyword">return</span> classify(tree[<span class="hljs-string">'left'</span>], x, annotate)</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            <span class="hljs-keyword">return</span> classify(tree[<span class="hljs-string">'right'</span>], x, annotate)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(tree, data)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    按行遍历data，对每个样本进行预测，将值存储起来，最后返回np.ndarray</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameter</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    tree, dict, 模型</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    data, pd.DataFrame, 数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    predictions, np.ndarray, 模型对这些样本的预测结果</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    predictions = np.zeros(len(data))</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(data)):</span><br><span class="line">        predictions[i] = classify(tree, data.iloc[i])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> predictions</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score</span><br></pre></td></tr></table></figure>
<p>不预剪枝的决策树在测试集上的精度</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy_score(predict(tree_without_pre_pruning, test_data), test_data[target])</span><br></pre></td></tr></table></figure>
<pre><code>0.80882472881494172
</code></pre><p>预剪枝的决策树在测试集上的精度</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy_score(predict(tree_with_pre_pruning, test_data), test_data[target])</span><br></pre></td></tr></table></figure>
<pre><code>0.80939564472718373
</code></pre><h2 id="test-在下方计算出带有预剪枝和不带预剪枝的决策树的精度，查准率，查全率和F1值-最大深度为6，使用信息增益率-，保留4位小数，四舍五入"><a href="#test-在下方计算出带有预剪枝和不带预剪枝的决策树的精度，查准率，查全率和F1值-最大深度为6，使用信息增益率-，保留4位小数，四舍五入" class="headerlink" title="test 在下方计算出带有预剪枝和不带预剪枝的决策树的精度，查准率，查全率和F1值(最大深度为6，使用信息增益率)，保留4位小数，四舍五入"></a>test 在下方计算出带有预剪枝和不带预剪枝的决策树的精度，查准率，查全率和F1值(最大深度为6，使用信息增益率)，保留4位小数，四舍五入</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> precision_score</span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> recall_score</span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> f1_score</span><br><span class="line">print(precision_score(predict(tree_without_pre_pruning, test_data), test_data[target]))</span><br><span class="line">print(precision_score(predict(tree_with_pre_pruning, test_data), test_data[target]))</span><br><span class="line">print(recall_score(predict(tree_without_pre_pruning, test_data), test_data[target]))</span><br><span class="line">print(recall_score(predict(tree_with_pre_pruning, test_data), test_data[target]))</span><br><span class="line">print(f1_score(predict(tree_without_pre_pruning, test_data), test_data[target]))</span><br><span class="line">print(f1_score(predict(tree_with_pre_pruning, test_data), test_data[target]))</span><br></pre></td></tr></table></figure>
<pre><code>0.998438365825
0.999848874112
0.809739755689
0.809494677597
0.894242916441
0.894658553076
</code></pre><h6 id="双击此处编辑"><a href="#双击此处编辑" class="headerlink" title="双击此处编辑"></a>双击此处编辑</h6><table>
<thead>
<tr>
<th>模型</th>
<th>精度</th>
<th>查准率</th>
<th>查全率</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>有预剪枝</td>
<td>0.8094</td>
<td>0.9998</td>
<td>0.8095</td>
<td>0.8947</td>
</tr>
<tr>
<td>无预剪枝</td>
<td>0.8088</td>
<td>0.9984</td>
<td>0.8097</td>
<td>0.8942</td>
</tr>
</tbody>
</table>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> pyecharts <span class="hljs-keyword">import</span> Tree</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_echarts_data</span><span class="hljs-params">(tree)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 当前顶点的dict</span></span><br><span class="line">    value = dict()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 如果传入的tree已经是叶子结点了</span></span><br><span class="line">    <span class="hljs-keyword">if</span> tree[<span class="hljs-string">'is_leaf'</span>] == <span class="hljs-keyword">True</span>:</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 它的value就设置为预测的标记</span></span><br><span class="line">        value[<span class="hljs-string">'value'</span>] = tree[<span class="hljs-string">'prediction'</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 它的名字就叫"label: 标记"</span></span><br><span class="line">        value[<span class="hljs-string">'name'</span>] = <span class="hljs-string">'label: %s'</span>%(tree[<span class="hljs-string">'prediction'</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 直接返回这个dict即可</span></span><br><span class="line">        <span class="hljs-keyword">return</span> value</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 如果传入的tree不是叶子结点，名字就叫当前这个顶点的划分特征，子树是一个list</span></span><br><span class="line">    <span class="hljs-comment"># 分别增加左子树和右子树到children中</span></span><br><span class="line">    value[<span class="hljs-string">'name'</span>] = tree[<span class="hljs-string">'splitting_feature'</span>]</span><br><span class="line">    value[<span class="hljs-string">'children'</span>] = [generate_echarts_data(tree[<span class="hljs-string">'left'</span>]), generate_echarts_data(tree[<span class="hljs-string">'right'</span>])]</span><br><span class="line">    <span class="hljs-keyword">return</span> value</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data1 = generate_echarts_data(tree_without_pre_pruning)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tree = Tree(width=<span class="hljs-number">800</span>, height=<span class="hljs-number">800</span>)</span><br><span class="line">tree.add(<span class="hljs-string">""</span>,</span><br><span class="line">         [data1],</span><br><span class="line">         tree_collapse_interval=<span class="hljs-number">5</span>,</span><br><span class="line">         tree_top=<span class="hljs-string">"15%"</span>,</span><br><span class="line">         tree_right=<span class="hljs-string">"20%"</span>,</span><br><span class="line">         tree_symbol = <span class="hljs-string">'rect'</span>,</span><br><span class="line">         tree_symbol_size = <span class="hljs-number">20</span>,</span><br><span class="line">         )</span><br><span class="line">tree.render()</span><br><span class="line">tree</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data2 = generate_echarts_data(tree_with_pre_pruning)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tree = Tree(width=<span class="hljs-number">800</span>, height=<span class="hljs-number">800</span>)</span><br><span class="line">tree.add(<span class="hljs-string">""</span>,</span><br><span class="line">         [data2],</span><br><span class="line">         tree_collapse_interval=<span class="hljs-number">5</span>,</span><br><span class="line">         tree_top=<span class="hljs-string">"15%"</span>,</span><br><span class="line">         tree_right=<span class="hljs-string">"20%"</span>,</span><br><span class="line">         tree_symbol = <span class="hljs-string">'rect'</span>,</span><br><span class="line">         tree_symbol_size = <span class="hljs-number">20</span>,</span><br><span class="line">         )</span><br><span class="line">tree.render()</span><br><span class="line">tree</span><br></pre></td></tr></table></figure>

        </div>
        
        
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">Like this article? Support the author with</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>Alipay</span>
    <div class="qrcode"><img src="/blog/images/alipay.jpg" alt="Alipay"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>Wechat</span>
    <div class="qrcode"><img src="/blog/images/wx_pay.jpg" alt="Wechat"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/blog/2018/09/17/large-scale-learnable-graph-convolutional-networks/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">Large-Scale Learnable Graph Convolutional Networks</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/blog/2018/09/11/logistic-regression/">
                <span class="level-item">神经网络基础</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>


</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered">
                <div>
                    
                        <figure class="image is-128x128 has-mb-6">
                            <img class="is-rounded" src="/blog/images/avatar.jpg" alt="Davidham">
                        </figure>
                    
                    
                    <p class="is-size-4 is-block">
                        Davidham
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        修电脑的
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>Beijing</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level is-mobile">
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Entradas
                    </p>
                    <p class="title has-text-weight-normal">
                        71
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Categorias
                    </p>
                    <p class="title has-text-weight-normal">
                        8
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Etiquetas
                    </p>
                    <p class="title has-text-weight-normal">
                        34
                    </p>
                </div>
            </div>
        </nav>
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/Davidham3" target="_blank">
                SEGUIR</a>
        </div>
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank" title="Github" href="https://github.com/Davidham3">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" title="Email" href="/blog/chaosong@bjtu.edu.cn">
                
                <i class="fa fa-envelope"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" title="Weibo" href="http://weibo.com/Davidham3">
                
                <i class="fab fa-weibo"></i>
                
            </a>
            
        </div>
        
    </div>
</div>
    
        
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Nube de etiquetas
        </h3>
        <a href="/blog/tags/attention/" style="font-size: 12px;">Attention</a> <a href="/blog/tags/graph/" style="font-size: 19px;">Graph</a> <a href="/blog/tags/hadoop/" style="font-size: 12px;">Hadoop</a> <a href="/blog/tags/kafka/" style="font-size: 10px;">Kafka</a> <a href="/blog/tags/ner/" style="font-size: 10px;">NER</a> <a href="/blog/tags/resnet/" style="font-size: 13px;">ResNet</a> <a href="/blog/tags/stdm/" style="font-size: 10px;">STDM</a> <a href="/blog/tags/sequence/" style="font-size: 10px;">Sequence</a> <a href="/blog/tags/spark/" style="font-size: 11px;">Spark</a> <a href="/blog/tags/spatial-temporal/" style="font-size: 17px;">Spatial-temporal</a> <a href="/blog/tags/time-series/" style="font-size: 16px;">Time Series</a> <a href="/blog/tags/algorithms/" style="font-size: 14px;">algorithms</a> <a href="/blog/tags/computer-vision/" style="font-size: 12px;">computer vision</a> <a href="/blog/tags/dataset/" style="font-size: 11px;">dataset</a> <a href="/blog/tags/deep-learning/" style="font-size: 20px;">deep learning</a> <a href="/blog/tags/graph/" style="font-size: 13px;">graph</a> <a href="/blog/tags/graph-convolutional-network/" style="font-size: 19px;">graph convolutional network</a> <a href="/blog/tags/image-style-transfer/" style="font-size: 11px;">image style transfer</a> <a href="/blog/tags/implicit-feedback/" style="font-size: 10px;">implicit feedback</a> <a href="/blog/tags/language-modeling/" style="font-size: 10px;">language modeling</a> <a href="/blog/tags/large-scale-learning/" style="font-size: 11px;">large-scale learning</a> <a href="/blog/tags/machine-learning/" style="font-size: 18px;">machine learning</a> <a href="/blog/tags/machine-translation/" style="font-size: 10px;">machine translation</a> <a href="/blog/tags/natural-language-processing/" style="font-size: 12px;">natural language processing</a> <a href="/blog/tags/normalization/" style="font-size: 10px;">normalization</a> <a href="/blog/tags/recommender-system/" style="font-size: 12px;">recommender system</a> <a href="/blog/tags/reinforcement-learning/" style="font-size: 10px;">reinforcement learning</a> <a href="/blog/tags/seq2seq/" style="font-size: 10px;">seq2seq</a> <a href="/blog/tags/software/" style="font-size: 17px;">software</a> <a href="/blog/tags/super-resolution/" style="font-size: 11px;">super resolution</a> <a href="/blog/tags/virtual-machine/" style="font-size: 10px;">virtual machine</a> <a href="/blog/tags/vscode/" style="font-size: 10px;">vscode</a> <a href="/blog/tags/已复现/" style="font-size: 15px;">已复现</a> <a href="/blog/tags/随笔/" style="font-size: 10px;">随笔</a>
    </div>
</div>

    
        
<div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Categorias
            </h3>
            <ul class="menu-list">
            <li>
        <a class="level is-marginless" href="/blog/categories/algorithms/">
            <span class="level-start">
                <span class="level-item">algorithms</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">6</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/blog/categories/dataset/">
            <span class="level-start">
                <span class="level-item">dataset</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">2</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/blog/categories/software/">
            <span class="level-start">
                <span class="level-item">software</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">5</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/blog/categories/分布式平台/">
            <span class="level-start">
                <span class="level-item">分布式平台</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">7</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/blog/categories/机器学习/">
            <span class="level-start">
                <span class="level-item">机器学习</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">4</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/blog/categories/自然语言处理/">
            <span class="level-start">
                <span class="level-item">自然语言处理</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/blog/categories/论文阅读笔记/">
            <span class="level-start">
                <span class="level-item">论文阅读笔记</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">41</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/blog/categories/随笔/">
            <span class="level-start">
                <span class="level-item">随笔</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">4</span>
            </span>
        </a></li>
            </ul>
        </div>
    </div>
</div>
    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
            
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Recientes
        </h3>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-07-12T11:57:39.000Z">2019-07-12</time></div>
                    <a href="/blog/2019/07/12/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/" class="has-link-black-ter is-size-6">STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-06-25T08:34:02.000Z">2019-06-25</time></div>
                    <a href="/blog/2019/06/25/self-attention-graph-pooling/" class="has-link-black-ter is-size-6">Self-Attention Graph Pooling</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-05-29T05:37:55.000Z">2019-05-29</time></div>
                    <a href="/blog/2019/05/29/session-based-social-recommendation-via-dynamic-graph-attention-networks/" class="has-link-black-ter is-size-6">Session-based Social Recommendation via Dynamic Graph Attention Networks</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-05-08T08:40:48.000Z">2019-05-08</time></div>
                    <a href="/blog/2019/05/08/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/" class="has-link-black-ter is-size-6">DeepSTN+: Context-aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-04-19T08:40:41.000Z">2019-04-19</time></div>
                    <a href="/blog/2019/04/19/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/" class="has-link-black-ter is-size-6">Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

        
            <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            Archivos
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/07/">
                <span class="level-start">
                    <span class="level-item">July 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/06/">
                <span class="level-start">
                    <span class="level-item">June 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/05/">
                <span class="level-start">
                    <span class="level-item">May 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/04/">
                <span class="level-start">
                    <span class="level-item">April 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">6</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/03/">
                <span class="level-start">
                    <span class="level-item">March 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/02/">
                <span class="level-start">
                    <span class="level-item">February 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/01/">
                <span class="level-start">
                    <span class="level-item">January 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/12/">
                <span class="level-start">
                    <span class="level-item">December 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/10/">
                <span class="level-start">
                    <span class="level-item">October 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/09/">
                <span class="level-start">
                    <span class="level-item">September 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">6</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/08/">
                <span class="level-start">
                    <span class="level-item">August 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/07/">
                <span class="level-start">
                    <span class="level-item">July 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">18</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/06/">
                <span class="level-start">
                    <span class="level-item">June 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">5</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/05/">
                <span class="level-start">
                    <span class="level-item">May 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/04/">
                <span class="level-start">
                    <span class="level-item">April 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/03/">
                <span class="level-start">
                    <span class="level-item">March 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/02/">
                <span class="level-start">
                    <span class="level-item">February 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">6</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/01/">
                <span class="level-start">
                    <span class="level-item">January 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2017/08/">
                <span class="level-start">
                    <span class="level-item">August 2017</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
        
            <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Etiquetas
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/attention/">
                        <span class="tag">Attention</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/graph/">
                        <span class="tag">Graph</span>
                        <span class="tag is-grey">20</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/hadoop/">
                        <span class="tag">Hadoop</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/kafka/">
                        <span class="tag">Kafka</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/ner/">
                        <span class="tag">NER</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/resnet/">
                        <span class="tag">ResNet</span>
                        <span class="tag is-grey">4</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/stdm/">
                        <span class="tag">STDM</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/sequence/">
                        <span class="tag">Sequence</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/spark/">
                        <span class="tag">Spark</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/spatial-temporal/">
                        <span class="tag">Spatial-temporal</span>
                        <span class="tag is-grey">12</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/time-series/">
                        <span class="tag">Time Series</span>
                        <span class="tag is-grey">10</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/algorithms/">
                        <span class="tag">algorithms</span>
                        <span class="tag is-grey">5</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/computer-vision/">
                        <span class="tag">computer vision</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/dataset/">
                        <span class="tag">dataset</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/deep-learning/">
                        <span class="tag">deep learning</span>
                        <span class="tag is-grey">38</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/graph/">
                        <span class="tag">graph</span>
                        <span class="tag is-grey">4</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/graph-convolutional-network/">
                        <span class="tag">graph convolutional network</span>
                        <span class="tag is-grey">20</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/image-style-transfer/">
                        <span class="tag">image style transfer</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/implicit-feedback/">
                        <span class="tag">implicit feedback</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/language-modeling/">
                        <span class="tag">language modeling</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/large-scale-learning/">
                        <span class="tag">large-scale learning</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/machine-learning/">
                        <span class="tag">machine learning</span>
                        <span class="tag is-grey">18</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/machine-translation/">
                        <span class="tag">machine translation</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/natural-language-processing/">
                        <span class="tag">natural language processing</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/normalization/">
                        <span class="tag">normalization</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/recommender-system/">
                        <span class="tag">recommender system</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/reinforcement-learning/">
                        <span class="tag">reinforcement learning</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/seq2seq/">
                        <span class="tag">seq2seq</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/software/">
                        <span class="tag">software</span>
                        <span class="tag is-grey">12</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/super-resolution/">
                        <span class="tag">super resolution</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/virtual-machine/">
                        <span class="tag">virtual machine</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/vscode/">
                        <span class="tag">vscode</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/已复现/">
                        <span class="tag">已复现</span>
                        <span class="tag is-grey">6</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/随笔/">
                        <span class="tag">随笔</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
            </div>
        </div>
    </div>
</div>
        
        </div>
    
</div>

                




<div class="column is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only has-order-3 column-right ">
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Recientes
        </h3>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-07-12T11:57:39.000Z">2019-07-12</time></div>
                    <a href="/blog/2019/07/12/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/" class="has-link-black-ter is-size-6">STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-06-25T08:34:02.000Z">2019-06-25</time></div>
                    <a href="/blog/2019/06/25/self-attention-graph-pooling/" class="has-link-black-ter is-size-6">Self-Attention Graph Pooling</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-05-29T05:37:55.000Z">2019-05-29</time></div>
                    <a href="/blog/2019/05/29/session-based-social-recommendation-via-dynamic-graph-attention-networks/" class="has-link-black-ter is-size-6">Session-based Social Recommendation via Dynamic Graph Attention Networks</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-05-08T08:40:48.000Z">2019-05-08</time></div>
                    <a href="/blog/2019/05/08/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/" class="has-link-black-ter is-size-6">DeepSTN+: Context-aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-04-19T08:40:41.000Z">2019-04-19</time></div>
                    <a href="/blog/2019/04/19/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/" class="has-link-black-ter is-size-6">Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            Archivos
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/07/">
                <span class="level-start">
                    <span class="level-item">July 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/06/">
                <span class="level-start">
                    <span class="level-item">June 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/05/">
                <span class="level-start">
                    <span class="level-item">May 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/04/">
                <span class="level-start">
                    <span class="level-item">April 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">6</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/03/">
                <span class="level-start">
                    <span class="level-item">March 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/02/">
                <span class="level-start">
                    <span class="level-item">February 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/01/">
                <span class="level-start">
                    <span class="level-item">January 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/12/">
                <span class="level-start">
                    <span class="level-item">December 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/10/">
                <span class="level-start">
                    <span class="level-item">October 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/09/">
                <span class="level-start">
                    <span class="level-item">September 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">6</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/08/">
                <span class="level-start">
                    <span class="level-item">August 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/07/">
                <span class="level-start">
                    <span class="level-item">July 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">18</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/06/">
                <span class="level-start">
                    <span class="level-item">June 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">5</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/05/">
                <span class="level-start">
                    <span class="level-item">May 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/04/">
                <span class="level-start">
                    <span class="level-item">April 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/03/">
                <span class="level-start">
                    <span class="level-item">March 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/02/">
                <span class="level-start">
                    <span class="level-item">February 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">6</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/01/">
                <span class="level-start">
                    <span class="level-item">January 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2017/08/">
                <span class="level-start">
                    <span class="level-item">August 2017</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Etiquetas
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/attention/">
                        <span class="tag">Attention</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/graph/">
                        <span class="tag">Graph</span>
                        <span class="tag is-grey">20</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/hadoop/">
                        <span class="tag">Hadoop</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/kafka/">
                        <span class="tag">Kafka</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/ner/">
                        <span class="tag">NER</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/resnet/">
                        <span class="tag">ResNet</span>
                        <span class="tag is-grey">4</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/stdm/">
                        <span class="tag">STDM</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/sequence/">
                        <span class="tag">Sequence</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/spark/">
                        <span class="tag">Spark</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/spatial-temporal/">
                        <span class="tag">Spatial-temporal</span>
                        <span class="tag is-grey">12</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/time-series/">
                        <span class="tag">Time Series</span>
                        <span class="tag is-grey">10</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/algorithms/">
                        <span class="tag">algorithms</span>
                        <span class="tag is-grey">5</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/computer-vision/">
                        <span class="tag">computer vision</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/dataset/">
                        <span class="tag">dataset</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/deep-learning/">
                        <span class="tag">deep learning</span>
                        <span class="tag is-grey">38</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/graph/">
                        <span class="tag">graph</span>
                        <span class="tag is-grey">4</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/graph-convolutional-network/">
                        <span class="tag">graph convolutional network</span>
                        <span class="tag is-grey">20</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/image-style-transfer/">
                        <span class="tag">image style transfer</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/implicit-feedback/">
                        <span class="tag">implicit feedback</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/language-modeling/">
                        <span class="tag">language modeling</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/large-scale-learning/">
                        <span class="tag">large-scale learning</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/machine-learning/">
                        <span class="tag">machine learning</span>
                        <span class="tag is-grey">18</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/machine-translation/">
                        <span class="tag">machine translation</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/natural-language-processing/">
                        <span class="tag">natural language processing</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/normalization/">
                        <span class="tag">normalization</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/recommender-system/">
                        <span class="tag">recommender system</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/reinforcement-learning/">
                        <span class="tag">reinforcement learning</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/seq2seq/">
                        <span class="tag">seq2seq</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/software/">
                        <span class="tag">software</span>
                        <span class="tag is-grey">12</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/super-resolution/">
                        <span class="tag">super resolution</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/virtual-machine/">
                        <span class="tag">virtual machine</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/vscode/">
                        <span class="tag">vscode</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/已复现/">
                        <span class="tag">已复现</span>
                        <span class="tag is-grey">6</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/随笔/">
                        <span class="tag">随笔</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
            </div>
        </div>
    </div>
</div>
    
    
</div>

            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/blog/">
                
                    Davidham&#39;s blog
                
                </a>
                <p class="is-size-7">
                &copy; 2019 Davidham&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Creative Commons" href="https://creativecommons.org/">
                        
                        <i class="fab fa-creative-commons"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/">
                        
                        <i class="fab fa-creative-commons-by"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-Hans");</script>


    
    
    
    <script src="/blog/js/animation.js"></script>
    

    
    
    
    <script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
    <script src="/blog/js/gallery.js" defer></script>
    

    
    

<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


    
    
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>

    
    

<a id="back-to-top" title="Back to Top" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/blog/js/back-to-top.js" defer></script>


    
    

    
    
    
    

    
    
    
    
    
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>
    <script src="/blog/js/clipboard.js" defer></script>
    

    
    
    


<script src="/blog/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Entradas',
                PAGES: 'Pages',
                CATEGORIES: 'Categorias',
                TAGS: 'Etiquetas',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/blog/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/blog/js/insight.js" defer></script>
<link rel="stylesheet" href="/blog/css/search.css">
<link rel="stylesheet" href="/blog/css/insight.css">
    
</body>
</html>