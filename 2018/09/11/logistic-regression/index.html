<!DOCTYPE html>
<html lang="zh">
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
<title>神经网络基础 - Davidham&#39;s blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



    <meta name="description" content="最近给本科生当机器学习课程的助教，给他们出的作业题需要看这些图，懒得放本地了，直接放博客里。发现jupyter导出markdown好方便，放到博客里面正好，改都不用改。">
<meta name="keywords" content="machine learning">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络基础">
<meta property="og:url" content="https://davidham3.github.io/2018/09/11/logistic-regression/index.html">
<meta property="og:site_name" content="Davidham&#39;s blog">
<meta property="og:description" content="最近给本科生当机器学习课程的助教，给他们出的作业题需要看这些图，懒得放本地了，直接放博客里。发现jupyter导出markdown好方便，放到博客里面正好，改都不用改。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://davidham3.github.io/blog/images/og_image.png">
<meta property="og:updated_time" content="2018-12-22T05:39:36.743Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络基础">
<meta name="twitter:description" content="最近给本科生当机器学习课程的助教，给他们出的作业题需要看这些图，懒得放本地了，直接放博客里。发现jupyter导出markdown好方便，放到博客里面正好，改都不用改。">
<meta name="twitter:image" content="https://davidham3.github.io/blog/images/og_image.png">







<link rel="icon" href="/blog/images/favicon.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
    
    <style>body>.footer,body>.navbar,body>.section{opacity:0}</style>
    

    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">
    

    
    

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">


    
    
    
    

<link rel="stylesheet" href="/blog/css/back-to-top.css">


    
    

    
    
    
    

    
    
<link rel="stylesheet" href="/blog/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

    
    
    

    
    
    


<link rel="stylesheet" href="/blog/css/style.css">
</head>
<body class="is-3-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/blog/">
            
                Davidham&#39;s blog
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item" href="/blog/">Home</a>
                
                <a class="navbar-item" href="/blog/archives">Archives</a>
                
                <a class="navbar-item" href="/blog/categories">Categories</a>
                
                <a class="navbar-item" href="/blog/tags">Tags</a>
                
                <a class="navbar-item" href="/blog/about">About</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                
                <a class="navbar-item search" title="Search" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-6-widescreen has-order-2 column-main"><div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-09-11T06:12:30.000Z">2018-09-11</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/blog/categories/机器学习/">机器学习</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    2 hours read (About 14523 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                神经网络基础
            
        </h1>
        <div class="content">
            <p>最近给本科生当机器学习课程的助教，给他们出的作业题需要看这些图，懒得放本地了，直接放博客里。发现jupyter导出markdown好方便，放到博客里面正好，改都不用改。<br><a id="more"></a></p>
<p>原来就想过一个问题，为什么我写出来的神经网络不收敛，loss会像火箭一样直接飞了。后来看了一些教程，发现有人在做梯度下降的时候，把梯度除以了梯度的二范数，我尝试之后发现还真好使了，在实验的时候发现是因为没有对数据集进行归一化，如果所有的数据都是很大的数，那么在反向传播的时候，计算出来的梯度的数量级会很大，这就导致更新得到的参数的数量级也很大，预测出的偏差就更大了，然后循环往复，如果给梯度除以一个梯度的二范数，其实就相当于把梯度的数量级降了，这样就可以训练了。但实际上还是将原始数据归一化比较好，对原始数据归一化还能让梯度下降的方向更多。如果数据都是正数，那下降方向会少很多，下降的时候会出现zig-zag现象。</p>
<h1 id="第二题：神经网络：线性回归"><a href="#第二题：神经网络：线性回归" class="headerlink" title="第二题：神经网络：线性回归"></a>第二题：神经网络：线性回归</h1><p>实验内容：</p>
<ol>
<li>学会梯度下降的基本思想</li>
<li>学会使用梯度下降求解线性回归</li>
<li>了解归一化处理的作用</li>
</ol>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p><img src="/blog/2018/09/11/logistic-regression/Fig0.png" alt="Figure0"></p>
<p>我们来完成最简单的线性回归，上图是一个最简单的神经网络，一个输入层，一个输出层，没有激活函数。<br>我们记输入为$X \in \mathbb{R}^{n \times m}$，输出为$Z \in \mathbb{R}^{n}$。输入包含了$n$个样本，$m$个特征，输出是对这$n$个样本的预测值。<br>输入层到输出层的权重和偏置，我们记为$W \in \mathbb{R}^{m}$和$b \in \mathbb{R}$。<br>输出层没有激活函数，所以上面的神经网络的前向传播过程写为：</p>
<p>$$<br>Z = XW + b<br>$$</p>
<p>我们使用均方误差作为模型的损失函数</p>
<p>$$<br>\mathrm{loss}(y, \hat{y}) = \frac{1}{n} \sum^n_{i=1}(y_i - \hat{y_i})^2<br>$$</p>
<p>我们通过调整参数$W$和$b$来降低均方误差，或者说是以降低均方误差为目标，学习参数$W$和参数$b$。当均方误差下降的时候，我们认为当前的模型的预测值$Z$与真值$y$越来越接近，也就是说模型正在学习如何让自己的预测值变得更准确。</p>
<p>在前面的课程中，我们已经学习了这种线性回归模型可以使用最小二乘法求解，最小二乘法在求解数据量较小的问题的时候很有效，但是最小二乘法的时间复杂度很高，一旦数据量变大，效率很低，实际应用中我们会使用梯度下降等基于梯度的优化算法来求解参数$W$和参数$b$。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降是一种常用的优化算法，通俗来说就是计算出参数的梯度（损失函数对参数的偏导数的导数值），然后将参数减去参数的梯度乘以一个很小的数（下面的公式），来改变参数，然后重新计算损失函数，再次计算梯度，再次进行调整，通过一定次数的迭代，参数就会收敛到最优点附近。</p>
<p>在我们的这个线性回归问题中，我们的参数是$W$和$b$，使用以下的策略更新参数：</p>
<p>$$<br>W := W - \alpha \frac{\partial \mathrm{loss}}{\partial W}<br>$$</p>
<p>$$<br>b := b - \alpha \frac{\partial \mathrm{loss}}{\partial b}<br>$$</p>
<p>其中，$\alpha$ 是学习率，一般设置为0.1，0.01等。</p>
<p>接下来我们会求解损失函数对参数的偏导数。</p>
<p>损失函数MSE记为：</p>
<p>$$<br>\mathrm{loss}(y, Z) = \frac{1}{n} \sum^n_{i = 1} (y_i - Z_i)^2<br>$$</p>
<p>其中，$Z \in \mathbb{R}^{n}$是我们的预测值，也就是神经网络输出层的输出值。这里我们有$n$个样本，实际上是将$n$个样本的预测值与他们的真值相减，取平方后加和。</p>
<p>我们计算损失函数对参数$W$的偏导数，根据链式法则，可以将偏导数拆成两项，分别求解后相乘：</p>
<p><strong>这里我们以矩阵的形式写出推导过程，感兴趣的同学可以尝试使用单个样本进行推到，然后推广到矩阵形式</strong></p>
<p>$$\begin{aligned}<br>\frac{\partial \mathrm{loss}}{\partial W} &amp;= \frac{\partial \mathrm{loss}}{\partial Z} \frac{\partial Z}{\partial W}\\<br>&amp;= - \frac{2}{n} X^\mathrm{T} (y - Z)\\<br>&amp;= \frac{2}{n} X^\mathrm{T} (Z - y)<br>\end{aligned}$$</p>
<p>同理，求解损失函数对参数$b$的偏导数:</p>
<p>$$\begin{aligned}<br>\frac{\partial \mathrm{loss}}{\partial b} &amp;= \frac{\partial \mathrm{loss}}{\partial Z} \frac{\partial Z}{\partial b}\\<br>&amp;= - \frac{2}{n} \sum^n_{i=1}(y_i - Z_i)\\<br>&amp;= \frac{2}{n} \sum^n_{i=1}(Z_i - y_i)<br>\end{aligned}$$</p>
<p><strong>因为参数$b$对每个样本的损失值都有贡献，所以我们需要将所有样本的偏导数都加和。</strong></p>
<p>其中，$\frac{\partial \mathrm{loss}}{\partial W} \in \mathbb{R}^{m}$，$\frac{\partial \mathrm{loss}}{\partial b} \in \mathbb{R}$，求解得到的梯度的维度与参数一致。</p>
<p>完成上式两个梯度的计算后，就可以使用梯度下降法对参数进行更新了。</p>
<p>训练神经网络的基本思路：</p>
<ol>
<li>首先对参数进行初始化，对参数进行随机初始化（也就是取随机值）</li>
<li>将样本输入神经网络，计算神经网络预测值 $Z$</li>
<li>计算损失值MSE</li>
<li>通过 $Z$ 和 $y$ ，以及 $X$ ，计算参数的梯度</li>
<li>使用梯度下降更新参数</li>
<li>循环1-5步，<strong>在反复迭代的过程中可以看到损失值不断减小的现象，如果没有下降说明出了问题</strong></li>
</ol>
<p>接下来我们来实现这个最简单的神经网络。</p>
<h2 id="1-导入数据"><a href="#1-导入数据" class="headerlink" title="1. 导入数据"></a>1. 导入数据</h2><p>使用kaggle房价数据，选3列作为特征</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 读取数据</span></span><br><span class="line">data = pd.read_csv(<span class="hljs-string">'data/kaggle_house_price_prediction/kaggle_hourse_price_train.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 使用这3列作为特征</span></span><br><span class="line">features = [<span class="hljs-string">'LotArea'</span>, <span class="hljs-string">'BsmtUnfSF'</span>, <span class="hljs-string">'GarageArea'</span>]</span><br><span class="line">target = <span class="hljs-string">'SalePrice'</span></span><br><span class="line">data = data[features + [target]]</span><br></pre></td></tr></table></figure>
<h2 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h2><p>40%做测试集，60%做训练集</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split</span><br><span class="line">trainX, testX, trainY, testY = train_test_split(data[features], data[target], test_size = <span class="hljs-number">0.4</span>, random_state = <span class="hljs-number">32</span>)</span><br></pre></td></tr></table></figure>
<p>训练集876个样本，3个特征，测试集584个样本，3个特征</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainX.shape, trainY.shape, testX.shape, testY.shape</span><br></pre></td></tr></table></figure>
<h2 id="3-参数初始化"><a href="#3-参数初始化" class="headerlink" title="3. 参数初始化"></a>3. 参数初始化</h2><p>这里，我们要初始化参数$W$和$b$，其中$W \in \mathbb{R}^m$，$b \in \mathbb{R}$，初始化的策略是将$W$初始化成一个随机数矩阵，参数$b$为0。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize</span><span class="hljs-params">(m)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    参数初始化，将W初始化成一个随机向量，b是一个长度为1的向量</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    m: int, 特征数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    W: np.ndarray, shape = (m, ), 参数W</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    b: np.ndarray, shape = (1, ), 参数b</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 指定随机种子，这样生成的随机数就是固定的了，这样就可以与下面的测试样例进行比对</span></span><br><span class="line">    np.random.seed(<span class="hljs-number">32</span>)</span><br><span class="line">    </span><br><span class="line">    W = np.random.normal(size = (m, )) * <span class="hljs-number">0.01</span></span><br><span class="line">    </span><br><span class="line">    b = np.zeros((<span class="hljs-number">1</span>, ))</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> W, b</span><br></pre></td></tr></table></figure>
<h2 id="4-前向传播"><a href="#4-前向传播" class="headerlink" title="4. 前向传播"></a>4. 前向传播</h2><p>这里，我们要完成输入矩阵$X$在神经网络中的计算，也就是完成 $Z = XW + b$ 的计算。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(X, W, b)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    前向传播，计算Z = XW + b</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray, shape = (n, m)，输入的数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    W: np.ndarray, shape = (m, )，权重</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    b: np.ndarray, shape = (1, )，偏置</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    Z: np.ndarray, shape = (n, )，线性组合后的值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 完成Z = XW + b的计算</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    Z = np.dot(X, W) + b</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> Z</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">tmp = forward(trainX, Wt, bt)</span><br><span class="line">print(tmp.mean()) <span class="hljs-comment"># -28.37377</span></span><br></pre></td></tr></table></figure>
<h2 id="5-损失函数"><a href="#5-损失函数" class="headerlink" title="5. 损失函数"></a>5. 损失函数</h2><p>接下来编写损失函数，我们以均方误差(MSE)作为损失函数，需要大家实现MSE的计算：</p>
<p>$$<br>\mathrm{loss}(y, Z) = \frac{1}{n} \sum^n_{i = 1} (y_i - Z_i)^2<br>$$</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mse</span><span class="hljs-params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    MSE，均方误差</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    y_true: np.ndarray, shape = (n, )，真值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    y_pred: np.ndarray, shape = (n, )，预测值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    loss: float，损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 计算MSE</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    loss = ((y_true - y_pred) ** <span class="hljs-number">2</span>).sum() / len(y_true)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">tmp = mse(trainY, forward(trainX, Wt, bt))</span><br><span class="line">print(tmp) <span class="hljs-comment"># 39381033680.5</span></span><br></pre></td></tr></table></figure>
<h2 id="6-反向传播"><a href="#6-反向传播" class="headerlink" title="6. 反向传播"></a>6. 反向传播</h2><p>这里我们要完成梯度的计算，也就是计算出损失函数对参数的偏导数的导数值：</p>
<p>$$<br>\frac{\partial \mathrm{loss}}{\partial W} = \frac{2}{n} X^\mathrm{T} (Z - y)<br>$$</p>
<p>$$<br>\frac{\partial \mathrm{loss}}{\partial b} = \frac{2}{n} \sum^n_{i=1}(Z_i - y_i)<br>$$</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_gradient</span><span class="hljs-params">(X, Z, y_true)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    计算梯度</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray, shape = (n, m)，输入的数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Z: np.ndarray, shape = (n, )，线性组合后的值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    y_true: np.ndarray, shape = (n, )，真值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    dW, np.ndarray, shape = (m, ), 参数W的梯度</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    db, np.ndarray, shape = (1, ), 参数b的梯度</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    n = len(y_true)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 计算W的梯度</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    dW = np.dot(X.T, (Z - y_true)) * <span class="hljs-number">2</span> / n</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 计算b的梯度</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    db = (Z - y_true).sum() / n</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> dW, db</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">Zt = forward(trainX, Wt, bt)</span><br><span class="line">dWt, dbt = compute_gradient(trainX, Zt, trainY)</span><br><span class="line">print(dWt.shape) <span class="hljs-comment"># (3,)</span></span><br><span class="line">print(dWt.mean()) <span class="hljs-comment"># -1532030241.25</span></span><br><span class="line">print(dbt.mean()) <span class="hljs-comment"># -182154.277882</span></span><br></pre></td></tr></table></figure>
<h2 id="7-梯度下降"><a href="#7-梯度下降" class="headerlink" title="7. 梯度下降"></a>7. 梯度下降</h2><p>这部分需要实现梯度下降的函数<br>$$<br>W := W - \alpha \frac{\partial \mathrm{loss}}{\partial W}<br>$$</p>
<p>$$<br>b := b - \alpha \frac{\partial \mathrm{loss}}{\partial b}<br>$$</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span><span class="hljs-params">(dW, db, W, b, learning_rate)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    梯度下降，参数更新，不需要返回值，W和b实际上是以引用的形式传入到函数内部，</span></span><br><span class="line"><span class="hljs-string">    函数内改变W和b会直接影响到它们本身</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    dW, np.ndarray, shape = (m, ), 参数W的梯度</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    db, np.ndarray, shape = (1, ), 参数b的梯度</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    W: np.ndarray, shape = (m, )，权重</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    b: np.ndarray, shape = (1, )，偏置</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    learning_rate, float，学习率</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 更新W</span></span><br><span class="line">    W -= learning_rate * dW</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 更新b</span></span><br><span class="line">    b -= learning_rate * db</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">print(Wt.mean()) <span class="hljs-comment"># 0.00405243937693</span></span><br><span class="line">print(bt.mean()) <span class="hljs-comment"># 0.0</span></span><br><span class="line"></span><br><span class="line">Zt = forward(trainX, Wt, bt)</span><br><span class="line">dWt, dbt = compute_gradient(trainX, Zt, trainY)</span><br><span class="line">update(dWt, dbt, Wt, bt, <span class="hljs-number">0.01</span>)</span><br><span class="line"></span><br><span class="line">print(Wt.shape) <span class="hljs-comment"># (3,)</span></span><br><span class="line">print(Wt.mean()) <span class="hljs-comment"># 15320302.4166</span></span><br><span class="line">print(bt.mean()) <span class="hljs-comment"># 1821.54277882</span></span><br></pre></td></tr></table></figure>
<p>完成整个参数更新的过程，先计算梯度，再更新参数，将compute_gradient和update组装在一起。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(X, Z, y_true, W, b, learning_rate)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    使用compute_gradient和update函数，先计算梯度，再更新参数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray, shape = (n, m)，输入的数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Z: np.ndarray, shape = (n, )，线性组合后的值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    y_true: np.ndarray, shape = (n, )，真值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    W: np.ndarray, shape = (m, )，权重</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    b: np.ndarray, shape = (1, )，偏置</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    learning_rate, float，学习率</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 计算参数的梯度</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    dW, db = compute_gradient(X, Z, y_true)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 更新参数</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    update(dW, db, W, b, learning_rate)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">print(Wt.mean()) <span class="hljs-comment"># 0.00405243937693</span></span><br><span class="line">print(bt.mean()) <span class="hljs-comment"># 0.0</span></span><br><span class="line"></span><br><span class="line">Zt = forward(trainX, Wt, bt)</span><br><span class="line">backward(trainX, Zt, trainY, Wt, bt, <span class="hljs-number">0.01</span>)</span><br><span class="line"></span><br><span class="line">print(Wt.shape) <span class="hljs-comment"># (3,)</span></span><br><span class="line">print(Wt.mean()) <span class="hljs-comment"># 15320302.4166</span></span><br><span class="line">print(bt.mean()) <span class="hljs-comment"># 1821.54277882</span></span><br></pre></td></tr></table></figure>
<h2 id="8-训练"><a href="#8-训练" class="headerlink" title="8. 训练"></a>8. 训练</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(trainX, trainY, testX, testY, W, b, epochs, learning_rate = <span class="hljs-number">0.01</span>, verbose = False)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    训练，我们要迭代epochs次，每次迭代的过程中，做一次前向传播和一次反向传播，更新参数</span></span><br><span class="line"><span class="hljs-string">    同时记录训练集和测试集上的损失值，后面画图用。然后循环往复，直到达到最大迭代次数epochs</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    trainX: np.ndarray, shape = (n, m), 训练集</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    trainY: np.ndarray, shape = (n, ), 训练集标记</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    testX: np.ndarray, shape = (n_test, m)，测试集</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    testY: np.ndarray, shape = (n_test, )，测试集的标记</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    W: np.ndarray, shape = (m, )，参数W</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    b: np.ndarray, shape = (1, )，参数b</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    epochs: int, 要迭代的轮数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    learning_rate: float, default 0.01，学习率</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    verbose: boolean, default False，是否打印损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    training_loss_list = []</span><br><span class="line">    testing_loss_list = []</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(epochs):</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 这里我们要将神经网络的输出值保存起来，因为后面反向传播的时候需要这个值</span></span><br><span class="line">        Z = forward(trainX, W, b)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算训练集的损失值</span></span><br><span class="line">        training_loss = mse(trainY, Z)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算测试集的损失值        </span></span><br><span class="line">        testing_loss = mse(testY, forward(testX, W, b))</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 将损失值存起来</span></span><br><span class="line">        training_loss_list.append(training_loss)</span><br><span class="line">        testing_loss_list.append(testing_loss)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 打印损失值，debug用</span></span><br><span class="line">        <span class="hljs-keyword">if</span> verbose:</span><br><span class="line">            print(<span class="hljs-string">'epoch %s training loss: %s'</span>%(epoch+<span class="hljs-number">1</span>, training_loss))</span><br><span class="line">            print(<span class="hljs-string">'epoch %s testing loss: %s'</span>%(epoch+<span class="hljs-number">1</span>, testing_loss))</span><br><span class="line">            print()</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 反向传播，参数更新</span></span><br><span class="line">        backward(trainX, Z, trainY, W, b, learning_rate)</span><br><span class="line">        </span><br><span class="line">    <span class="hljs-keyword">return</span> training_loss_list, testing_loss_list</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">print(Wt.mean())          <span class="hljs-comment"># 0.00405243937693</span></span><br><span class="line">print(bt.mean())          <span class="hljs-comment"># 0.0</span></span><br><span class="line"></span><br><span class="line">training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, Wt, bt, <span class="hljs-number">2</span>, learning_rate = <span class="hljs-number">0.01</span>, verbose = <span class="hljs-keyword">False</span>)</span><br><span class="line"></span><br><span class="line">print(training_loss_list) <span class="hljs-comment"># [39381033680.460075, 3.3902307664083424e+23]</span></span><br><span class="line">print(testing_loss_list)  <span class="hljs-comment"># [38555252685.093872, 4.1516070070405267e+23]</span></span><br><span class="line">print(Wt.mean())          <span class="hljs-comment"># -5.70557904608e+13</span></span><br><span class="line">print(bt.mean())          <span class="hljs-comment"># -4412133889.08</span></span><br></pre></td></tr></table></figure>
<h2 id="9-检查"><a href="#9-检查" class="headerlink" title="9. 检查"></a>9. 检查</h2><p>编写一个绘制损失值变化曲线的函数</p>
<p>一般我们通过绘制损失函数的变化曲线来判断模型的拟合状态。</p>
<p>一般来说，随着迭代轮数的增加，训练集的loss在下降，而测试集的loss在上升，这说明我们正在不断地让模型在训练集上表现得越来越好，在测试集上表现得越来越糟糕，这就是过拟合的体现。  </p>
<p>如果训练集loss和测试集loss共同下降，这就是我们想要的结果，说明模型正在很好的学习。  </p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_loss_curve</span><span class="hljs-params">(training_loss_list, testing_loss_list)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    绘制损失值变化曲线</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    plt.figure(figsize = (<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))</span><br><span class="line">    plt.plot(training_loss_list, label = <span class="hljs-string">'training loss'</span>)</span><br><span class="line">    plt.plot(testing_loss_list, label = <span class="hljs-string">'testing loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="hljs-string">'epoch'</span>)</span><br><span class="line">    plt.ylabel(<span class="hljs-string">'loss'</span>)</span><br><span class="line">    plt.legend()</span><br></pre></td></tr></table></figure>
<p>上面这些函数就是完成整个神经网络需要的函数了</p>
<table>
<thead>
<tr>
<th>函数名</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>initialize</td>
<td>参数初始化</td>
</tr>
<tr>
<td>forward</td>
<td>给定数据，计算神经网络的输出值</td>
</tr>
<tr>
<td>mse</td>
<td>给定真值，计算神经网络的预测值与真值之间的差距</td>
</tr>
<tr>
<td>backward</td>
<td>计算参数的梯度，并实现参数的更新</td>
</tr>
<tr>
<td>compute_gradient</td>
<td>计算参数的梯度</td>
</tr>
<tr>
<td>update</td>
<td>参数的更新</td>
</tr>
<tr>
<td>backward</td>
<td>计算参数梯度，并且更新参数</td>
</tr>
<tr>
<td>train</td>
<td>训练神经网络</td>
</tr>
<tr>
<td>plot_loss_curve</td>
<td>绘制损失函数的变化曲线</td>
</tr>
</tbody>
</table>
<p>我们使用参数初始化函数和训练函数，完成神经网络的训练。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 特征数m</span></span><br><span class="line">m = trainX.shape[<span class="hljs-number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 参数初始化</span></span><br><span class="line">W, b = initialize(m)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 训练20轮，学习率为0.01</span></span><br><span class="line">training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, W, b, <span class="hljs-number">20</span>, learning_rate = <span class="hljs-number">0.01</span>, verbose = <span class="hljs-keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>绘制损失值的变化曲线</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_loss_curve(training_loss_list, testing_loss_list)</span><br></pre></td></tr></table></figure>
<p>通过打印损失的信息我们可以看到损失值持续上升，这就说明哪里出了问题。但是如果所有的测试样例都通过了，就说明我们的实现是没有问题的。运行下面的测试样例，观察哪里出了问题。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">print(<span class="hljs-string">'epoch 0, W:'</span>, Wt)  <span class="hljs-comment"># [-0.00348894  0.00983703  0.00580923]</span></span><br><span class="line">print(<span class="hljs-string">'epoch 0, b:'</span>, bt)  <span class="hljs-comment"># [ 0.]</span></span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">Zt = forward(trainX, Wt, bt)</span><br><span class="line">dWt, dbt = compute_gradient(trainX, Zt, trainY)</span><br><span class="line">print(<span class="hljs-string">'dWt:'</span>, dWt) <span class="hljs-comment"># [ -4.18172940e+09  -2.19880296e+08  -1.94481031e+08]</span></span><br><span class="line">print(<span class="hljs-string">'db:'</span>, dbt) <span class="hljs-comment"># -182154.277882</span></span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">update(dWt, dbt, Wt, bt, <span class="hljs-number">0.01</span>)</span><br><span class="line">print(<span class="hljs-string">'epoch 1, W:'</span>, Wt)  <span class="hljs-comment"># [ 41817293.96016914   2198802.97412493   1944810.31544994]</span></span><br><span class="line">print(<span class="hljs-string">'epoch 1, b:'</span>, bt)  <span class="hljs-comment"># [ 1821.54277882]</span></span><br></pre></td></tr></table></figure>
<p>可以看到，我们最开始的参数都是在 $10^{-3}$ 这个数量级上，而第一轮迭代时计算出的梯度的数量级在 $10^8$ 左右，这就导致使用梯度下降更新的时候，让参数变成了 $10^6$ 这个数量级左右（学习率为0.01）。产生这样的问题的主要原因是：我们的原始数据 $X$ 没有经过适当的处理，直接扔到了神经网络中进行训练，导致在计算梯度时，由于 $X$ 的数量级过大，导致梯度的数量级变大，在参数更新时使得参数的数量级不断上升，导致参数无法收敛。</p>
<p>解决的方法也很简单，对参数进行归一化处理，将其标准化，使均值为0，缩放到 $[-1, 1]$附近。</p>
<h2 id="10-标准化处理"><a href="#10-标准化处理" class="headerlink" title="10. 标准化处理"></a>10. 标准化处理</h2><p>标准化处理和第一题一样</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler</span><br><span class="line">stand = StandardScaler()</span><br><span class="line">trainX_normalized = stand.fit_transform(trainX)</span><br><span class="line">testX_normalized = stand.transform(testX)</span><br></pre></td></tr></table></figure>
<p>重新训练模型，这次我们迭代40轮，学习率设置为0.1</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = trainX.shape[<span class="hljs-number">1</span>]</span><br><span class="line">W, b = initialize(m)</span><br><span class="line">training_loss_list, testing_loss_list = train(trainX_normalized, trainY, testX_normalized, testY, W, b, <span class="hljs-number">40</span>, learning_rate = <span class="hljs-number">0.1</span>, verbose = <span class="hljs-keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>打印损失值变化曲线</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_loss_curve(training_loss_list, testing_loss_list)</span><br></pre></td></tr></table></figure>
<p>计算测试集上的MSE</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">prediction = forward(testX_normalized, W, b)</span><br><span class="line">mse(testY, prediction) ** <span class="hljs-number">0.5</span></span><br></pre></td></tr></table></figure>
<h1 id="第三题：神经网络：对数几率回归"><a href="#第三题：神经网络：对数几率回归" class="headerlink" title="第三题：神经网络：对数几率回归"></a>第三题：神经网络：对数几率回归</h1><p>实验内容：</p>
<ol>
<li>完成对数几率回归</li>
<li>使用梯度下降求解模型参数</li>
<li>绘制模型损失值的变化曲线</li>
<li>调整学习率和迭代轮数，观察损失值曲线的变化</li>
<li>按照给定的学习率和迭代轮数，初始化新的参数，绘制其训练集和测试集损失值的变化曲线，完成表格内精度的填写</li>
</ol>
<p>对数几率回归，二分类问题的分类算法，属于线性模型中的一种，我们可以将其抽象为最简单的神经网络。</p>
<p><img src="/blog/2018/09/11/logistic-regression/Fig1.png" alt="Figure1"></p>
<p>只有一个输入层和一个输出层，还有一个激活函数，$\rm sigmoid$，简记为$\sigma$。<br>我们设输入为$X \in \mathbb{R}^{n \times m}$，输入层到输出层的权重为$W \in \mathbb{R}^{m}$，偏置$b \in \mathbb{R}$。</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>$$<br>\mathrm{sigmoid}(x) = \frac{1}{1 + e^{-x}}<br>$$</p>
<p>这个激活函数，会将输出层的神经元的输出值转换为一个 $(0, 1)$ 区间内的数。</p>
<p>因为是二分类问题，我们设类别为0和1，我们将输出值大于0.5的样本分为1类，输出值小于0.5的类分为0类。</p>
<h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>$$<br>Z = XW + b\\<br>\hat{y} = \sigma(Z)<br>$$</p>
<p>其中，$O \in \mathbb{R}^{n}$为输出层的结果，$\sigma$为$\rm sigmoid$激活函数。</p>
<p><strong>注意：这里我们其实是做了广播，将$b$复制了$n-1$份后拼接成了维数为$n$的向量。</strong></p>
<p>所以对数几率回归就可以写为：</p>
<p>$$<br>\hat{y} = \frac{1}{1 + e^{-XW + b}}<br>$$</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>使用对数损失函数，因为对数损失函数较其他损失函数有更好的性质，感兴趣的同学可以去查相关的资料。 </p>
<p>针对二分类问题的对数损失函数：</p>
<p>$$<br>\mathrm{loss}(y, \hat{y}) = - y \log{\hat{y}} - (1 - y) \log{(1 - \hat{y})}<br>$$</p>
<p>在这个对数几率回归中，我们的损失函数对所有样本取个平均值：</p>
<p>$$<br>\mathrm{loss}(y, \hat{y}) = - \frac{1}{n} \sum^n_{i = 1}[y_i \log{\hat{y_i}} + (1 - y_i) \log{(1 - \hat{y_i})}]<br>$$</p>
<p><strong>注意，这里我们的提到的$\log$均为$\ln$，在numpy中为</strong><code>np.log</code>。</p>
<p>因为我们的类别只有0和1，所以在这个对数损失函数中，要么前一项为0，要么后一项为0。</p>
<p>如果当前样本的类别为0，那么前一项就为0，损失函数变为 $- \log{(1 - \hat{y})}$ ，因为我们的预测值 $0 &lt; \hat{y} &lt; 1$ ，所以 $0 &lt; 1 - \hat{y} &lt; 1$ ，$- \log{(1 - \hat{y})} &gt; 0$ ，为了降低损失值，模型需要让预测值 $\hat{y}$不断地趋于0。</p>
<p>同理，如果当前样本的类别为1，那么降低损失值就可以使模型的预测值趋于1。</p>
<h2 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h2><p>求得损失函数对参数的偏导数后，我们就可以使用<strong>梯度下降</strong>进行参数更新：</p>
<p>$$<br>W := W - \alpha \frac{\partial \mathrm{loss}}{\partial W}\\<br>b := b - \alpha \frac{\partial \mathrm{loss}}{\partial b}<br>$$</p>
<p>其中，$\alpha$ 是学习率，一般设置为0.1，0.01等。</p>
<p>经过<strong>一定次数</strong>的迭代后，参数会收敛至最优点。这种基于梯度的优化算法很常用，训练神经网络主要使用这类优化算法。</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>我们使用梯度下降更新参数$W$和$b$。为此需要求得损失函数对参数$W$和$b$的偏导数，根据链式法则有：</p>
<p>$$\begin{aligned}<br>\frac{\partial \mathrm{loss}}{\partial W} &amp;= \frac{\partial \mathrm{loss}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial Z} \frac{\partial Z}{\partial W}<br>\end{aligned}<br>$$</p>
<p>这里我们一项一项求，先求第一项：</p>
<p>$$\begin{aligned}<br>\frac{\partial \mathrm{loss}}{\partial \hat{y}} = - \frac{1}{n} \sum^n_{i = 1} [\frac{y}{\hat{y}} - \frac{1 - y}{1 - \hat{y}}]<br>\end{aligned}<br>$$</p>
<p>第二项：</p>
<p>$$\begin{aligned}<br>\frac{\partial \hat{y}}{\partial Z} &amp; = \frac{\partial (\frac{1}{1 + e^{-Z}})}{\partial Z}\\<br>&amp; = \frac{e^{-Z}}{(1 + e^{-Z})^2}\\<br>&amp; = \frac{e^{-Z}}{(1 + e^{-Z})} \frac{1}{(1 + e^{-Z})}\\<br>&amp; = \frac{e^{-Z}}{(1 + e^{-Z})} (1 - \frac{e^{-Z}}{(1 + e^{-Z})})\\<br>&amp; = \sigma(Z)(1 - \sigma(Z))<br>\end{aligned}<br>$$</p>
<p>第三项：</p>
<p>$$<br>\frac{\partial Z}{\partial W} = X^{\mathrm{T}}<br>$$</p>
<p>综上：</p>
<p>$$\begin{aligned}<br>\frac{\partial \mathrm{loss}}{\partial W} &amp;= \frac{\partial \mathrm{loss}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial Z} \frac{\partial Z}{\partial W}\\<br>&amp;= - \frac{1}{n} \sum^n_{i = 1} [\frac{y_i}{\hat{y_i}} - \frac{1 - y_i}{1 - \hat{y_i}}] [\sigma(Z_i)(1 - \sigma(Z_i))] {X_i}^{\mathrm{T}}\\<br>&amp;= - \frac{1}{n} \sum^n_{i = 1} [\frac{y_i}{\hat{y_i}} - \frac{1 - y_i}{1 - \hat{y_i}}] [\hat{y_i}(1 - \hat{y_i})] {X_i}^{\mathrm{T}}\\<br>&amp;= - \frac{1}{n} \sum^n_{i = 1} [y_i(1 - \hat{y_i}) - \hat{y_i}(1 - y_i)] {X_i}^{\mathrm{T}}\\<br>&amp;= - \frac{1}{n} \sum^n_{i = 1} (y_i - y_i \hat{y_i} - \hat{y_i} + y_i \hat{y_i}) {X_i}^{\mathrm{T}}\\<br>&amp;= - \frac{1}{n} \sum^n_{i = 1} (y_i - \hat{y_i}) {X_i}^{\mathrm{T}}\\<br>&amp;= \frac{1}{n} [X^{\mathrm{T}}(\hat{y} - y)]<br>\end{aligned}<br>$$</p>
<p>同理，求$\rm loss$对$b$的偏导数：</p>
<p><strong>注意，由于$b$是被广播成$n \times K$的矩阵，因此实际上$b$对每个样本的损失都有贡献，因此对其求偏导时，要把$n$个样本对它的偏导数加和。</strong></p>
<p>$$\begin{aligned}<br>\frac{\partial \mathrm{loss}}{\partial b} &amp;= \frac{\partial \mathrm{loss}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial Z} \frac{\partial Z}{\partial b}\\<br>&amp;= - \frac{1}{n} \sum^n_{i = 1} [\frac{y_i}{\hat{y_i}} - \frac{1 - y_i}{1 - \hat{y_i}}] [\sigma(Z_i)(1 - \sigma(Z_i))]\\<br>&amp;= - \frac{1}{n} \sum^n_{i = 1} [\frac{y_i}{\hat{y_i}} - \frac{1 - y_i}{1 - \hat{y_i}}] [\hat{y_i}(1 - \hat{y_i})]\\<br>&amp;= - \frac{1}{n} \sum^n_{i = 1} [y_i(1 - \hat{y_i}) - \hat{y_i}(1 - y_i)]\\<br>&amp;= - \frac{1}{n} \sum^n_{i = 1} (y_i - y_i \hat{y_i} - \hat{y_i} + y_i \hat{y_i})\\<br>&amp;= \frac{1}{n} \sum^n_{i = 1} (\hat{y_i} - y_i)\\<br>\end{aligned}$$</p>
<p>这样，我们就得到了损失函数对参数的偏导数，然后就可以使用梯度下降算法更新参数</p>
<h2 id="1-导入数据集"><a href="#1-导入数据集" class="headerlink" title="1. 导入数据集"></a>1. 导入数据集</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>我们生成半月形数据</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_moons</span><br><span class="line">X, y = make_moons(n_samples = <span class="hljs-number">2000</span>, noise = <span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)</span><br></pre></td></tr></table></figure>
<p>选择40%的数据作为测试集，60%作为训练集</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split</span><br><span class="line">trainX, testX, trainY, testY = train_test_split(X, y, test_size = <span class="hljs-number">0.4</span>, random_state = <span class="hljs-number">32</span>)</span><br><span class="line">trainY = trainY</span><br><span class="line">testY = testY</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainX.shape, trainY.shape, testX.shape, testY.shape</span><br></pre></td></tr></table></figure>
<h2 id="2-数据预处理-1"><a href="#2-数据预处理-1" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h2><p>使用和第一题一样的预处理方式</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler</span><br><span class="line">s = StandardScaler()</span><br><span class="line">trainX = s.fit_transform(trainX)</span><br><span class="line">testX = s.transform(testX)</span><br></pre></td></tr></table></figure>
<h2 id="3-定义神经网络"><a href="#3-定义神经网络" class="headerlink" title="3. 定义神经网络"></a>3. 定义神经网络</h2><h3 id="3-1-参数初始化"><a href="#3-1-参数初始化" class="headerlink" title="3.1 参数初始化"></a>3.1 参数初始化</h3><p>我们需要对神经网络的参数进行初始化，这个网络中只有两个参数，一个$W \in \mathbb{R}^{m}$，一个$b \in \mathbb{R}$。初始化的时候，我们将参数W随机初始化，参数b初始化为0。为什么要对神经网络的参数进行随机初始化，感兴趣的同学可以去查相关的资料。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize</span><span class="hljs-params">(m)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    初始化参数W和参数b</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    W: np.ndarray, shape = (m, )，参数W</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    b: np.ndarray, shape = (1, )，参数b</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    np.random.seed(<span class="hljs-number">32</span>)</span><br><span class="line">    W = np.random.normal(size = (m, )) * <span class="hljs-number">0.01</span></span><br><span class="line">    b = np.zeros((<span class="hljs-number">1</span>, ))</span><br><span class="line">    <span class="hljs-keyword">return</span> W, b</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">print(Wt.shape) <span class="hljs-comment"># (2,)</span></span><br><span class="line">print(bt.shape) <span class="hljs-comment"># (1,)</span></span><br></pre></td></tr></table></figure>
<h3 id="3-2-前向传播"><a href="#3-2-前向传播" class="headerlink" title="3.2 前向传播"></a>3.2 前向传播</h3><p>接下来我们要定义神经网络前向传播的过程。</p>
<p>首先计算$Z = XW + b$</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">linear_combination</span><span class="hljs-params">(X, W, b)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    完成Z = XW + b的计算</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray, shape = (n, m)，输入的数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    W: np.ndarray, shape = (m, )，权重</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    b: np.ndarray, shape = (1, )，偏置</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    Z: np.ndarray, shape = (n, )，线性组合后的值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    Z = np.dot(X, W) + b                          <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> Z</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">linear_combination(trainX, Wt, bt).shape <span class="hljs-comment">#(1200,)</span></span><br></pre></td></tr></table></figure>
<p>接下来实现激活函数$\rm sigmoid$</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">my_sigmoid</span><span class="hljs-params">(x)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    simgoid 1 / (1 + exp(-x))</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray, 待激活的值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    activations = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> activations</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">Zt = linear_combination(trainX, Wt, bt)</span><br><span class="line">my_sigmoid(Zt).mean() <span class="hljs-comment"># 0.49999</span></span><br></pre></td></tr></table></figure>
<p>在实现$\rm sigmoid$的时候，可能会遇到上溢(overflow)的问题，可以看到$\rm sigmoid$中有一个指数运算<br>$$<br>\mathrm{sigmoid}(x) = \frac{1}{1 + e^{-x}}<br>$$<br>当$x$很大的时候，我们使用<code>numpy.exp(x)</code>会直接溢出</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.exp(<span class="hljs-number">1e56</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my_sigmoid(np.array([<span class="hljs-number">-1e56</span>]))</span><br></pre></td></tr></table></figure>
<p>虽说程序没有报错，只是抛出了warning，但还是应该解决一下。</p>
<p>解决这种问题的方法有很多，比如，我们可以将$\rm sigmoid$进行变换：</p>
<p>$$\begin{aligned}<br>\mathrm{sigmoid}(x) &amp;= \frac{1}{1 + e^{-x}}\\<br>&amp;= \frac{e^x}{1 + e^x}\\<br>&amp;= \frac{1}{2} + \frac{1}{2} \mathrm{tanh}(\frac{x}{2})<br>\end{aligned}$$</p>
<p>其中，$\mathrm{tanh}(x) = \frac{\mathrm{sinh}(x)}{\mathrm{cosh}(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}$</p>
<p>转换成这种形式后，我们就可以直接利用<code>numpy.tanh</code>完成$\rm sigmoid$的计算，就不会产生上溢的问题了。</p>
<p>除此以外，最好的解决方法是使用scipy中的<code>expit</code>函数，完成$\rm sigmoid$的计算。我们现在做的都是神经网络底层相关的运算，很容易出现数值不稳定性相关的问题，最好的办法就是使用别人已经实现好的函数，这样就能减少我们很多的工作量，同时又快速地完成任务。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> scipy.special <span class="hljs-keyword">import</span> expit</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(X)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">return</span> expit(X)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">sigmoid(np.array([<span class="hljs-number">-1e56</span>]))</span><br></pre></td></tr></table></figure>
<p>接下来完成整个前向传播的函数，也就是 $Z = XW+b$ 和 $\hat{y} = \mathrm{sigmoid}(Z)$</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(X, W, b)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    完成输入矩阵X到最后激活后的预测值y_pred的计算过程</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray, shape = (n, m)，数据，一行一个样本，一列一个特征</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    W: np.ndarray, shape = (m, )，权重</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    b: np.ndarray, shape = (1, )，偏置</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    y_pred: np.ndarray, shape = (n, )，模型对每个样本的预测值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 求Z</span></span><br><span class="line">    Z = linear_combination(X, W, b) <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 求激活后的预测值</span></span><br><span class="line">    y_pred = sigmoid(Z)             <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">forward(trainX, Wt, bt).mean() <span class="hljs-comment"># 0.4999(没有四舍五入)</span></span><br></pre></td></tr></table></figure>
<p>接下来完成损失函数的编写，我们使用的是对数损失，这里需要注意的一个问题是：</p>
<p>$$<br>\mathrm{loss}(y, \hat{y}) = - \frac{1}{n}[ y \log{\hat{y}} + (1 - y) \log{(1 - \hat{y})}]<br>$$</p>
<p>在这个对数损失中，$\hat{y}$中不能有$0$和$1$，如果有$0$，那么损失函数中的前半部分，$\log{0}$就会出错，如果有$1$，那么后半部分$\log{(1-1)}$就会出错。</p>
<p>所以我们要先将$\hat{y}$中的$0$和$1$改变一下，把$0$变成一个比较小但是大于$0$的数，把$1$变成小于$1$但是足够大的数。使用<code>numpy.clip</code>函数就可以作到这点。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">logloss</span><span class="hljs-params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    给定真值y，预测值y_hat，计算对数损失并返回</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    y_true: np.ndarray, shape = (n, ), 真值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    y_pred: np.ndarray, shape = (n, )，预测值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    loss: float, 损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 下面这句话会把y_pred里面小于1e-10的数变成1e-10，大于1 - 1e-10的数变成1 - 1e-10</span></span><br><span class="line">    y_hat = np.clip(y_pred, <span class="hljs-number">1e-10</span>, <span class="hljs-number">1</span> - <span class="hljs-number">1e-10</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 求解对数损失</span></span><br><span class="line">    loss = - np.sum(y_true * np.log(y_hat) + (<span class="hljs-number">1</span> - y_true) * np.log(<span class="hljs-number">1</span> - y_hat)) / len(y_true)  <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">logloss(trainY, forward(trainX, Wt, bt)) <span class="hljs-comment"># 0.69740</span></span><br></pre></td></tr></table></figure>
<h3 id="3-3-反向传播"><a href="#3-3-反向传播" class="headerlink" title="3.3 反向传播"></a>3.3 反向传播</h3><p>我们接下来要完成损失函数对参数的偏导数的计算</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_gradient</span><span class="hljs-params">(y_true, y_pred, X)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    给定预测值y_pred，真值y_true，传入的输入数据X，计算损失函数对参数W的偏导数的导数值dW，以及对b的偏导数的导数值db</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    y_true: np.ndarray, shape = (n, ), 真值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    y_pred: np.ndarray, shape = (n, )，预测值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray, shape = (n, m)，数据，一行一个样本，一列一个特征</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    dW: np.ndarray, shape = (m, ), 损失函数对参数W的偏导数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    db: float, 损失函数对参数b的偏导数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 求损失函数对参数W的偏导数的导数值</span></span><br><span class="line">    dW = np.dot(X.T, (y_pred - y_true)) / len(y_pred)     <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 求损失函数对参数b的偏导数的导数值</span></span><br><span class="line">    db = np.sum(y_pred - y_true) / len(y_pred)  <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> dW, db</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">dWt, dbt = compute_gradient(trainY, forward(trainX, Wt, bt), trainX)</span><br><span class="line">print(dWt.shape) <span class="hljs-comment"># (2, )</span></span><br><span class="line">print(dWt.sum()) <span class="hljs-comment"># 0.04625</span></span><br><span class="line">print(dbt)       <span class="hljs-comment"># 0.00999</span></span><br></pre></td></tr></table></figure>
<h3 id="3-4-参数更新"><a href="#3-4-参数更新" class="headerlink" title="3.4 参数更新"></a>3.4 参数更新</h3><p>给定学习率，结合上一步求出的偏导数，完成梯度下降的更新公式</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span><span class="hljs-params">(W, b, dW, db, learning_rate)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    梯度下降，给定参数W，参数b，以及损失函数对他们的偏导数，使用梯度下降更新参数W和参数b</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    W: np.ndarray, shape = (m, )，参数W</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    b: np.ndarray, shape = (1, )，参数b</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    dW: np.ndarray, shape = (m, ), 损失函数对参数W的偏导数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    db: float, 损失函数对参数b的偏导数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    learning_rate, float，学习率</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 对参数W进行更新</span></span><br><span class="line">    W -= learning_rate * dW</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 对参数b进行更新</span></span><br><span class="line">    b -= learning_rate * db   <span class="hljs-comment"># YOUR CODE HERE</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">print(Wt)  <span class="hljs-comment"># [-0.00348894  0.00983703]</span></span><br><span class="line">print(bt)  <span class="hljs-comment"># [ 0.]</span></span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">dWt, dbt = compute_gradient(trainY, forward(trainX, Wt, bt), trainX)</span><br><span class="line">print(dWt) <span class="hljs-comment"># [-0.28650366  0.33276308]</span></span><br><span class="line">print(dbt) <span class="hljs-comment"># 0.00999999939463</span></span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">update(Wt, bt, dWt, dbt, <span class="hljs-number">0.01</span>)</span><br><span class="line">print(Wt)  <span class="hljs-comment"># [-0.00062391  0.0065094 ]</span></span><br><span class="line">print(bt)  <span class="hljs-comment"># [ -9.99999939e-05]</span></span><br></pre></td></tr></table></figure>
<p>我们来完成整个反向传播和更新参数的函数</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(y_true, y_pred, X, W, b, learning_rate)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    反向传播，包含了计算损失函数对各个参数的偏导数的过程，以及梯度下降更新参数的过程</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    y_true: np.ndarray, shape = (n, ), 真值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    y_pred: np.ndarray, shape = (n, )，预测值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray, shape = (n, m)，数据，一行一个样本，一列一个特征</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    W: np.ndarray, shape = (m, )，参数W</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    b: np.ndarray, shape = (1, )，参数b</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    dW: np.ndarray, shape = (m, ), 损失函数对参数W的偏导数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    db: float, 损失函数对参数b的偏导数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    learning_rate, float，学习率</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 求参数W和参数b的梯度</span></span><br><span class="line">    dW, db = compute_gradient(y_true, y_pred, X)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 梯度下降</span></span><br><span class="line">    update(W, b, dW, db, learning_rate)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">y_predt = forward(trainX, Wt, bt)</span><br><span class="line">loss_1 = logloss(trainY, y_predt)</span><br><span class="line">print(loss_1)                             <span class="hljs-comment"># 0.697403529518</span></span><br><span class="line"></span><br><span class="line">backward(trainY, y_predt, trainX, Wt, bt, <span class="hljs-number">0.01</span>)</span><br><span class="line"></span><br><span class="line">y_predt = forward(trainX, Wt, bt)</span><br><span class="line">loss_2 = logloss(trainY, y_predt)</span><br><span class="line">print(loss_2)                             <span class="hljs-comment"># 0.695477626714</span></span><br></pre></td></tr></table></figure>
<h2 id="4-训练函数的编写"><a href="#4-训练函数的编写" class="headerlink" title="4. 训练函数的编写"></a>4. 训练函数的编写</h2><p>我们已经实现了完成训练需要的子函数，接下来就是组装了</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(trainX, trainY, testX, testY, W, b, epochs, learning_rate = <span class="hljs-number">0.01</span>, verbose = False)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    训练，我们要迭代epochs次，每次迭代的过程中，做一次前向传播和一次反向传播</span></span><br><span class="line"><span class="hljs-string">    同时记录训练集和测试集上的损失值，后面画图用</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    trainX: np.ndarray, shape = (n, m), 训练集</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    trainY: np.ndarray, shape = (n, ), 训练集标记</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    testX: np.ndarray, shape = (n_test, m)，测试集</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    testY: np.ndarray, shape = (n_test, )，测试集的标记</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    W: np.ndarray, shape = (m, )，参数W</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    b: np.ndarray, shape = (1, )，参数b</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    epochs: int, 要迭代的轮数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    learning_rate: float, default 0.01，学习率</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    verbose: boolean, default False，是否打印损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    training_loss_list = []</span><br><span class="line">    testing_loss_list = []</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(epochs):</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算训练集前向传播得到的预测值</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        train_y_pred = forward(trainX, W, b)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># 计算当前训练集的损失值</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        training_loss = logloss(trainY, train_y_pred)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算测试集前向传播得到的预测值</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        test_y_pred = forward(testX, W, b)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 计算当前测试集的损失值</span></span><br><span class="line">        testing_loss = logloss(testY, test_y_pred)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-keyword">if</span> verbose == <span class="hljs-keyword">True</span>:</span><br><span class="line">            print(<span class="hljs-string">'epoch %s, training loss:%s'</span>%(i + <span class="hljs-number">1</span>, training_loss))</span><br><span class="line">            print(<span class="hljs-string">'epoch %s, testing loss:%s'</span>%(i + <span class="hljs-number">1</span>, testing_loss))</span><br><span class="line">            print()</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 保存损失值</span></span><br><span class="line">        training_loss_list.append(training_loss)</span><br><span class="line">        testing_loss_list.append(testing_loss)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># 反向传播更新参数</span></span><br><span class="line">        <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">        backward(trainY, train_y_pred, trainX, W, b, learning_rate)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> training_loss_list, testing_loss_list</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, Wt, bt, <span class="hljs-number">2</span>, <span class="hljs-number">0.1</span>)</span><br><span class="line">print(training_loss_list)  <span class="hljs-comment"># [0.69740352951773121, 0.67843729060725722]</span></span><br><span class="line">print(testing_loss_list)   <span class="hljs-comment"># [0.69743661286103986, 0.67880126235588389]</span></span><br></pre></td></tr></table></figure>
<h2 id="5-绘制模型损失值变化曲线"><a href="#5-绘制模型损失值变化曲线" class="headerlink" title="5. 绘制模型损失值变化曲线"></a>5. 绘制模型损失值变化曲线</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_loss_curve</span><span class="hljs-params">(training_loss_list, testing_loss_list)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    绘制损失值变化曲线</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    plt.figure(figsize = (<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))</span><br><span class="line">    plt.plot(training_loss_list, label = <span class="hljs-string">'training loss'</span>)</span><br><span class="line">    plt.plot(testing_loss_list, label = <span class="hljs-string">'testing loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="hljs-string">'epoch'</span>)</span><br><span class="line">    plt.ylabel(<span class="hljs-string">'loss'</span>)</span><br><span class="line">    plt.legend()</span><br></pre></td></tr></table></figure>
<h2 id="6-预测"><a href="#6-预测" class="headerlink" title="6. 预测"></a>6. 预测</h2><p>接下来编写一个预测的函数，事实上，$\rm sigmoid$输出的是当前这个样本为正例的概率，也就是说，这个输出值是一个0到1的值，一般我们将大于0.5的值变成1，小于0.5的值变成0，也就是说，如果当前输出的概率值大于0.5，那我们认为这个样本的类别就是1，否则就是0，这样输出的就是类标了。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(X, W, b)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    预测，调用forward函数完成神经网络对输入X的计算，然后完成类别的划分，大于0.5的变为1，小于等于0.5的变为0</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray, shape = (n, m), 训练集</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    W: np.ndarray, shape = (m, 1)，参数W</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    b: np.ndarray, shape = (1, )，参数b</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    prediction: np.ndarray, shape = (n, 1)，预测的标记</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    prediction = (forward(testX, W, b) &gt; <span class="hljs-number">0.5</span>).astype(<span class="hljs-string">'uint8'</span>)  <span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">    <span class="hljs-keyword">return</span> prediction</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score</span><br><span class="line">Wt, bt = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">predictiont = predict(testX, Wt, bt)</span><br><span class="line">accuracy_score(testY, predictiont)  <span class="hljs-comment"># 0.16250000000000001</span></span><br></pre></td></tr></table></figure>
<h2 id="7-训练一个神经网络"><a href="#7-训练一个神经网络" class="headerlink" title="7. 训练一个神经网络"></a>7. 训练一个神经网络</h2><p>我们的学习率是0.01，迭代200轮</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W, b = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, W, b, <span class="hljs-number">200</span>, <span class="hljs-number">0.01</span>)</span><br></pre></td></tr></table></figure>
<p>计算测试集精度</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">prediction = predict(testX, W, b)</span><br><span class="line">accuracy_score(testY, prediction)  <span class="hljs-comment"># 0.83625000000000005</span></span><br></pre></td></tr></table></figure>
<p>绘制损失值变化曲线</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_loss_curve(training_loss_list, testing_loss_list)</span><br></pre></td></tr></table></figure>
<h1 id="test：初始化新的参数，学习率和迭代轮数按下表设置，绘制其训练集和测试集损失值的变化曲线，完成表格内精度的填写"><a href="#test：初始化新的参数，学习率和迭代轮数按下表设置，绘制其训练集和测试集损失值的变化曲线，完成表格内精度的填写" class="headerlink" title="test：初始化新的参数，学习率和迭代轮数按下表设置，绘制其训练集和测试集损失值的变化曲线，完成表格内精度的填写"></a>test：初始化新的参数，学习率和迭代轮数按下表设置，绘制其训练集和测试集损失值的变化曲线，完成表格内精度的填写</h1><h6 id="双击此处填写"><a href="#双击此处填写" class="headerlink" title="双击此处填写"></a>双击此处填写</h6><table>
<thead>
<tr>
<th>学习率</th>
<th>迭代轮数</th>
<th>测试集精度</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.0001</td>
<td>200</td>
<td>0.3325</td>
</tr>
<tr>
<td>0.1</td>
<td>1000</td>
<td>0.84</td>
</tr>
</tbody>
</table>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">W2, b2 = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, W2, b2, <span class="hljs-number">200</span>, <span class="hljs-number">0.0001</span>)</span><br><span class="line">prediction = predict(testX, W2, b2)</span><br><span class="line">print(accuracy_score(prediction, testY))</span><br><span class="line">plot_loss_curve(training_loss_list, testing_loss_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># YOUR CODE HERE</span></span><br><span class="line">W2, b2 = initialize(trainX.shape[<span class="hljs-number">1</span>])</span><br><span class="line">training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, W2, b2, <span class="hljs-number">1000</span>, <span class="hljs-number">0.1</span>)</span><br><span class="line">prediction = predict(testX, W2, b2)</span><br><span class="line">print(accuracy_score(prediction, testY))</span><br><span class="line">plot_loss_curve(training_loss_list, testing_loss_list)</span><br></pre></td></tr></table></figure>
<h1 id="第四题：神经网络：三层感知机"><a href="#第四题：神经网络：三层感知机" class="headerlink" title="第四题：神经网络：三层感知机"></a>第四题：神经网络：三层感知机</h1><p>实现内容：</p>
<ol>
<li>实现一个三层感知机</li>
<li>对手写数字数据集进行分类</li>
<li>绘制损失值变化曲线</li>
</ol>
<p>在这道题中，我们要实现一个三层感知机</p>
<p><img src="/blog/2018/09/11/logistic-regression/Fig2.png" alt="Figure2"></p>
<h2 id="前向传播-1"><a href="#前向传播-1" class="headerlink" title="前向传播"></a>前向传播</h2><p>我们实现一个最简单的三层感知机，一个输入层，一个隐藏层，一个输出层，隐藏层单元个数为$h$个，输出层有$K$个单元。</p>
<ol>
<li>我们将第一层的输入，定义为$X \in \mathbb{R}^{n \times m}$，n个样本，m个特征。  </li>
<li>输入层到隐藏层之间的权重(weight)与偏置(bias)，分别为$W_1 \in \mathbb{R}^{m \times h}$，$b_1 \in \mathbb{R}^{1 \times h}$。  </li>
<li>隐藏层到输出层的权重和偏置分为别$W_2 \in \mathbb{R}^{h \times K}$，$b_2 \in \mathbb{R}^{1 \times K}$。</li>
</ol>
<p>隐藏层的激活函数选用ReLU</p>
<p>$$<br>\mathrm{ReLU}(x) = \max (0, x)<br>$$</p>
<p>我们用$H_1$表示第一个隐藏层的输出值，$O$表示输出层的输出值，这样，前向传播即可定义为</p>
<p>$$<br>Z = XW_1 + b_1\\<br>H_1 = \mathrm{ReLU}(Z)\\<br>O = H_1 W_2 + b_2<br>$$</p>
<p>其中，$H_1 \in \mathbb{R}^{n \times h}$，$O \in \mathbb{R}^{n \times K}$。</p>
<p><strong>注意：这里我们其实是做了广播，将$b_1$复制了$n-1$份后拼接成了维数为$n \times h$的矩阵，同理，$b_2$也做了广播，拼成了$n \times K$的矩阵。</strong></p>
<p>最后一层的输出，使用softmax函数激活，得到神经网络计算出的各类的概率值：</p>
<p>$$<br>\begin{aligned}<br>\hat{y_i} &amp; = \mathrm{softmax}(O_i)\\<br>&amp; = \frac{\exp{(O_i)}}{\sum^{K}_{k=1} \exp{(O_k)}}<br>\end{aligned}<br>$$</p>
<p>其中，$\hat{y_i}$表示第$i$类的概率值，也就是输出层第$i$个神经元经$\mathrm{softmax}$激活后的值。</p>
<h2 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数使用交叉熵损失函数：<br>$$\mathrm{cross_entropy}(y, \hat{y}) = -\sum^{K}_{k=1}y_k \log{(\hat{y_k})}$$</p>
<p>这样，$n$个样本的平均损失为：<br>$$<br>\mathrm{loss} = - \frac{1}{n} \sum_n \sum^{K}_{k=1} y_k \log{(\hat{y_k})}<br>$$</p>
<p><strong>注意，这里我们的提到的$\log$均为$\ln$，在numpy中为</strong><code>np.log</code></p>
<h2 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h2><p>我们使用梯度下降训练模型，求解方式就是求出损失函数对参数的偏导数，即参数的梯度，然后将参数减去梯度乘以学习率，进行参数的更新。<br>$$<br>W := W - \alpha \frac{\partial \mathrm{loss}}{\partial W}<br>$$<br>其中，$\alpha$是学习率。</p>
<p>在这道题中，交叉熵损失函数的求导比较麻烦，我们先求神经网络的输出层的偏导数，写成链式法则的形式：</p>
<p>$$<br>\frac{\partial \mathrm{loss}}{\partial O_i} = \frac{\partial \mathrm{loss}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial O_i}<br>$$</p>
<p>首先求解第一项：<br>$$<br>\frac{\partial \mathrm{loss}}{\partial \hat{y}} = - \frac{1}{n} \sum_n \sum^{K}_{k=1} y_k \frac{1}{\hat{y_k}}<br>$$</p>
<p>然后求解第二项，因为$\hat{y_k}$的分母是$\sum_k \exp{(O_k)}$，里面包含$O_i$，所以每一个$\hat{y_k}$的分母都包含$O_i$，这就要求反向传播的时候需要考虑这$K$项，将这$K$项的偏导数加在一起。</p>
<p>这$K$项分别为：$\frac{\exp{(O_1)}}{\sum_k \exp{(O_k)}}$，$\frac{\exp{(O_2)}}{\sum_k \exp{(O_k)}}$，…，$\frac{\exp{(O_i)}}{\sum_k \exp{(O_k)}}$，…，$\frac{\exp{(O_k)}}{\sum_k \exp{(O_k)}}$。</p>
<p>显然，这里只有分子带有$O_i$的这项与其他的项不同，因为分子和分母同时包含了$O_i$，而其他的项只有分母包含了$O_i$。</p>
<p>这就需要在求解$\frac{\partial \hat{y}}{\partial O_i}$的时候分两种情况讨论</p>
<ol>
<li>分子带$O_i$</li>
<li>分子不带$O_i$</li>
</ol>
<p>第一种情况，当分子含有$O_i$时：</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial \hat{y_i}}{\partial O_i} &amp; = \frac{\partial \hat{y_i}}{\partial O_i}\\<br>&amp; = \frac{\exp{(O_i)} (\sum^{K}_{k=1} \exp{(O_k)}) - (\exp{(O_i)})^2 }{(\sum^{K}_{k=1} \exp{(O_k)})^2}\\<br>&amp; = \frac{\exp{(O_i)}}{\sum^{K}_{k=1} \exp{(O_k)}} \frac{\sum^{K}_{k=1} \exp{(O_k)} - \exp{(O_i)}}{\sum^{K}_{k=1} \exp{(O_k)}}\\<br>&amp; = \hat{y_i} ( 1 - \hat{y_i} )<br>\end{aligned}<br>$$</p>
<p>第二种情况，当分子不含$O_i$时，我们用$j$表示当前项的下标：</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial \hat{y_j}}{\partial O_i} &amp; = \frac{- \exp{(O_j)} \exp{(O_i)}}{(\sum^{K}_{k=1} \exp{(O_k)})^2}\\<br>&amp; = - \hat{y_j} \hat{y_i}<br>\end{aligned}<br>$$</p>
<p>这样，$\mathrm{loss}$对$O_i$的偏导数即为：<br>$$\begin{aligned}<br>\frac{\partial \mathrm{loss}}{\partial O_i} &amp; = \frac{\partial \mathrm{loss}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial O_i}\\<br>&amp; = (- \frac{1}{n} \sum_n \sum^{K}_{k=1} y_k \frac{1}{\hat{y_k}}) \frac{\partial \hat{y}}{\partial O_i}\\<br>&amp; = - \frac{1}{n} \sum_n (y_i \frac{1}{\hat{y_i}} \hat{y_i} ( 1 - \hat{y_i} ) + \sum^K_{k \not= i} y_k \frac{1}{\hat{y_k}}( - \hat{y_k} \hat{y_i}))\\<br>&amp; = - \frac{1}{n} \sum_n ( y_i - y_i \hat{y_i} - \sum^K_{k \not= i} y_k \hat{y_i})\\<br>&amp; = - \frac{1}{n} \sum_n ( y_i  - \hat{y_i} \sum^K_{k = 1} y_k )<br>\end{aligned}<br>$$</p>
<p>由于我们处理的多类分类任务，一个样本只对应一个标记，所以$\sum^K_{k = 1} y_k = 1$，上式在这种问题中，即可化简为：</p>
<p>$$\begin{aligned}<br>\frac{\partial \mathrm{loss}}{\partial O_i} &amp;= - \frac{1}{n} \sum_n ( y_i  - \hat{y_i})\\<br>&amp; = \frac{1}{n} \sum_n (\hat{y_i} -  y_i)<br>\end{aligned}<br>$$</p>
<p>将其写成矩阵表达式：</p>
<p>$$\begin{aligned}<br>\frac{\partial \mathrm{loss}}{\partial O} &amp;= \frac{1}{n} (\hat{y} - y)<br>\end{aligned}<br>$$</p>
<p>也就是说，我们的损失函数对输出层的$K$个神经单元的偏导数为$\mathrm{softmax}$激活值减去真值。</p>
<p>接下来我们需要求损失函数对参数$W_2$和$b_2$的偏导数</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial loss}{\partial W_2} &amp; = \frac{\partial \mathrm{loss}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial O} \frac{\partial O}{\partial W_2}\\<br>&amp; = \frac{\partial loss}{\partial O} \frac{\partial O}{\partial W_2}\\<br>&amp; = \frac{1}{n} (\hat{y} - y) \frac{\partial O}{\partial W_2}\\<br>&amp; = \frac{1}{n} [{H_1}^\mathrm{T} (\hat{y} - y)]<br>\end{aligned}<br>$$</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial loss}{\partial b_2} &amp; = \frac{\partial \mathrm{loss}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial O} \frac{\partial O}{\partial b_2}\\<br>&amp; = \frac{\partial loss}{\partial O} \frac{\partial O}{\partial b_2}\\<br>&amp; = \frac{1}{n} (\hat{y} - y) \frac{\partial O}{\partial b_2}\\<br>&amp; = \frac{1}{n} \sum^n_{i=1} (\hat{y_i} - y_i)<br>\end{aligned}<br>$$</p>
<p>其中，$\frac{\partial loss}{\partial W_2} \in \mathbb{R}^{h \times K}$，$\frac{\partial loss}{\partial b_2} \in \mathbb{R}^{1 \times K}$。<br><strong>注意，由于$b_2$是被广播成$n \times K$的矩阵，因此实际上$b_2$对每个样本的损失都有贡献，因此对其求偏导时，要把$n$个样本对它的偏导数加和。</strong></p>
<p>同理，我们可以求得$\mathrm{loss}$对$W_1$和$b_1$的偏导数：</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial loss}{\partial W_1} &amp; = \frac{\partial \mathrm{loss}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial O} \frac{\partial O}{\partial H_1} \frac{\partial H_1}{\partial Z} \frac{\partial Z}{\partial W_1}\\<br>&amp; = \frac{\partial loss}{\partial O} \frac{\partial O}{\partial H_1} \frac{\partial H_1}{\partial Z} \frac{\partial Z}{\partial W_1}\\<br>&amp; = \frac{1}{n} {X}^\mathrm{T} [(\hat{y} - y) {W_2}^\mathrm{T} \frac{\partial H_1}{\partial Z}]\\<br>\end{aligned}<br>$$</p>
<p>由于我们使用的是$\mathrm{ReLU}$激活函数，它的偏导数为：</p>
<p>$$\frac{\partial \mathrm{ReLU(x)}}{\partial x} =<br>\begin{cases}<br>0 &amp; \text{if } x &lt; 0\\<br>1 &amp; \text{if } x \geq 0<br>\end{cases}<br>$$</p>
<p>所以上式为：</p>
<p>$$<br>\frac{\partial loss}{\partial {W_1}_{ij}} =<br>\begin{cases}<br>0 &amp; \text{if } {Z}_{ij} &lt; 0\\<br>    \frac{1}{n} {X}^\mathrm{T} (\hat{y} - y) {W_2}^\mathrm{T} &amp; \text{if } {Z}_{ij} \geq 0<br>\end{cases}<br>$$</p>
<p>其中，${W_1}_{ij}$表示矩阵$W_1$第$i$行第$j$列的值，${Z}_{ij}$表示矩阵$Z$第$i$行第$j$列的值。<br>同理：</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial loss}{\partial b_1} &amp; = \frac{\partial \mathrm{loss}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial O} \frac{\partial O}{\partial H_1} \frac{\partial H_1}{\partial Z} \frac{\partial Z}{\partial b_1}\\<br>&amp; = \frac{\partial loss}{\partial O} \frac{\partial O}{\partial H_1} \frac{\partial H_1}{\partial Z} \frac{\partial Z}{\partial b_1}\\<br>&amp; = \frac{1}{n} (\hat{y} - y) {W_2}^\mathrm{T} \frac{\partial H_1}{\partial Z}\\<br>&amp; = \begin{cases}<br>0 &amp;\text{if } {Z}_{ij} &lt; 0\\<br>\frac{1}{n} \sum_n (\hat{y} - y) {W_2}^\mathrm{T} &amp;\text{if } {Z}_{ij} \geq 0<br>\end{cases}<br>\end{aligned}<br>$$</p>
<p>其中，$\frac{\partial loss}{\partial W_1} \in \mathbb{R}^{m \times h}$，$\frac{\partial loss}{\partial b_1} \in \mathbb{R}^{1 \times h}$。</p>
<h2 id="参数更新-1"><a href="#参数更新-1" class="headerlink" title="参数更新"></a>参数更新</h2><p>求得损失函数对四个参数的偏导数后，我们就可以使用梯度下降进行参数更新：<br>$$<br>W_2 := W_2 - \alpha \frac{\partial \mathrm{loss}}{\partial W_2}\\<br>b_2 := b_2 - \alpha \frac{\partial \mathrm{loss}}{\partial b_2}\\<br>W_1 := W_1 - \alpha \frac{\partial \mathrm{loss}}{\partial W_1}\\<br>b_1 := b_1 - \alpha \frac{\partial \mathrm{loss}}{\partial b_1}\\<br>$$<br>其中，$\alpha$是学习率</p>
<p>以上内容，就是一个三层感知机的前向传播与反向传播过程。</p>
<h2 id="1-导入数据-1"><a href="#1-导入数据-1" class="headerlink" title="1. 导入数据"></a>1. 导入数据</h2><p>使用第一题的手写数字数据集</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> time</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_digits</span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure>
<p>40%做测试集，60%做训练集</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainX, testX, trainY, testY = train_test_split(load_digits()[<span class="hljs-string">'data'</span>], load_digits()[<span class="hljs-string">'target'</span>], test_size = <span class="hljs-number">0.4</span>, random_state = <span class="hljs-number">32</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainX.shape, trainY.shape, testX.shape, testY.shape</span><br></pre></td></tr></table></figure>
<h2 id="2-数据预处理-2"><a href="#2-数据预处理-2" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h2><p>使用和第一题一样的标准化处理方法</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler</span><br><span class="line">s = StandardScaler()</span><br><span class="line">trainX = s.fit_transform(trainX)</span><br><span class="line">testX = s.transform(testX)</span><br></pre></td></tr></table></figure>
<p>接下来还要处理输出。<br>我们的神经网络是针对每个样本，输出其分别属于$K$类的概率，我们要找最大的那个概率，对应的是哪个类。<br>我们当前的trainY和testY，每个样本都是一个类标，我们需要将其变成one_hot编码，也就是，假设当前样本的类别是3，我们需要把它变成一个长度为10的向量，其中第4个元素为1，其他元素都为0。得到的矩阵分别记为trainY_mat和testY_mat。<br>这样，模型训练完成后，会针对每个样本输出十个数，分别代表这个样本属于$0,1,…,9$的概率，那我们只要取最大的那个数的下标，就知道模型认为这个样本是哪类了。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">trainY_mat = np.zeros((len(trainY), <span class="hljs-number">10</span>))</span><br><span class="line">trainY_mat[np.arange(<span class="hljs-number">0</span>, len(trainY), <span class="hljs-number">1</span>), trainY] = <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">testY_mat = np.zeros((len(testY), <span class="hljs-number">10</span>))</span><br><span class="line">testY_mat[np.arange(<span class="hljs-number">0</span>, len(testY), <span class="hljs-number">1</span>), testY] = <span class="hljs-number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainY_mat.shape, testY_mat.shape</span><br></pre></td></tr></table></figure>
<h2 id="3-参数初始化-1"><a href="#3-参数初始化-1" class="headerlink" title="3. 参数初始化"></a>3. 参数初始化</h2><p>这题和上一题的区别是，我们把参数用dict存起来</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize</span><span class="hljs-params">(h, K)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    参数初始化</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    h: int: 隐藏层单元个数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    K: int: 输出层单元个数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    parameters: dict，参数，键是"W1", "b1", "W2", "b2"</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    np.random.seed(<span class="hljs-number">32</span>)</span><br><span class="line">    W_1 = np.random.normal(size = (trainX.shape[<span class="hljs-number">1</span>], h)) * <span class="hljs-number">0.01</span></span><br><span class="line">    b_1 = np.zeros((<span class="hljs-number">1</span>, h))</span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="hljs-number">32</span>)</span><br><span class="line">    W_2 = np.random.normal(size = (h, K)) * <span class="hljs-number">0.01</span></span><br><span class="line">    b_2 = np.zeros((<span class="hljs-number">1</span>, K))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="hljs-string">'W1'</span>: W_1, <span class="hljs-string">'b1'</span>: b_1, <span class="hljs-string">'W2'</span>: W_2, <span class="hljs-string">'b2'</span>: b_2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">parameterst = initialize(<span class="hljs-number">50</span>, <span class="hljs-number">10</span>)</span><br><span class="line">print(parameterst[<span class="hljs-string">'W1'</span>].shape) <span class="hljs-comment"># (64, 50)</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'b1'</span>].shape) <span class="hljs-comment"># (1, 50)</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'W2'</span>].shape) <span class="hljs-comment"># (50, 10)</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'b2'</span>].shape) <span class="hljs-comment"># (1, 10)</span></span><br></pre></td></tr></table></figure>
<h2 id="4-前向传播-1"><a href="#4-前向传播-1" class="headerlink" title="4. 前向传播"></a>4. 前向传播</h2><p>完成Z的计算</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">linear_combination</span><span class="hljs-params">(X, W, b)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    计算Z，Z = XW + b</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray, shape = (n, m)，输入的数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    W: np.ndarray, shape = (m, h)，权重</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    b: np.ndarray, shape = (1, h)，偏置</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    Z: np.ndarray, shape = (n, h)，线性组合后的值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># Z = XW + b</span></span><br><span class="line">    Z = np.dot(X, W) + b</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> Z</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">parameterst = initialize(<span class="hljs-number">50</span>, <span class="hljs-number">10</span>)</span><br><span class="line">Zt = linear_combination(trainX, parameterst[<span class="hljs-string">'W1'</span>], parameterst[<span class="hljs-string">'b1'</span>])</span><br><span class="line">print(Zt.shape) <span class="hljs-comment"># (1078, 50)</span></span><br><span class="line">print(Zt.mean()) <span class="hljs-comment"># -5.27304442123e-19</span></span><br></pre></td></tr></table></figure>
<p>$\rm ReLU$激活函数</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ReLU</span><span class="hljs-params">(X)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    ReLU激活函数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray，待激活的矩阵</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    activations: np.ndarray, 激活后的矩阵</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    activations = X.copy()</span><br><span class="line">    activations[activations &lt; <span class="hljs-number">0</span>] = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">return</span> activations</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">parameterst = initialize(<span class="hljs-number">50</span>, <span class="hljs-number">10</span>)</span><br><span class="line">Zt = linear_combination(trainX, parameterst[<span class="hljs-string">'W1'</span>], parameterst[<span class="hljs-string">'b1'</span>])</span><br><span class="line">Ht = ReLU(Zt)</span><br><span class="line">print(Ht.mean()) <span class="hljs-comment"># 0.0304</span></span><br><span class="line"></span><br><span class="line">Ot = linear_combination(Ht, parameterst[<span class="hljs-string">'W2'</span>], parameterst[<span class="hljs-string">'b2'</span>])</span><br><span class="line">print(Ot.shape) <span class="hljs-comment"># (1078, 10)</span></span><br><span class="line">print(Ot.mean()) <span class="hljs-comment"># 0.0006</span></span><br></pre></td></tr></table></figure>
<p>$\rm softmax$激活  </p>
<p>$$<br>\mathrm{softmax}(O_i) = \frac{\exp{(O_i)}}{\sum^{K}_{k=1} \exp{(O_k)}}<br>$$</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">my_softmax</span><span class="hljs-params">(O)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    softmax激活</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    denominator = np.exp(O).sum(axis = <span class="hljs-number">1</span>, keepdims = <span class="hljs-keyword">True</span>)</span><br><span class="line">    <span class="hljs-keyword">return</span> np.exp(O) / denominator</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">test1 = np.array([[<span class="hljs-number">-1e32</span>, <span class="hljs-number">-1e32</span>, <span class="hljs-number">-1e32</span>]])</span><br><span class="line">test2 = np.array([[<span class="hljs-number">1e32</span>, <span class="hljs-number">1e32</span>, <span class="hljs-number">1e32</span>]])</span><br><span class="line">print(my_softmax(test1))</span><br><span class="line">print(my_softmax(test2))</span><br></pre></td></tr></table></figure>
<p>这里，其实是有数值计算上的问题的，假设，我们最后的输出有三个数，每个数都特别小，理论上来说，通过$\rm softmax$激活后，三个值都是$\frac{1}{3}$。但实际上就不是这样了，实际上会导致分母为0，除法就不能做了。如果每个数都特别大，会导致做指数运算的时候上溢。</p>
<p>我们需要用其他的方法来实现$\rm softmax$。</p>
<p>我们将传入$\rm softmax$的向量，每个元素减去他们中的最大值，即</p>
<p>$$<br>\mathrm{softmax}(O_i) = \mathrm{softmax}(O_i - \mathrm{max(O)})<br>$$</p>
<p>这个式子是成立的，感兴趣的同学可以证明一下上面的式子。</p>
<p>当我们做了这样的变换后，向量$O$中的最大值就变成了0，就不会上溢了，而分母中最少有一项为1，也不会出现下溢导致分母为0的问题了。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">softmax</span><span class="hljs-params">(O)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    softmax激活函数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    O: np.ndarray，待激活的矩阵</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    activations: np.ndarray, 激活后的矩阵</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># YOUR CODE HEER</span></span><br><span class="line">    t = O - np.max(O, axis = <span class="hljs-number">1</span>, keepdims = <span class="hljs-keyword">True</span>)</span><br><span class="line">    denominator = np.exp(t).sum(axis = <span class="hljs-number">1</span>, keepdims = <span class="hljs-keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    activations = np.exp(t) / denominator</span><br><span class="line">    <span class="hljs-keyword">return</span> activations</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">parameterst = initialize(<span class="hljs-number">50</span>, <span class="hljs-number">10</span>)</span><br><span class="line">Zt = linear_combination(trainX, parameterst[<span class="hljs-string">'W1'</span>], parameterst[<span class="hljs-string">'b1'</span>])</span><br><span class="line">Ht = ReLU(Zt)</span><br><span class="line">Ot = linear_combination(Ht, parameterst[<span class="hljs-string">'W2'</span>], parameterst[<span class="hljs-string">'b2'</span>])</span><br><span class="line">y_pred = softmax(Ot)</span><br><span class="line"></span><br><span class="line">print(y_pred.shape)  <span class="hljs-comment"># (1078, 10)</span></span><br><span class="line">print(Ot.mean())     <span class="hljs-comment"># 0.000600192658464</span></span><br><span class="line">print(y_pred.mean()) <span class="hljs-comment"># 0.1</span></span><br></pre></td></tr></table></figure>
<p>接下来是实现损失函数，交叉熵损失函数：</p>
<p>$$<br>\mathrm{loss} = - \frac{1}{n} \sum_n \sum^{K}_{k=1} y_k \log{(\hat{y_k})}<br>$$</p>
<p>这里又会出一个问题，交叉熵损失函数中，我们需要对$\rm softmax$的激活值取对数，也就是$\log{\hat{y}}$，这就要求我们的激活值全都是大于0的数，不能等于0，但是我们实现的$\rm softmax$在有些时候确实会输出0，比如：</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">softmax(np.array([[<span class="hljs-number">1e32</span>, <span class="hljs-number">0</span>, <span class="hljs-number">-1e32</span>]]))</span><br></pre></td></tr></table></figure>
<p>这就使得在计算loss的时候会出现问题，解决这个问题的方法是$\rm log \ softmax$。所谓$\rm log \ softmax$，就是将交叉熵中的对数运算与$\rm softmax$结合起来，避开为0的情况</p>
<p>$$\begin{aligned}<br>\log{\frac{\exp{(O_i)}}{\sum_K \exp{(O_k)}}} &amp;= \log{\frac{\exp{(O_i - \mathrm{max}(O))}}{\sum_K \exp{(O_k - \mathrm{max}(O))}}}\\<br>&amp;= O_i - \mathrm{max}(O) - \log{\sum_K \exp{(O_k - \mathrm{max}(O))}}<br>\end{aligned}<br>$$</p>
<p>这样我们再计算$\rm loss$的时候就可以把输出层的输出直接放到$\rm log \ softmax$中计算，不用先激活，再取对数了。</p>
<p>我们先编写<code>log_softmax</code></p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">log_softmax</span><span class="hljs-params">(x)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    log softmax</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    x: np.ndarray，待激活的矩阵</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    log_activations: np.ndarray, 激活后取了对数的矩阵</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 获取每行的最大值</span></span><br><span class="line">    max_ = np.max(x, axis = <span class="hljs-number">1</span>, keepdims = <span class="hljs-keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 指数运算</span></span><br><span class="line">    exp_x = np.exp(x - max_)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 每行求和</span></span><br><span class="line">    Z = np.sum(exp_x, axis = <span class="hljs-number">1</span>, keepdims = <span class="hljs-keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 求log softmax</span></span><br><span class="line">    log_activations = x - max_ - np.log(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> log_activations</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">parameterst = initialize(<span class="hljs-number">50</span>, <span class="hljs-number">10</span>)</span><br><span class="line">Zt = linear_combination(trainX, parameterst[<span class="hljs-string">'W1'</span>], parameterst[<span class="hljs-string">'b1'</span>])</span><br><span class="line">Ht = ReLU(Zt)</span><br><span class="line">Ot = linear_combination(Ht, parameterst[<span class="hljs-string">'W2'</span>], parameterst[<span class="hljs-string">'b2'</span>])</span><br><span class="line">t = log_softmax(Ot)</span><br><span class="line">print(t.shape)  <span class="hljs-comment"># (1078, 10)</span></span><br><span class="line">print(t.mean()) <span class="hljs-comment"># -2.30259148717</span></span><br></pre></td></tr></table></figure>
<p>然后编写<code>cross_entropy_with_softmax</code></p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cross_entropy_with_softmax</span><span class="hljs-params">(y_true, O)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    求解交叉熵损失函数，这里需要使用log softmax，所以参数分别是真值和未经softmax激活的输出值</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    y_true: np.ndarray，shape = (n, K), 真值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    loss: float, 平均的交叉熵损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 平均交叉熵损失</span></span><br><span class="line">    loss = - np.sum(log_softmax(O) * y_true) / len(y_true)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">parameterst = initialize(<span class="hljs-number">50</span>, <span class="hljs-number">10</span>)</span><br><span class="line">Zt = linear_combination(trainX, parameterst[<span class="hljs-string">'W1'</span>], parameterst[<span class="hljs-string">'b1'</span>])</span><br><span class="line">Ht = ReLU(Zt)</span><br><span class="line">Ot = linear_combination(Ht, parameterst[<span class="hljs-string">'W2'</span>], parameterst[<span class="hljs-string">'b2'</span>])</span><br><span class="line">losst = cross_entropy_with_softmax(trainY_mat, Ot)</span><br><span class="line">print(losst.mean()) <span class="hljs-comment"># 2.30266707958</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    前向传播，从输入一直到输出层softmax激活前的值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray, shape = (n, m)，输入的数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    parameters: dict，参数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 输入层到隐藏层</span></span><br><span class="line">    Z = linear_combination(X, parameters[<span class="hljs-string">'W1'</span>], parameters[<span class="hljs-string">'b1'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 隐藏层的激活</span></span><br><span class="line">    H = ReLU(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 隐藏层到输出层</span></span><br><span class="line">    O = linear_combination(H, parameters[<span class="hljs-string">'W2'</span>], parameters[<span class="hljs-string">'b2'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> O</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">parameterst = initialize(<span class="hljs-number">50</span>, <span class="hljs-number">10</span>)</span><br><span class="line">Ot = forward(trainX, parameterst)</span><br><span class="line">print(Ot.mean()) <span class="hljs-comment"># 0.000600192658464</span></span><br></pre></td></tr></table></figure>
<h2 id="5-反向传播"><a href="#5-反向传播" class="headerlink" title="5. 反向传播"></a>5. 反向传播</h2><p>先计算梯度</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_gradient</span><span class="hljs-params">(y_true, y_pred, H, Z, X, parameters)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    计算梯度</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    y_true: np.ndarray，shape = (n, K), 真值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    H: np.ndarray, shape = (n, h)，隐藏层激活后的值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray, shape = (n, m)，输入的原始数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    parameters: dict，参数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    grads: dict, 梯度</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 计算W2的梯度</span></span><br><span class="line">    dW2 = np.dot(H.T, (y_pred - y_true)) / len(y_pred)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 计算b2的梯度</span></span><br><span class="line">    db2 = np.sum(y_pred - y_true, axis = <span class="hljs-number">0</span>) / len(y_pred)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 计算ReLU的梯度</span></span><br><span class="line">    relu_grad = Z.copy()</span><br><span class="line">    relu_grad[relu_grad &lt; <span class="hljs-number">0</span>] = <span class="hljs-number">0</span></span><br><span class="line">    relu_grad[relu_grad &gt;= <span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 计算W1的梯度</span></span><br><span class="line">    dW1 = np.dot(X.T, np.dot(y_pred - y_true, parameters[<span class="hljs-string">'W2'</span>].T) * relu_grad) / len(y_pred)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 计算b1的梯度</span></span><br><span class="line">    db1 = np.sum((np.dot(y_pred - y_true, parameters[<span class="hljs-string">'W2'</span>].T) * relu_grad), axis = <span class="hljs-number">0</span>) / len(y_pred)</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="hljs-string">'dW2'</span>: dW2, <span class="hljs-string">'db2'</span>: db2, <span class="hljs-string">'dW1'</span>: dW1, <span class="hljs-string">'db1'</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">parameterst = initialize(<span class="hljs-number">50</span>, <span class="hljs-number">10</span>)</span><br><span class="line"></span><br><span class="line">Zt = linear_combination(trainX, parameterst[<span class="hljs-string">'W1'</span>], parameterst[<span class="hljs-string">'b1'</span>])</span><br><span class="line">Ht = ReLU(Zt)</span><br><span class="line">Ot = linear_combination(Ht, parameterst[<span class="hljs-string">'W2'</span>], parameterst[<span class="hljs-string">'b2'</span>])</span><br><span class="line">y_predt = softmax(Ot)</span><br><span class="line"></span><br><span class="line">gradst = compute_gradient(trainY_mat, y_predt, Ht, Zt, trainX, parameterst)</span><br><span class="line"></span><br><span class="line">print(gradst[<span class="hljs-string">'dW1'</span>].sum()) <span class="hljs-comment"># 0.0429186117668</span></span><br><span class="line">print(gradst[<span class="hljs-string">'db1'</span>].sum()) <span class="hljs-comment"># -5.05985151857e-05</span></span><br><span class="line">print(gradst[<span class="hljs-string">'dW2'</span>].sum()) <span class="hljs-comment"># -2.16840434497e-18</span></span><br><span class="line">print(gradst[<span class="hljs-string">'db2'</span>].sum()) <span class="hljs-comment"># -1.34441069388e-17</span></span><br></pre></td></tr></table></figure>
<p>梯度下降，参数更新</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span><span class="hljs-params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    参数更新</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    parameters: dict，参数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    grads: dict, 梯度</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    learning_rate: float, 学习率</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    parameters[<span class="hljs-string">'W2'</span>] -= learning_rate * grads[<span class="hljs-string">'dW2'</span>]</span><br><span class="line">    parameters[<span class="hljs-string">'b2'</span>] -= learning_rate * grads[<span class="hljs-string">'db2'</span>]</span><br><span class="line">    parameters[<span class="hljs-string">'W1'</span>] -= learning_rate * grads[<span class="hljs-string">'dW1'</span>]</span><br><span class="line">    parameters[<span class="hljs-string">'b1'</span>] -= learning_rate * grads[<span class="hljs-string">'db1'</span>]</span><br></pre></td></tr></table></figure>
<p>反向传播，参数更新</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">parameterst = initialize(<span class="hljs-number">50</span>, <span class="hljs-number">10</span>)</span><br><span class="line">print(parameterst[<span class="hljs-string">'W1'</span>].sum())  <span class="hljs-comment"># 0.583495454481</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'b1'</span>].sum())  <span class="hljs-comment"># 0.0</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'W2'</span>].sum())  <span class="hljs-comment"># 0.1888716431</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'b2'</span>].sum())  <span class="hljs-comment"># 0.0</span></span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">Zt = linear_combination(trainX, parameterst[<span class="hljs-string">'W1'</span>], parameterst[<span class="hljs-string">'b1'</span>])</span><br><span class="line">Ht = ReLU(Zt)</span><br><span class="line">Ot = linear_combination(Ht, parameterst[<span class="hljs-string">'W2'</span>], parameterst[<span class="hljs-string">'b2'</span>])</span><br><span class="line">y_predt = softmax(Ot)</span><br><span class="line"></span><br><span class="line">gradst = compute_gradient(trainY_mat, y_predt, Ht, Zt, trainX, parameterst)</span><br><span class="line">update(parameterst, gradst, <span class="hljs-number">0.1</span>)</span><br><span class="line"></span><br><span class="line">print(parameterst[<span class="hljs-string">'W1'</span>].sum())  <span class="hljs-comment"># 0.579203593304</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'b1'</span>].sum())  <span class="hljs-comment"># 5.05985151857e-06</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'W2'</span>].sum())  <span class="hljs-comment"># 0.1888716431</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'b2'</span>].sum())  <span class="hljs-comment"># 1.24683249836e-18</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(y_true, y_pred, H, Z, X, parameters, learning_rate)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    计算梯度，参数更新</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    y_true: np.ndarray，shape = (n, K), 真值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    H: np.ndarray, shape = (n, h)，隐藏层激活后的值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray, shape = (n, m)，输入的原始数据</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    parameters: dict，参数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    learning_rate: float, 学习率</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    grads = compute_gradient(y_true, y_pred, H, Z, X, parameters)</span><br><span class="line">    update(parameters, grads, learning_rate)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">parameterst = initialize(<span class="hljs-number">50</span>, <span class="hljs-number">10</span>)</span><br><span class="line">print(parameterst[<span class="hljs-string">'W1'</span>].sum())  <span class="hljs-comment"># 0.583495454481</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'b1'</span>].sum())  <span class="hljs-comment"># 0.0</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'W2'</span>].sum())  <span class="hljs-comment"># 0.1888716431</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'b2'</span>].sum())  <span class="hljs-comment"># 0.0</span></span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">Zt = linear_combination(trainX, parameterst[<span class="hljs-string">'W1'</span>], parameterst[<span class="hljs-string">'b1'</span>])</span><br><span class="line">Ht = ReLU(Zt)</span><br><span class="line">Ot = linear_combination(Ht, parameterst[<span class="hljs-string">'W2'</span>], parameterst[<span class="hljs-string">'b2'</span>])</span><br><span class="line">y_predt = softmax(Ot)</span><br><span class="line"></span><br><span class="line">backward(trainY_mat, y_predt, Ht, Zt, trainX, parameterst, <span class="hljs-number">0.1</span>)</span><br><span class="line"></span><br><span class="line">print(parameterst[<span class="hljs-string">'W1'</span>].sum())  <span class="hljs-comment"># 0.579203593304</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'b1'</span>].sum())  <span class="hljs-comment"># 5.05985151857e-06</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'W2'</span>].sum())  <span class="hljs-comment"># 0.1888716431</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'b2'</span>].sum())  <span class="hljs-comment"># 1.24683249836e-18</span></span><br></pre></td></tr></table></figure>
<h2 id="6-训练"><a href="#6-训练" class="headerlink" title="6. 训练"></a>6. 训练</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(trainX, trainY, testX, testY, parameters, epochs, learning_rate = <span class="hljs-number">0.01</span>, verbose = False)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    训练</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    trainX: np.ndarray, shape = (n, m), 训练集</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    trainY: np.ndarray, shape = (n, K), 训练集标记</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    testX: np.ndarray, shape = (n_test, m)，测试集</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    testY: np.ndarray, shape = (n_test, K)，测试集的标记</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    parameters: dict，参数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    epochs: int, 要迭代的轮数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    learning_rate: float, default 0.01，学习率</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    verbose: boolean, default False，是否打印损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 存储损失值</span></span><br><span class="line">    training_loss_list = []</span><br><span class="line">    testing_loss_list = []</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(epochs):</span><br><span class="line">        <span class="hljs-comment"># 这里要计算出Z和H，因为后面反向传播计算梯度的时候需要这两个矩阵</span></span><br><span class="line">        Z = linear_combination(trainX, parameters[<span class="hljs-string">'W1'</span>], parameters[<span class="hljs-string">'b1'</span>])</span><br><span class="line">        H = ReLU(Z)</span><br><span class="line">        train_O = linear_combination(H, parameters[<span class="hljs-string">'W2'</span>], parameters[<span class="hljs-string">'b2'</span>])</span><br><span class="line">        train_y_pred = softmax(train_O)</span><br><span class="line">        training_loss = cross_entropy_with_softmax(trainY, train_O)</span><br><span class="line">        </span><br><span class="line">        test_O = forward(testX, parameters)</span><br><span class="line">        testing_loss = cross_entropy_with_softmax(testY, test_O)</span><br><span class="line">        <span class="hljs-keyword">if</span> verbose == <span class="hljs-keyword">True</span>:</span><br><span class="line">            print(<span class="hljs-string">'epoch %s, training loss:%s'</span>%(i + <span class="hljs-number">1</span>, training_loss))</span><br><span class="line">            print(<span class="hljs-string">'epoch %s, testing loss:%s'</span>%(i + <span class="hljs-number">1</span>, testing_loss))</span><br><span class="line">            print()</span><br><span class="line">        </span><br><span class="line">        training_loss_list.append(training_loss)</span><br><span class="line">        testing_loss_list.append(testing_loss)</span><br><span class="line">        </span><br><span class="line">        backward(trainY, train_y_pred, H, Z, trainX, parameters, learning_rate)</span><br><span class="line">    <span class="hljs-keyword">return</span> training_loss_list, testing_loss_list</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line">parameterst = initialize(<span class="hljs-number">50</span>, <span class="hljs-number">10</span>)</span><br><span class="line">print(parameterst[<span class="hljs-string">'W1'</span>].sum())  <span class="hljs-comment"># 0.583495454481</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'b1'</span>].sum())  <span class="hljs-comment"># 0.0</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'W2'</span>].sum())  <span class="hljs-comment"># 0.1888716431</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'b2'</span>].sum())  <span class="hljs-comment"># 0.0</span></span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameterst, <span class="hljs-number">1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-keyword">False</span>)</span><br><span class="line"></span><br><span class="line">print(parameterst[<span class="hljs-string">'W1'</span>].sum())  <span class="hljs-comment"># 0.579203593304</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'b1'</span>].sum())  <span class="hljs-comment"># 5.05985151857e-06</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'W2'</span>].sum())  <span class="hljs-comment"># 0.1888716431</span></span><br><span class="line">print(parameterst[<span class="hljs-string">'b2'</span>].sum())  <span class="hljs-comment"># 1.24683249836e-18</span></span><br></pre></td></tr></table></figure>
<h2 id="7-绘制模型损失值变化曲线"><a href="#7-绘制模型损失值变化曲线" class="headerlink" title="7. 绘制模型损失值变化曲线"></a>7. 绘制模型损失值变化曲线</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_loss_curve</span><span class="hljs-params">(training_loss_list, testing_loss_list)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    绘制损失值变化曲线</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    plt.figure(figsize = (<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))</span><br><span class="line">    plt.plot(training_loss_list, label = <span class="hljs-string">'training loss'</span>)</span><br><span class="line">    plt.plot(testing_loss_list, label = <span class="hljs-string">'testing loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="hljs-string">'epoch'</span>)</span><br><span class="line">    plt.ylabel(<span class="hljs-string">'loss'</span>)</span><br><span class="line">    plt.legend()</span><br></pre></td></tr></table></figure>
<h2 id="8-预测"><a href="#8-预测" class="headerlink" title="8. 预测"></a>8. 预测</h2><p>模型训练完后，我们的就可以进行预测了，需要注意的是，我们的神经网络是针对每个样本，输出其分别属于$K$类的概率，我们要找最大的那个概率，对应的是哪个类。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    预测，调用forward函数完成神经网络对输入X的计算，然后完成类别的划分，取每行最大的那个数的下标作为标记</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    X: np.ndarray, shape = (n, m), 训练集</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    parameters: dict，参数</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    ----------</span></span><br><span class="line"><span class="hljs-string">    prediction: np.ndarray, shape = (n, 1)，预测的标记</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line">    <span class="hljs-comment"># 用forward函数得到softmax激活前的值</span></span><br><span class="line">    O = forward(X, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 计算softmax激活后的值</span></span><br><span class="line">    y_pred = softmax(O)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># 取每行最大的元素对应的下标</span></span><br><span class="line">    prediction = np.argmax(y_pred, axis = <span class="hljs-number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> prediction</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 测试样例</span></span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">parameterst = initialize(<span class="hljs-number">50</span>, <span class="hljs-number">10</span>)</span><br><span class="line">training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameterst, <span class="hljs-number">1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-keyword">False</span>)</span><br><span class="line"></span><br><span class="line">predictiont = predict(testX, parameterst)</span><br><span class="line">accuracy_score(predictiont, testY)  <span class="hljs-comment"># 0.15994436717663421</span></span><br></pre></td></tr></table></figure>
<h2 id="9-训练一个三层感知机"><a href="#9-训练一个三层感知机" class="headerlink" title="9. 训练一个三层感知机"></a>9. 训练一个三层感知机</h2><p>隐藏层单元数设置为50，输出层单元数为10，我们设置学习率为0.03，迭代轮数为1000轮</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">start_time = time()</span><br><span class="line"></span><br><span class="line">h = <span class="hljs-number">50</span></span><br><span class="line">K = <span class="hljs-number">10</span></span><br><span class="line">parameters = initialize(h, K)</span><br><span class="line">training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameters, <span class="hljs-number">1000</span>, <span class="hljs-number">0.03</span>, <span class="hljs-keyword">False</span>)</span><br><span class="line"></span><br><span class="line">end_time = time()</span><br><span class="line">print(<span class="hljs-string">'training time: %s s'</span>%(end_time - start_time))</span><br></pre></td></tr></table></figure>
<p>计算测试集精度</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">prediction = predict(testX, parameters)</span><br><span class="line">accuracy_score(prediction, testY)</span><br></pre></td></tr></table></figure>
<p>绘制损失值变化曲线</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_loss_curve(training_loss_list, testing_loss_list)</span><br></pre></td></tr></table></figure>
<h2 id="更换数据集"><a href="#更换数据集" class="headerlink" title="更换数据集"></a>更换数据集</h2><p>我们换一个数据集，使用MNIST手写数字数据集，我们使用的是kaggle提供的MNIST手写数字识别比赛的训练集。这个数据集还是手写数字的图片，只不过像素变成了 $28 \times 28$，图片的尺寸变大了，而且数据集的样本量也大了。我们取30%为测试集，70%为训练集。训练集样本数有29400个，测试集12600个。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="hljs-string">'data/kaggle_mnist/mnist_train.csv'</span>)</span><br><span class="line">X = data.values[:, <span class="hljs-number">1</span>:].astype(<span class="hljs-string">'float32'</span>)</span><br><span class="line">Y = data.values[:, <span class="hljs-number">0</span>]</span><br><span class="line"></span><br><span class="line">trainX, testX, trainY, testY = train_test_split(X, Y, test_size = <span class="hljs-number">0.3</span>, random_state = <span class="hljs-number">32</span>)</span><br><span class="line"></span><br><span class="line">trainY_mat = np.zeros((len(trainY), <span class="hljs-number">10</span>))</span><br><span class="line">trainY_mat[np.arange(<span class="hljs-number">0</span>, len(trainY), <span class="hljs-number">1</span>), trainY] = <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">testY_mat = np.zeros((len(testY), <span class="hljs-number">10</span>))</span><br><span class="line">testY_mat[np.arange(<span class="hljs-number">0</span>, len(testY), <span class="hljs-number">1</span>), testY] = <span class="hljs-number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainX.shape, trainY.shape, trainY_mat.shape, testX.shape, testY.shape, testY_mat.shape</span><br></pre></td></tr></table></figure>
<p>绘制训练集前10个图像</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">_, figs = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>, figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">4</span>))</span><br><span class="line"><span class="hljs-keyword">for</span> f, img, lbl <span class="hljs-keyword">in</span> zip(figs, trainX[:<span class="hljs-number">10</span>], trainY[:<span class="hljs-number">10</span>]):</span><br><span class="line">    f.imshow(img.reshape((<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)), cmap = <span class="hljs-string">'gray'</span>)</span><br><span class="line">    f.set_title(lbl)</span><br><span class="line">    f.axes.get_xaxis().set_visible(<span class="hljs-keyword">False</span>)</span><br><span class="line">    f.axes.get_yaxis().set_visible(<span class="hljs-keyword">False</span>)</span><br></pre></td></tr></table></figure>
<h2 id="test：请你使用kaggle-MNIST数据集，根据下表设定各个超参数，计算测试集上的精度，绘制损失值变化曲线，填写下表"><a href="#test：请你使用kaggle-MNIST数据集，根据下表设定各个超参数，计算测试集上的精度，绘制损失值变化曲线，填写下表" class="headerlink" title="test：请你使用kaggle MNIST数据集，根据下表设定各个超参数，计算测试集上的精度，绘制损失值变化曲线，填写下表"></a>test：请你使用kaggle MNIST数据集，根据下表设定各个超参数，计算测试集上的精度，绘制损失值变化曲线，填写下表</h2><p>任务流程：</p>
<ol>
<li>对数据集进行标准化处理</li>
<li>设定学习率和迭代轮数进行训练</li>
<li>计算测试集精度</li>
<li>绘制曲线</li>
</ol>
<h6 id="双击此处填写-1"><a href="#双击此处填写-1" class="headerlink" title="双击此处填写"></a>双击此处填写</h6><p>精度保留4位小数；训练时间单位为秒，保留两位小数。</p>
<table>
<thead>
<tr>
<th>隐藏层单元数</th>
<th>学习率</th>
<th>迭代轮数</th>
<th>测试集精度</th>
<th>训练时间(秒)</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>0.1</td>
<td>50</td>
<td>0.8071</td>
<td>50.47</td>
</tr>
<tr>
<td>100</td>
<td>0.1</td>
<td>100</td>
<td>0.8877</td>
<td>100.99</td>
</tr>
<tr>
<td>100</td>
<td>0.1</td>
<td>150</td>
<td>0.8998</td>
<td>147.81</td>
</tr>
<tr>
<td>100</td>
<td>0.1</td>
<td>500</td>
<td>0.9197</td>
<td>497.71</td>
</tr>
<tr>
<td>100</td>
<td>0.01</td>
<td>500</td>
<td>0.8125</td>
<td>484.04</td>
</tr>
</tbody>
</table>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stand = StandardScaler()</span><br><span class="line">trainX_normalized = stand.fit_transform(trainX)</span><br><span class="line">testX_normalized = stand.fit_transform(testX)</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">start_time = time()</span><br><span class="line"></span><br><span class="line">h = <span class="hljs-number">100</span></span><br><span class="line">K = <span class="hljs-number">10</span></span><br><span class="line">parameters = initialize(h, K)</span><br><span class="line">training_loss_list, testing_loss_list = train(trainX_normalized, trainY_mat, testX_normalized, testY_mat, parameters, <span class="hljs-number">50</span>, <span class="hljs-number">0.1</span>)</span><br><span class="line">prediction = predict(testX_normalized, parameters)</span><br><span class="line">print(<span class="hljs-string">'testing accuracy:'</span>, accuracy_score(prediction, testY))</span><br><span class="line">plot_loss_curve(training_loss_list, testing_loss_list)</span><br><span class="line"></span><br><span class="line">end_time = time()</span><br><span class="line">print(<span class="hljs-string">'training time: %s s'</span>%(end_time - start_time))</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">start_time = time()</span><br><span class="line"></span><br><span class="line">h = <span class="hljs-number">100</span></span><br><span class="line">K = <span class="hljs-number">10</span></span><br><span class="line">parameters = initialize(h, K)</span><br><span class="line">training_loss_list, testing_loss_list = train(trainX_normalized, trainY_mat, testX_normalized, testY_mat, parameters, <span class="hljs-number">100</span>, <span class="hljs-number">0.1</span>)</span><br><span class="line">prediction = predict(testX_normalized, parameters)</span><br><span class="line">print(<span class="hljs-string">'testing accuracy:'</span>, accuracy_score(prediction, testY))</span><br><span class="line">plot_loss_curve(training_loss_list, testing_loss_list)</span><br><span class="line"></span><br><span class="line">end_time = time()</span><br><span class="line">print(<span class="hljs-string">'training time: %s s'</span>%(end_time - start_time))</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">start_time = time()</span><br><span class="line"></span><br><span class="line">h = <span class="hljs-number">100</span></span><br><span class="line">K = <span class="hljs-number">10</span></span><br><span class="line">parameters = initialize(h, K)</span><br><span class="line">training_loss_list, testing_loss_list = train(trainX_normalized, trainY_mat, testX_normalized, testY_mat, parameters, <span class="hljs-number">150</span>, <span class="hljs-number">0.1</span>)</span><br><span class="line">prediction = predict(testX_normalized, parameters)</span><br><span class="line">print(<span class="hljs-string">'testing accuracy:'</span>, accuracy_score(prediction, testY))</span><br><span class="line">plot_loss_curve(training_loss_list, testing_loss_list)</span><br><span class="line"></span><br><span class="line">end_time = time()</span><br><span class="line">print(<span class="hljs-string">'training time: %s s'</span>%(end_time - start_time))</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">start_time = time()</span><br><span class="line"></span><br><span class="line">h = <span class="hljs-number">100</span></span><br><span class="line">K = <span class="hljs-number">10</span></span><br><span class="line">parameters = initialize(h, K)</span><br><span class="line">training_loss_list, testing_loss_list = train(trainX_normalized, trainY_mat, testX_normalized, testY_mat, parameters, <span class="hljs-number">500</span>, <span class="hljs-number">0.1</span>)</span><br><span class="line">prediction = predict(testX_normalized, parameters)</span><br><span class="line">print(<span class="hljs-string">'testing accuracy:'</span>, accuracy_score(prediction, testY))</span><br><span class="line">plot_loss_curve(training_loss_list, testing_loss_list)</span><br><span class="line"></span><br><span class="line">end_time = time()</span><br><span class="line">print(<span class="hljs-string">'training time: %s s'</span>%(end_time - start_time))</span><br></pre></td></tr></table></figure>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">start_time = time()</span><br><span class="line"></span><br><span class="line">h = <span class="hljs-number">100</span></span><br><span class="line">K = <span class="hljs-number">10</span></span><br><span class="line">parameters = initialize(h, K)</span><br><span class="line">training_loss_list, testing_loss_list = train(trainX_normalized, trainY_mat, testX_normalized, testY_mat, parameters, <span class="hljs-number">500</span>, <span class="hljs-number">0.01</span>)</span><br><span class="line">prediction = predict(testX_normalized, parameters)</span><br><span class="line">print(<span class="hljs-string">'testing accuracy:'</span>, accuracy_score(prediction, testY))</span><br><span class="line">plot_loss_curve(training_loss_list, testing_loss_list)</span><br><span class="line"></span><br><span class="line">end_time = time()</span><br><span class="line">print(<span class="hljs-string">'training time: %s s'</span>%(end_time - start_time))</span><br></pre></td></tr></table></figure>

        </div>
        
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                    <span class="is-size-6 has-text-grey has-mr-7">#</span>
                    <a class="has-link-grey -link" href="/blog/tags/machine-learning/">machine learning</a>
                </div>
            </div>
        </div>
        
        
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">Like this article? Support the author with</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>Alipay</span>
    <div class="qrcode"><img src="/blog/images/alipay.jpg" alt="Alipay"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>Wechat</span>
    <div class="qrcode"><img src="/blog/images/wx_pay.jpg" alt="Wechat"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/blog/2018/09/13/决策树实现/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">决策树实现</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/blog/2018/09/01/决策树为什么要引入随机数/">
                <span class="level-item">决策树为什么要引入随机数</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>


</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered">
                <div>
                    
                        <figure class="image is-128x128 has-mb-6">
                            <img class="is-rounded" src="/blog/images/avatar.jpg" alt="Davidham">
                        </figure>
                    
                    
                    <p class="is-size-4 is-block">
                        Davidham
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        修电脑的
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>Beijing</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level is-mobile">
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Entradas
                    </p>
                    <p class="title has-text-weight-normal">
                        71
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Categorias
                    </p>
                    <p class="title has-text-weight-normal">
                        8
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Etiquetas
                    </p>
                    <p class="title has-text-weight-normal">
                        34
                    </p>
                </div>
            </div>
        </nav>
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/Davidham3" target="_blank">
                SEGUIR</a>
        </div>
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank" title="Github" href="https://github.com/Davidham3">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" title="Email" href="/blog/chaosong@bjtu.edu.cn">
                
                <i class="fa fa-envelope"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" title="Weibo" href="http://weibo.com/Davidham3">
                
                <i class="fab fa-weibo"></i>
                
            </a>
            
        </div>
        
    </div>
</div>
    
        
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Nube de etiquetas
        </h3>
        <a href="/blog/tags/attention/" style="font-size: 12px;">Attention</a> <a href="/blog/tags/graph/" style="font-size: 19px;">Graph</a> <a href="/blog/tags/hadoop/" style="font-size: 12px;">Hadoop</a> <a href="/blog/tags/kafka/" style="font-size: 10px;">Kafka</a> <a href="/blog/tags/ner/" style="font-size: 10px;">NER</a> <a href="/blog/tags/resnet/" style="font-size: 13px;">ResNet</a> <a href="/blog/tags/stdm/" style="font-size: 10px;">STDM</a> <a href="/blog/tags/sequence/" style="font-size: 10px;">Sequence</a> <a href="/blog/tags/spark/" style="font-size: 11px;">Spark</a> <a href="/blog/tags/spatial-temporal/" style="font-size: 17px;">Spatial-temporal</a> <a href="/blog/tags/time-series/" style="font-size: 16px;">Time Series</a> <a href="/blog/tags/algorithms/" style="font-size: 14px;">algorithms</a> <a href="/blog/tags/computer-vision/" style="font-size: 12px;">computer vision</a> <a href="/blog/tags/dataset/" style="font-size: 11px;">dataset</a> <a href="/blog/tags/deep-learning/" style="font-size: 20px;">deep learning</a> <a href="/blog/tags/graph/" style="font-size: 13px;">graph</a> <a href="/blog/tags/graph-convolutional-network/" style="font-size: 19px;">graph convolutional network</a> <a href="/blog/tags/image-style-transfer/" style="font-size: 11px;">image style transfer</a> <a href="/blog/tags/implicit-feedback/" style="font-size: 10px;">implicit feedback</a> <a href="/blog/tags/language-modeling/" style="font-size: 10px;">language modeling</a> <a href="/blog/tags/large-scale-learning/" style="font-size: 11px;">large-scale learning</a> <a href="/blog/tags/machine-learning/" style="font-size: 18px;">machine learning</a> <a href="/blog/tags/machine-translation/" style="font-size: 10px;">machine translation</a> <a href="/blog/tags/natural-language-processing/" style="font-size: 12px;">natural language processing</a> <a href="/blog/tags/normalization/" style="font-size: 10px;">normalization</a> <a href="/blog/tags/recommender-system/" style="font-size: 12px;">recommender system</a> <a href="/blog/tags/reinforcement-learning/" style="font-size: 10px;">reinforcement learning</a> <a href="/blog/tags/seq2seq/" style="font-size: 10px;">seq2seq</a> <a href="/blog/tags/software/" style="font-size: 17px;">software</a> <a href="/blog/tags/super-resolution/" style="font-size: 11px;">super resolution</a> <a href="/blog/tags/virtual-machine/" style="font-size: 10px;">virtual machine</a> <a href="/blog/tags/vscode/" style="font-size: 10px;">vscode</a> <a href="/blog/tags/已复现/" style="font-size: 15px;">已复现</a> <a href="/blog/tags/随笔/" style="font-size: 10px;">随笔</a>
    </div>
</div>

    
        
<div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Categorias
            </h3>
            <ul class="menu-list">
            <li>
        <a class="level is-marginless" href="/blog/categories/algorithms/">
            <span class="level-start">
                <span class="level-item">algorithms</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">6</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/blog/categories/dataset/">
            <span class="level-start">
                <span class="level-item">dataset</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">2</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/blog/categories/software/">
            <span class="level-start">
                <span class="level-item">software</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">5</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/blog/categories/分布式平台/">
            <span class="level-start">
                <span class="level-item">分布式平台</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">7</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/blog/categories/机器学习/">
            <span class="level-start">
                <span class="level-item">机器学习</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">4</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/blog/categories/自然语言处理/">
            <span class="level-start">
                <span class="level-item">自然语言处理</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/blog/categories/论文阅读笔记/">
            <span class="level-start">
                <span class="level-item">论文阅读笔记</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">41</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/blog/categories/随笔/">
            <span class="level-start">
                <span class="level-item">随笔</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">4</span>
            </span>
        </a></li>
            </ul>
        </div>
    </div>
</div>
    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
            
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Recientes
        </h3>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-07-12T11:57:39.000Z">2019-07-12</time></div>
                    <a href="/blog/2019/07/12/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/" class="has-link-black-ter is-size-6">STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-06-25T08:34:02.000Z">2019-06-25</time></div>
                    <a href="/blog/2019/06/25/self-attention-graph-pooling/" class="has-link-black-ter is-size-6">Self-Attention Graph Pooling</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-05-29T05:37:55.000Z">2019-05-29</time></div>
                    <a href="/blog/2019/05/29/session-based-social-recommendation-via-dynamic-graph-attention-networks/" class="has-link-black-ter is-size-6">Session-based Social Recommendation via Dynamic Graph Attention Networks</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-05-08T08:40:48.000Z">2019-05-08</time></div>
                    <a href="/blog/2019/05/08/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/" class="has-link-black-ter is-size-6">DeepSTN+: Context-aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-04-19T08:40:41.000Z">2019-04-19</time></div>
                    <a href="/blog/2019/04/19/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/" class="has-link-black-ter is-size-6">Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

        
            <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            Archivos
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/07/">
                <span class="level-start">
                    <span class="level-item">July 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/06/">
                <span class="level-start">
                    <span class="level-item">June 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/05/">
                <span class="level-start">
                    <span class="level-item">May 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/04/">
                <span class="level-start">
                    <span class="level-item">April 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">6</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/03/">
                <span class="level-start">
                    <span class="level-item">March 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/02/">
                <span class="level-start">
                    <span class="level-item">February 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/01/">
                <span class="level-start">
                    <span class="level-item">January 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/12/">
                <span class="level-start">
                    <span class="level-item">December 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/10/">
                <span class="level-start">
                    <span class="level-item">October 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/09/">
                <span class="level-start">
                    <span class="level-item">September 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">6</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/08/">
                <span class="level-start">
                    <span class="level-item">August 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/07/">
                <span class="level-start">
                    <span class="level-item">July 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">18</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/06/">
                <span class="level-start">
                    <span class="level-item">June 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">5</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/05/">
                <span class="level-start">
                    <span class="level-item">May 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/04/">
                <span class="level-start">
                    <span class="level-item">April 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/03/">
                <span class="level-start">
                    <span class="level-item">March 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/02/">
                <span class="level-start">
                    <span class="level-item">February 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">6</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/01/">
                <span class="level-start">
                    <span class="level-item">January 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2017/08/">
                <span class="level-start">
                    <span class="level-item">August 2017</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
        
            <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Etiquetas
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/attention/">
                        <span class="tag">Attention</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/graph/">
                        <span class="tag">Graph</span>
                        <span class="tag is-grey">20</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/hadoop/">
                        <span class="tag">Hadoop</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/kafka/">
                        <span class="tag">Kafka</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/ner/">
                        <span class="tag">NER</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/resnet/">
                        <span class="tag">ResNet</span>
                        <span class="tag is-grey">4</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/stdm/">
                        <span class="tag">STDM</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/sequence/">
                        <span class="tag">Sequence</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/spark/">
                        <span class="tag">Spark</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/spatial-temporal/">
                        <span class="tag">Spatial-temporal</span>
                        <span class="tag is-grey">12</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/time-series/">
                        <span class="tag">Time Series</span>
                        <span class="tag is-grey">10</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/algorithms/">
                        <span class="tag">algorithms</span>
                        <span class="tag is-grey">5</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/computer-vision/">
                        <span class="tag">computer vision</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/dataset/">
                        <span class="tag">dataset</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/deep-learning/">
                        <span class="tag">deep learning</span>
                        <span class="tag is-grey">38</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/graph/">
                        <span class="tag">graph</span>
                        <span class="tag is-grey">4</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/graph-convolutional-network/">
                        <span class="tag">graph convolutional network</span>
                        <span class="tag is-grey">20</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/image-style-transfer/">
                        <span class="tag">image style transfer</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/implicit-feedback/">
                        <span class="tag">implicit feedback</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/language-modeling/">
                        <span class="tag">language modeling</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/large-scale-learning/">
                        <span class="tag">large-scale learning</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/machine-learning/">
                        <span class="tag">machine learning</span>
                        <span class="tag is-grey">18</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/machine-translation/">
                        <span class="tag">machine translation</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/natural-language-processing/">
                        <span class="tag">natural language processing</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/normalization/">
                        <span class="tag">normalization</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/recommender-system/">
                        <span class="tag">recommender system</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/reinforcement-learning/">
                        <span class="tag">reinforcement learning</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/seq2seq/">
                        <span class="tag">seq2seq</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/software/">
                        <span class="tag">software</span>
                        <span class="tag is-grey">12</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/super-resolution/">
                        <span class="tag">super resolution</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/virtual-machine/">
                        <span class="tag">virtual machine</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/vscode/">
                        <span class="tag">vscode</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/已复现/">
                        <span class="tag">已复现</span>
                        <span class="tag is-grey">6</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/随笔/">
                        <span class="tag">随笔</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
            </div>
        </div>
    </div>
</div>
        
        </div>
    
</div>

                




<div class="column is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only has-order-3 column-right ">
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Recientes
        </h3>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-07-12T11:57:39.000Z">2019-07-12</time></div>
                    <a href="/blog/2019/07/12/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/" class="has-link-black-ter is-size-6">STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-06-25T08:34:02.000Z">2019-06-25</time></div>
                    <a href="/blog/2019/06/25/self-attention-graph-pooling/" class="has-link-black-ter is-size-6">Self-Attention Graph Pooling</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-05-29T05:37:55.000Z">2019-05-29</time></div>
                    <a href="/blog/2019/05/29/session-based-social-recommendation-via-dynamic-graph-attention-networks/" class="has-link-black-ter is-size-6">Session-based Social Recommendation via Dynamic Graph Attention Networks</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-05-08T08:40:48.000Z">2019-05-08</time></div>
                    <a href="/blog/2019/05/08/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/" class="has-link-black-ter is-size-6">DeepSTN+: Context-aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-04-19T08:40:41.000Z">2019-04-19</time></div>
                    <a href="/blog/2019/04/19/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/" class="has-link-black-ter is-size-6">Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/blog/categories/论文阅读笔记/">论文阅读笔记</a>
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            Archivos
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/07/">
                <span class="level-start">
                    <span class="level-item">July 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/06/">
                <span class="level-start">
                    <span class="level-item">June 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/05/">
                <span class="level-start">
                    <span class="level-item">May 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/04/">
                <span class="level-start">
                    <span class="level-item">April 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">6</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/03/">
                <span class="level-start">
                    <span class="level-item">March 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/02/">
                <span class="level-start">
                    <span class="level-item">February 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2019/01/">
                <span class="level-start">
                    <span class="level-item">January 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/12/">
                <span class="level-start">
                    <span class="level-item">December 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/10/">
                <span class="level-start">
                    <span class="level-item">October 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/09/">
                <span class="level-start">
                    <span class="level-item">September 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">6</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/08/">
                <span class="level-start">
                    <span class="level-item">August 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/07/">
                <span class="level-start">
                    <span class="level-item">July 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">18</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/06/">
                <span class="level-start">
                    <span class="level-item">June 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">5</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/05/">
                <span class="level-start">
                    <span class="level-item">May 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/04/">
                <span class="level-start">
                    <span class="level-item">April 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/03/">
                <span class="level-start">
                    <span class="level-item">March 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/02/">
                <span class="level-start">
                    <span class="level-item">February 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">6</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2018/01/">
                <span class="level-start">
                    <span class="level-item">January 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/blog/archives/2017/08/">
                <span class="level-start">
                    <span class="level-item">August 2017</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Etiquetas
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/attention/">
                        <span class="tag">Attention</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/graph/">
                        <span class="tag">Graph</span>
                        <span class="tag is-grey">20</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/hadoop/">
                        <span class="tag">Hadoop</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/kafka/">
                        <span class="tag">Kafka</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/ner/">
                        <span class="tag">NER</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/resnet/">
                        <span class="tag">ResNet</span>
                        <span class="tag is-grey">4</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/stdm/">
                        <span class="tag">STDM</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/sequence/">
                        <span class="tag">Sequence</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/spark/">
                        <span class="tag">Spark</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/spatial-temporal/">
                        <span class="tag">Spatial-temporal</span>
                        <span class="tag is-grey">12</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/time-series/">
                        <span class="tag">Time Series</span>
                        <span class="tag is-grey">10</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/algorithms/">
                        <span class="tag">algorithms</span>
                        <span class="tag is-grey">5</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/computer-vision/">
                        <span class="tag">computer vision</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/dataset/">
                        <span class="tag">dataset</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/deep-learning/">
                        <span class="tag">deep learning</span>
                        <span class="tag is-grey">38</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/graph/">
                        <span class="tag">graph</span>
                        <span class="tag is-grey">4</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/graph-convolutional-network/">
                        <span class="tag">graph convolutional network</span>
                        <span class="tag is-grey">20</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/image-style-transfer/">
                        <span class="tag">image style transfer</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/implicit-feedback/">
                        <span class="tag">implicit feedback</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/language-modeling/">
                        <span class="tag">language modeling</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/large-scale-learning/">
                        <span class="tag">large-scale learning</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/machine-learning/">
                        <span class="tag">machine learning</span>
                        <span class="tag is-grey">18</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/machine-translation/">
                        <span class="tag">machine translation</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/natural-language-processing/">
                        <span class="tag">natural language processing</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/normalization/">
                        <span class="tag">normalization</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/recommender-system/">
                        <span class="tag">recommender system</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/reinforcement-learning/">
                        <span class="tag">reinforcement learning</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/seq2seq/">
                        <span class="tag">seq2seq</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/software/">
                        <span class="tag">software</span>
                        <span class="tag is-grey">12</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/super-resolution/">
                        <span class="tag">super resolution</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/virtual-machine/">
                        <span class="tag">virtual machine</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/vscode/">
                        <span class="tag">vscode</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/已复现/">
                        <span class="tag">已复现</span>
                        <span class="tag is-grey">6</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/blog/tags/随笔/">
                        <span class="tag">随笔</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
            </div>
        </div>
    </div>
</div>
    
    
</div>

            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/blog/">
                
                    Davidham&#39;s blog
                
                </a>
                <p class="is-size-7">
                &copy; 2019 Davidham&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Creative Commons" href="https://creativecommons.org/">
                        
                        <i class="fab fa-creative-commons"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/">
                        
                        <i class="fab fa-creative-commons-by"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-Hans");</script>


    
    
    
    <script src="/blog/js/animation.js"></script>
    

    
    
    
    <script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
    <script src="/blog/js/gallery.js" defer></script>
    

    
    

<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


    
    
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>

    
    

<a id="back-to-top" title="Back to Top" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/blog/js/back-to-top.js" defer></script>


    
    

    
    
    
    

    
    
    
    
    
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>
    <script src="/blog/js/clipboard.js" defer></script>
    

    
    
    


<script src="/blog/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Entradas',
                PAGES: 'Pages',
                CATEGORIES: 'Categorias',
                TAGS: 'Etiquetas',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/blog/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/blog/js/insight.js" defer></script>
<link rel="stylesheet" href="/blog/css/search.css">
<link rel="stylesheet" href="/blog/css/insight.css">
    
</body>
</html>