<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/blog/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/blog/css/main.css?v=6.0.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png?v=6.0.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png?v=6.0.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png?v=6.0.0">


  <link rel="mask-icon" href="/blog/images/logo.svg?v=6.0.0" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Davidham的博客，写点学习笔记啥的，邮箱17120410@bjtu.edu.cn">
<meta property="og:type" content="website">
<meta property="og:title" content="categories">
<meta property="og:url" content="http://yoursite.com/categories/index.html">
<meta property="og:site_name" content="Davidham&#39;s blog">
<meta property="og:description" content="Davidham的博客，写点学习笔记啥的，邮箱17120410@bjtu.edu.cn">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-02-19T13:29:39.253Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="categories">
<meta name="twitter:description" content="Davidham的博客，写点学习笔记啥的，邮箱17120410@bjtu.edu.cn">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Gemini',
    version: '6.0.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/categories/"/>





  <title>Image Style Transfer Using Convolutional Neural Networks 阅读笔记 | Davidham's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/blog/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Davidham's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">修电脑的</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/blog/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/blog/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/blog/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/blog/2018/02/24/image-style-transfer-using-convolutional-neural-networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Davidham">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/blog/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Davidham's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Image Style Transfer Using Convolutional Neural Networks 阅读笔记</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-24T23:51:39+08:00">2018-02-24</time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/机器学习/论文阅读笔记/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>大体原理：选择两张图片，一张作为风格图片，一张作为内容图片，任务是将风格图片中的风格，迁移到内容图片上。方法也比较简单，利用在ImageNet上训练好的VGG19，因为这种深层次的卷积神经网络的卷积核可以有效的捕捉一些特征，越靠近输入的卷积层捕捉到的信息层次越低，而越靠近输出的卷积层捕捉到的信息层次越高，因此可以用高层次的卷积层捕捉到的信息作为对风格图片风格的捕捉。而低层次的卷积层用来捕捉内容图片中的内容。所以实际的操作就是，将内容图片扔到训练好的VGG19中，取出低层次的卷积层的输出，保存起来，然后再把风格图片放到VGG19中，取出高层次的卷积层的输出，保存起来。然后随机生成一张图片，扔到VGG19中，将刚才保存下来的卷积层的输出的那些卷积层的结果拿出来，和那些保存的结果做个loss，然后对输入的随机生成的图片进行优化即可。<br><a id="more"></a><br>论文：Gatys L A, Ecker A S, Bethge M. Image Style Transfer Using Convolutional Neural Networks[C]// Computer Vision and Pattern Recognition. IEEE, 2016:2414-2423. CVPR 2016.</p>
<h1 id="Image-Style-Transfer-Using-Convolutional-Neural-Networks"><a href="#Image-Style-Transfer-Using-Convolutional-Neural-Networks" class="headerlink" title="Image Style Transfer Using Convolutional Neural Networks"></a>Image Style Transfer Using Convolutional Neural Networks</h1><h1 id="大体原理"><a href="#大体原理" class="headerlink" title="大体原理"></a>大体原理</h1><p>选择两张图片，一张作为风格图片，一张作为内容图片，任务是将风格图片中的风格，迁移到内容图片上。方法也比较简单，利用在ImageNet上训练好的VGG19，因为这种深层次的卷积神经网络的卷积核可以有效的捕捉一些特征，越靠近输入的卷积层捕捉到的信息层次越低，而越靠近输出的卷积层捕捉到的信息层次越高，因此可以用高层次的卷积层捕捉到的信息作为对风格图片风格的捕捉。而低层次的卷积层用来捕捉内容图片中的内容。所以实际的操作就是，将内容图片扔到训练好的VGG19中，取出低层次的卷积层的输出，保存起来，然后再把风格图片放到VGG19中，取出高层次的卷积层的输出，保存起来。然后随机生成一张图片，扔到VGG19中，将刚才保存下来的卷积层的输出的那些卷积层的结果拿出来，和那些保存的结果做个loss，然后对输入的随机生成的图片进行优化即可。(Fig2)<br><img src="/blog/2018/02/24/image-style-transfer-using-convolutional-neural-networks/Fig2.PNG" alt="Fig2"></p>
<center>Figure 2. Style transfer algorithm. First content and style features are extracted and stored. The style image $\vec{a}$ is passed through the network and its style representation $A^l$ on all layers included are computed and stored(left). The content image $\vec{p}$ is passed through the network and the content representation $P^l$ in one layer is stored(right). Then a random white noise image $\vec{x}$ is passed through the network and its style features $G^l$ and content features $F^l$ are computed. On each layer included in the style representation, the element-wise mean squared difference between $G^l$ and $A^l$ is computed to give the style loss $\mathcal{L}_{style}$(left). Also the mean squared difference between $F^l$ and $P^l$ is computed to give the content loss $\mathcal{L}_{content}(right)$. The total loss $\mathcal{L}_{total}$ is then a linear combination between the content and the style loss. Its derivative with respect to the pixel values can be computed using error back-propagation(middle). This gradient is used to iteratively update the image $\vec{x}$ until it simultaneously matches the style features of the style image $\vec{a}$ and the content features of the content image $\vec{p}$(middle, bottom).</center>

<h1 id="Deep-image-representations"><a href="#Deep-image-representations" class="headerlink" title="Deep image representations"></a>Deep image representations</h1><p>We used the feature space provided by a normalized version of the 16 convolutional and 5 pooling layers of the 19-layer VGG network. We normalized the network by scaling the weights such that the mean activation of each convolutional filter over images and positions is equal to one. Such re-scaling can be done for the VGG network without changing its output, because it contains only rectifying linear activation functions and no normalization or pooling over feature maps.<br>其实这里我不是很明白为什么不会影响输出。</p>
<h2 id="content-representation"><a href="#content-representation" class="headerlink" title="content representation"></a>content representation</h2><p>A layer with $N_l$ distinct filters has $N_l$ feature maps each of size $M_l$, where $M_l$ is the height times the width of the feature map. So the responses in a layer $l$ can be stored in a matrix $F^l \in \mathcal{R}^{N_l \times M_l}$ where $F^l_{ij}$ is the activation of the $i^{th}$ filter at position $j$ in layer $l$.<br>Let $\vec{p}$ and $\vec{x}$ be the original image and the image that is generated, and $P^l$ and $F^l$ their respective feature representation in layer $l$.<br>We then define the squared-error loss between the two feature representations</p>
<script type="math/tex; mode=display">\mathcal{L}_{content}(\vec{p}, \vec{x}, l) = \frac{1}{2}\sum_{i, j}(F^l_{ij}-P^l_{ij})^2</script><p>The derivative of this loss with respect to the activations in layer $l$ equals</p>
<p>\begin{equation}<br>\frac{\partial{\mathcal{L}_{content}}}{\partial{F^l_{ij}}}=\left\{<br>\begin{aligned}<br>&amp; (F^l - P^l)_{ij} &amp; if \ F^l_{ij} &gt; 0 \\<br>&amp; 0 &amp; if \ F^l_{ij} &lt; 0<br>\end{aligned}<br>\right.<br>\end{equation}</p>
<p>from which the gradient with respect to the image $\vec{x}$ can be computed using standard error back-propagation.</p>
<p>When Convolutional Neural Networks are trained on object recongnition, they develop a representation of the image that makes object information increasingly explicit along the processing hierarchy. Higher layers in the network capture the high-level <em>content</em> in terms of objects and their arrangement in the input image but do not constrain the exact pixel values of the reconstruction very much. We therefore refer to the feature responses in higher layers of the network as the <em>content representation</em>.</p>
<h2 id="style-representation"><a href="#style-representation" class="headerlink" title="style representation"></a>style representation</h2><p>To obtain a representation of the style of an input image, we use a feature space designed to capture texture information. This feature space can be built on top of the filter responses in any layer of the network. It consists of the correlations between the different filter responses, where the expecation is taken over the spatial extent of the feature maps. These feature correlations are given by the Gram matrix $G^l \in \mathcal{R}^{N_l \times N_l}$, where $G^l_{ij}$ is the inner product between the vecotrized feature maps $i$ and $j$ in layer $l$:</p>
<script type="math/tex; mode=display">G^l_{ij}=\sum_kF^l_{ik}F^l_{jk}.</script><p>By inducing the feature corelations of multiple layers, we obtain a stationary, multi-scale representation of the input image, which captures its texture information but not the global arrangement. Again, we can visualise the information captured by these style feature spaces built on different layers of the network by constructing an image that matches the style representation of a given input image. This is done by using gradient descent from a white noise image to minimise the mean-squared distance between the entries of the Gram matrices from the original image and the Gram matrices of the image to be generated.<br>Let $\vec{a}$ and $\vec{x}$ be the original image and the image that is generated, and $A^l$ and $G^l$ their respective style representation in layer $l$. The contribution of layer $l$ to the toal loss is then</p>
<script type="math/tex; mode=display">E_l = \frac{1}{4N^2_lM^2_l}\sum_{i,j}(G^l_{ij} - A^l_{ij})^2</script><p>and the total style loss is</p>
<script type="math/tex; mode=display">\mathcal{L}_{style}(\vec{a}, \vec{x})=\sum^L_{l=0}w_lE_l,</script><p>where $w_L$ are weighting factors of the contribution of each layer to the total loss (see below for specific values of $w_l$ in our results). The derivative of $E_l$ with respect to the activations in layer $l$ can be computed analytically:</p>
<p>\begin{equation}<br>\frac{\partial{E_l}}{\partial{F^l_{ij}}}=\left\{<br>\begin{aligned}<br>&amp; \frac{1}{N^2_lM^2_l}((F^l)^T(G^l-A^l))_{ji} &amp; if \ F^l_{ij} &gt; 0 \\<br>&amp; 0 &amp; if \ F^l_{ij} &lt; 0<br>\end{aligned}<br>\right.<br>\end{equation}<br>The gradient of $E_l$ with respect to the pixel values $\vec{x}$ can be readily computed using standard error back-propagation.</p>
<h2 id="style-transfer"><a href="#style-transfer" class="headerlink" title="style transfer"></a>style transfer</h2><p>To transfer the style of an artwork $\vec{a}$ onto a photograph $\vec{p}$ we synthesise a new image that simultaneously matches the content representation of $\vec{p}$ and the style representation of $\vec{a}$. Thus we jointly minimise the distance of the feature representations of a white noise image fron the content representation of the photograph in one layer and the style representation of the painting defined on a numebr of layers of the Convolutional Neural Network. The loss function we minimise is</p>
<script type="math/tex; mode=display">\mathcal{L}_{total}(\vec{p}, \vec{a}, \vec{x})=\alpha \mathcal{L}_{content}(\vec{p}, \vec{x}) + \beta \mathcal{L}_{style}(\vec{a}, \vec{x})</script><p>where $\alpha$ and $\beta$ are the weighting factors for content and style reconstruction, respectively. The gradient with respect to the pixel values $\frac{\partial{\mathcal{L}_{total}}}{\partial{\vec{x}}}$ can be used as input for some numerical optimisation strategy. Here we use <strong>L-BFGS</strong>, which we found to work best for image synthesis. To extract image information on comparable scales, we always resized the style image to the same size as the content image before computing its feature representations.</p>
<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><h2 id="Trade-off-between-content-and-style-matching"><a href="#Trade-off-between-content-and-style-matching" class="headerlink" title="Trade-off between content and style matching"></a>Trade-off between content and style matching</h2><p>Since the loss function we minimise during image synthesis is a linear combination between the loss functions for content and style respectively, we can smoothly regulate the emphasis on either reconstructing the content or the style(Fig4).<br><img src="/blog/2018/02/24/image-style-transfer-using-convolutional-neural-networks/Fig4.PNG" alt="Fig4"></p>
<center>Figure 4. Relative weighting of matching content and style of the respective source images. The ratio $\alpha / \beta$ between matching the content and matching the style increases from top left to bottom right. A high emphasis on the style effectively produces a texturised version of the style image(top left). A high emphasis on the content produces an image with only little stylisation(bottom right). In practice one can smoothly interpolate between the two extremes.</center>

<h2 id="Effect-of-different-layers-of-the-Convolutional-Neural-Network"><a href="#Effect-of-different-layers-of-the-Convolutional-Neural-Network" class="headerlink" title="Effect of different layers of the Convolutional Neural Network"></a>Effect of different layers of the Convolutional Neural Network</h2><p><img src="/blog/2018/02/24/image-style-transfer-using-convolutional-neural-networks/Fig5.PNG" alt="Fig5"></p>
<p><center>Figure 5. The effect of matching the content representation in different layers of the network. Matching the content on layer 'conv2_2' preserves much of the fine structure of the original photograph and the synthesised image looks as if the texture of the painting is simply blended over the photograph(middle). When matching the content on layer 'conv4_2' the texture of the painting and the content of the photograph merge together such that the content of photograph is displayed in the style of the painting(bottom). Both images were generated with the same choice of parameters($\alpha / \beta = 1 \times 10^{-3}$). The painting that served as the style image is shown in the bottom left corner and is name <i>Jesuiten Ⅲ</i> by Lyonel Feininger, 1915.</center><br>Another important factor in the image synthesis process is the choice of layers to match the content and style representation on. As outlined above, the style representation is a multi-scale representation that includes multiple layers of the neural network. The number and position of these layers determines the local scale on which the style is matched, leading to different visual experiences. We find that matching the style representations up to higher layers in the network preserves local images structures an increasingly large scale, leading to a smoother and more continuous visual experience. Thus, the visually most appealing images are usually created by matching the style representation up to high layers in the network, which is why for all images shown we match the style features in layers ‘conv1_1’, ‘conv2_1’, ‘conv3_1’, ‘conv4_1’and ‘conv5_1’ of the network.<br>To analyse the effect of using different layers to match the content features, we present a style transfer result obtained by stylising a photograph with the same artwork and parameter configuration ($\alpha / \beta = 1 \times 10^{-3}$), but in one matching the content features on layer ‘conv2_2’ and in the other on layer ‘conv4_2’(Fig5). When matching the content on a lower layer of the network, the algorithm matches much of the detailed pixel information in the photograph and the generated image appears as if the texture of the artwork is merely blended over the photograph(Fig5, middle). In contrast, when matching the content features on a higher layer of the network, deatiled pixel information of the photograph is not as strongly constraint and the texture of the artwork and the content of the photograph are properly merged. That is, the fine structure of the image, for example the edges and colour map, is altered such that it agrees with the style of the artwork while displaying the content of the photograph(Fig5, bottom).</p>
<h2 id="Initialisation-of-gradient-descent"><a href="#Initialisation-of-gradient-descent" class="headerlink" title="Initialisation of gradient descent"></a>Initialisation of gradient descent</h2><p><img src="/blog/2018/02/24/image-style-transfer-using-convolutional-neural-networks/Fig6.PNG" alt="Fig6"></p>
<p><center>Figure 6. Initialisation of the gradient descent. <b>A</b> Initialised from the content image. <b>B</b> Initialised from the style image. <b>C</b> Four samples of images initialised from different white noise images. For all images the ratio $\alpha / \beta$ was equal to $1 \times 10^{-3}$</center><br>We have initialised all images shown so far with white noise. However, one could also initialise the image synthesis with either the content image or the style image. We explored these two alternatives(Fig6 A, B): although they bias the final image somewhat towards the spatial structure of the initialisation, the different intialisation do not seem to have a strong effect on the outcome of the synthesis procedure. It should be noted that only initialising with noise allows to generate an arbitrary number of new images(Fig6 C). Initialising with a fixed image always deterministically leads to the same outcome (up to stochasticity in the gradient descent procedure).</p>
<h1 id="implementation"><a href="#implementation" class="headerlink" title="implementation"></a>implementation</h1><p>关于实现的部分，我自己用mxnet实现了一下，但是发现和mxnet的example里面给的非常不一样。在他们的实现里面提到了Total variation denoising。而且，论文中的loss function是sum of square，而图2中给出是MSE，取了个平均值。我实现是时候没有取平均，导致loss很大，但是也可以训练。但是自己实现的梯度下降很难收敛，需要对梯度进行归一化，后来使用MXNet的gluon的Trainer训练会比原来好很多。</p>
<h2 id="Total-variation-denoising"><a href="#Total-variation-denoising" class="headerlink" title="Total variation denoising"></a>Total variation denoising</h2><p>In signal processing, total variation denoising, also known as total variation regularization, is a process, most often used in digital image processing, that has applications in noise removal.<br><img src="/blog/2018/02/24/image-style-transfer-using-convolutional-neural-networks/ROF_Denoising_Example.png" alt="Total variation denoising"></p>
<p><center>Example of application of the Rudin et al.[1] total variation denoising technique to an image corrupted by Gaussian noise. This example created using demo_tv.m by Guy Gilboa, see external links.</center><br>It is based on the principle that signals with excessive and possibly spurious detail have high total variation, that is, the integral of the absolute gradient of the signla is high. According to this principle, reducing the total variation of the signal subject to it being a close match to the original signal, removes unwanted detail whilst preserving important details such as edges. The concept was pioneered by Rudin, Osher, and Fatemi in 1992 and so is today known as the ROF model.<br>This noise removal technique has advantages over simple techniques such as linear smoothing or median filtering which reduce noise but at the same time smooth away edges to a greater or lesser degree. By contrast, total variation denoising is remarkably effective at simultaneously preserving edges whilst smoothing away noise in flat regions, even at low signal-to-noise ratios.</p>
<h3 id="1D-signal-series"><a href="#1D-signal-series" class="headerlink" title="1D signal series"></a>1D signal series</h3><p>For a digital signal $y_n$, we can, for example, define the total variation as:</p>
<script type="math/tex; mode=display">V(y)=\sum_n\vert y_{n+1}-y_n\vert</script><p>Given an input signal $x_n$, the goal of total variation denoising is to find an approximation, call it $y_n$, that has smaller total variation than $x_n$ but is “close” to $x_n$. One measure of closeness is the sum of square errors:</p>
<script type="math/tex; mode=display">E(x, y)=\frac{1}{2}\sum_n(x_n - y_n)^2</script><p>So the total variation denoising problem amounts to minimizing the following discrete functional over the signal $y_n$:</p>
<script type="math/tex; mode=display">E(x, y) + \lambda V(y)</script><p>By differentiating this functional with respect to $y_n$, we can derive a corresponding Euler-lagrange equation, that can be numerically integrated with the original signal $x_n$ as initial condition. This was the original approach. Alternatively, since this is a convex functional, techniques from convex optimization can be used to minimize it and find the solution $y_n$.</p>
<h3 id="Regularization-properties"><a href="#Regularization-properties" class="headerlink" title="Regularization properties"></a>Regularization properties</h3><p>The regularization parameter $\lambda $ plays a critical role in the denoising process. When $\lambda = 0$, there is no smoothing and the result is the same as minimizing the sum of squares. As $\lambda \to \infty $, however, the total variation term plays an increasingly strong role, which forces the result to have smaller total variation, at the expanse of being less like the input (noisy) signal. Thus, the choice of regularization parameter is critical to achieving just the right amount of noise removal.</p>
<h3 id="2D-signal-images"><a href="#2D-signal-images" class="headerlink" title="2D signal images"></a>2D signal images</h3><p>We now consider 2D signals $y$, such as images. The total variation norm proposed by the 1992 paper is</p>
<script type="math/tex; mode=display">V(y) = \sum_{i,j}\sqrt{\vert y_{i+1,j}-y_{i,j}\vert ^2 + \vert y_{i, j+1} - y_{i, j}\vert ^2}</script><p>and is isotropic and not differentiable. A variation that is sometimes used, since it may sometimes be easier to minimize, is an anisotropic version</p>
<script type="math/tex; mode=display">V_{aniso}(y) = \sum_{i,j}\sqrt{\vert y_{i+1,j}-y_{i,j}\vert ^2} + \sqrt{\vert y_{i,j+1} - y_{i,j}\vert ^2} = \sum_{i,j}\vert y_{i+1,j}-y_{i,j}\vert + \vert y_{i,j+1}-y_{i,j}\vert</script><p>The standard total variation denoising problem is still of the form</p>
<script type="math/tex; mode=display">\min_yE(x,y)+\lambda V(y)</script><p>where $E$ is the 2D L2 norm. In contrast to the 1D case, solving this denoising is non-trivial. A recent algorithm that solves this is known as the primal dual method.<br>Due in part to much research in compressed sensing in the mid-2000s, there are many algorithms, such as the split-Bregman method, that solve variants of this problem.</p>
<p>不过我个人在实现的时候，实现了两个版本，一个是增加了total variation denoising，另一个是没增加total variation denoising的a。<br>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> transform</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon.model_zoo <span class="keyword">import</span> vision <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> Parameter</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"></span><br><span class="line">content_image_path = <span class="string">'../../gluon-tutorial-zh/img/pine-tree.jpg'</span></span><br><span class="line">style_image_path = <span class="string">'the_starry_night.jpg'</span></span><br><span class="line"></span><br><span class="line">rgb_mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">rgb_std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocessing</span><span class="params">(img, image_shape, ctx = mx.cpu<span class="params">()</span>)</span>:</span></span><br><span class="line">    newImage = transform.resize(img, image_shape)</span><br><span class="line">    newImage = newImage.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    newImage = (newImage - rgb_mean.reshape(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)) / rgb_std.reshape(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> nd.array(np.expand_dims(newImage, <span class="number">0</span>), ctx = ctx)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postprocessing</span><span class="params">(img)</span>:</span></span><br><span class="line">    newImage = img[<span class="number">0</span>].asnumpy() * rgb_std.reshape(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>) + rgb_mean.reshape(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> newImage.transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)).clip(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_net</span><span class="params">(style_layers, content_layers)</span>:</span></span><br><span class="line">    net = nn.HybridSequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max(style_layers + content_layers) + <span class="number">1</span>):</span><br><span class="line">        net.add(pretrained_net.features[i])</span><br><span class="line">    net.hybridize()</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_features</span><span class="params">(net, img, content_layers, style_layers)</span>:</span></span><br><span class="line">    x = img.copy()</span><br><span class="line">    content_results = []</span><br><span class="line">    style_results = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(net)):</span><br><span class="line">        x = net[i](x)</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> content_layers:</span><br><span class="line">            content_results.append(x)</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> style_layers:</span><br><span class="line">            style_results.append(x)</span><br><span class="line">    <span class="keyword">return</span> content_results, style_results</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">content_loss</span><span class="params">(content_results, content_target)</span>:</span></span><br><span class="line">    losses = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(content_results)):</span><br><span class="line">        losses.append((content_results[i] - content_target[i]).square().sum())</span><br><span class="line">    <span class="keyword">return</span> nd.add_n(*losses) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram</span><span class="params">(feature_map)</span>:</span></span><br><span class="line">    N = feature_map.shape[<span class="number">1</span>]</span><br><span class="line">    M = np.prod(feature_map.shape[<span class="number">2</span>:])</span><br><span class="line">    new_feature_map = feature_map.reshape((N, M))</span><br><span class="line">    <span class="keyword">return</span> nd.dot(new_feature_map, new_feature_map.T)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_loss</span><span class="params">(style_results, style_target, weights)</span>:</span></span><br><span class="line">    losses = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(style_results)):</span><br><span class="line">        l = (gram(style_results[i]) - style_target[i]).square().sum() \</span><br><span class="line">            / (<span class="number">4</span> * np.prod(style_results[i].shape[<span class="number">1</span>:]))</span><br><span class="line">        losses.append(weights[i] * l)</span><br><span class="line">    <span class="keyword">return</span> nd.add_n(*losses)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_loss</span><span class="params">(content_loss_result, style_loss_result, ratio)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> content_loss_result * ratio + style_loss_result</span><br><span class="line"></span><br><span class="line">style_layers = [<span class="number">2</span>, <span class="number">7</span>, <span class="number">16</span>, <span class="number">25</span>, <span class="number">34</span>] <span class="comment"># 这里与论文不同，我选的层比论文给出的更深，为了捕捉到更抽象的style</span></span><br><span class="line">content_layers = [<span class="number">21</span>]</span><br><span class="line">net = get_net(style_layers, content_layers)</span><br><span class="line"></span><br><span class="line">content_image = io.imread(content_image_path)</span><br><span class="line">style_image = io.imread(style_image_path)</span><br><span class="line"></span><br><span class="line">pretrained_net = models.vgg19(pretrained=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">ctx = mx.gpu(<span class="number">1</span>)</span><br><span class="line">net.collect_params().reset_ctx(ctx)</span><br><span class="line"></span><br><span class="line">content_img = preprocessing(content_image, (<span class="number">200</span>, <span class="number">300</span>), ctx = ctx)</span><br><span class="line">style_img = preprocessing(style_image, (<span class="number">200</span>, <span class="number">300</span>), ctx = ctx)</span><br><span class="line"></span><br><span class="line">output = Parameter(<span class="string">'output'</span>, shape=content_img.shape)</span><br><span class="line">output.initialize(ctx=ctx)</span><br><span class="line"><span class="comment"># output.set_data(nd.random_normal(shape = content_img.shape).abs())</span></span><br><span class="line">output.set_data(content_img)</span><br><span class="line"></span><br><span class="line">content_img_result, _ = extract_features(net, content_img, content_layers, style_layers)</span><br><span class="line">_, style_img_result = extract_features(net, style_img, content_layers, style_layers)</span><br><span class="line"></span><br><span class="line">content_results, style_results = extract_features(net, output.data(), content_layers, style_layers)</span><br><span class="line">style_target = [gram(i) <span class="keyword">for</span> i <span class="keyword">in</span> style_img_result]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">trainer = Trainer([output], <span class="string">'adam'</span>,</span><br><span class="line">                            &#123;<span class="string">'learning_rate'</span>: <span class="number">0.01</span>, <span class="string">'beta1'</span>: <span class="number">0.9</span>, <span class="string">'beta2'</span>: <span class="number">0.99</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">3000</span>):</span><br><span class="line">    <span class="keyword">with</span> autograd.record():</span><br><span class="line">        content_results, style_results = extract_features(net, output.data(), content_layers, style_layers)</span><br><span class="line">        loss = get_loss(content_loss(content_results, content_img_result),</span><br><span class="line">                        style_loss(style_results, style_target, [<span class="number">0.2</span>] * <span class="number">5</span>),</span><br><span class="line">                        <span class="number">1e-4</span>)</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        print(loss.asscalar())</span><br><span class="line">    trainer.step(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.imshow(postprocessing(output.data()))</span><br></pre></td></tr></table></figure></p>
<p>这里在实现的时候，使用了这个2D图像的total variation denoising，也就是，每个像素应尽可能的与左侧和上方的像素相近。所以最后的优化目标是三部分组成，第一部分是content loss，第二部分是style loss，第三部分是total variation loss。<br>研究一下mxnet给出的example<br>model_vgg19.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment"># or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment"># distributed with this work for additional information</span></span><br><span class="line"><span class="comment"># regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment"># to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment"># "License"); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment"># with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing,</span></span><br><span class="line"><span class="comment"># software distributed under the License is distributed on an</span></span><br><span class="line"><span class="comment"># "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY</span></span><br><span class="line"><span class="comment"># KIND, either express or implied.  See the License for the</span></span><br><span class="line"><span class="comment"># specific language governing permissions and limitations</span></span><br><span class="line"><span class="comment"># under the License.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> find_mxnet</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">import</span> os, sys</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line">ConvExecutor = namedtuple(<span class="string">'ConvExecutor'</span>, [<span class="string">'executor'</span>, <span class="string">'data'</span>, <span class="string">'data_grad'</span>, <span class="string">'style'</span>, <span class="string">'content'</span>, <span class="string">'arg_dict'</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_symbol</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># declare symbol</span></span><br><span class="line">    data = mx.sym.Variable(<span class="string">"data"</span>)</span><br><span class="line">    conv1_1 = mx.symbol.Convolution(name=<span class="string">'conv1_1'</span>, data=data , num_filter=<span class="number">64</span>, pad=(<span class="number">1</span>,<span class="number">1</span>), kernel=(<span class="number">3</span>,<span class="number">3</span>), stride=(<span class="number">1</span>,<span class="number">1</span>), no_bias=<span class="keyword">False</span>, workspace=<span class="number">1024</span>)</span><br><span class="line">    relu1_1 = mx.symbol.Activation(name=<span class="string">'relu1_1'</span>, data=conv1_1 , act_type=<span class="string">'relu'</span>)</span><br><span class="line">    conv1_2 = mx.symbol.Convolution(name=<span class="string">'conv1_2'</span>, data=relu1_1 , num_filter=<span class="number">64</span>, pad=(<span class="number">1</span>,<span class="number">1</span>), kernel=(<span class="number">3</span>,<span class="number">3</span>), stride=(<span class="number">1</span>,<span class="number">1</span>), no_bias=<span class="keyword">False</span>, workspace=<span class="number">1024</span>)</span><br><span class="line">    relu1_2 = mx.symbol.Activation(name=<span class="string">'relu1_2'</span>, data=conv1_2 , act_type=<span class="string">'relu'</span>)</span><br><span class="line">    pool1 = mx.symbol.Pooling(name=<span class="string">'pool1'</span>, data=relu1_2 , pad=(<span class="number">0</span>,<span class="number">0</span>), kernel=(<span class="number">2</span>,<span class="number">2</span>), stride=(<span class="number">2</span>,<span class="number">2</span>), pool_type=<span class="string">'avg'</span>)</span><br><span class="line">    conv2_1 = mx.symbol.Convolution(name=<span class="string">'conv2_1'</span>, data=pool1 , num_filter=<span class="number">128</span>, pad=(<span class="number">1</span>,<span class="number">1</span>), kernel=(<span class="number">3</span>,<span class="number">3</span>), stride=(<span class="number">1</span>,<span class="number">1</span>), no_bias=<span class="keyword">False</span>, workspace=<span class="number">1024</span>)</span><br><span class="line">    relu2_1 = mx.symbol.Activation(name=<span class="string">'relu2_1'</span>, data=conv2_1 , act_type=<span class="string">'relu'</span>)</span><br><span class="line">    conv2_2 = mx.symbol.Convolution(name=<span class="string">'conv2_2'</span>, data=relu2_1 , num_filter=<span class="number">128</span>, pad=(<span class="number">1</span>,<span class="number">1</span>), kernel=(<span class="number">3</span>,<span class="number">3</span>), stride=(<span class="number">1</span>,<span class="number">1</span>), no_bias=<span class="keyword">False</span>, workspace=<span class="number">1024</span>)</span><br><span class="line">    relu2_2 = mx.symbol.Activation(name=<span class="string">'relu2_2'</span>, data=conv2_2 , act_type=<span class="string">'relu'</span>)</span><br><span class="line">    pool2 = mx.symbol.Pooling(name=<span class="string">'pool2'</span>, data=relu2_2 , pad=(<span class="number">0</span>,<span class="number">0</span>), kernel=(<span class="number">2</span>,<span class="number">2</span>), stride=(<span class="number">2</span>,<span class="number">2</span>), pool_type=<span class="string">'avg'</span>)</span><br><span class="line">    conv3_1 = mx.symbol.Convolution(name=<span class="string">'conv3_1'</span>, data=pool2 , num_filter=<span class="number">256</span>, pad=(<span class="number">1</span>,<span class="number">1</span>), kernel=(<span class="number">3</span>,<span class="number">3</span>), stride=(<span class="number">1</span>,<span class="number">1</span>), no_bias=<span class="keyword">False</span>, workspace=<span class="number">1024</span>)</span><br><span class="line">    relu3_1 = mx.symbol.Activation(name=<span class="string">'relu3_1'</span>, data=conv3_1 , act_type=<span class="string">'relu'</span>)</span><br><span class="line">    conv3_2 = mx.symbol.Convolution(name=<span class="string">'conv3_2'</span>, data=relu3_1 , num_filter=<span class="number">256</span>, pad=(<span class="number">1</span>,<span class="number">1</span>), kernel=(<span class="number">3</span>,<span class="number">3</span>), stride=(<span class="number">1</span>,<span class="number">1</span>), no_bias=<span class="keyword">False</span>, workspace=<span class="number">1024</span>)</span><br><span class="line">    relu3_2 = mx.symbol.Activation(name=<span class="string">'relu3_2'</span>, data=conv3_2 , act_type=<span class="string">'relu'</span>)</span><br><span class="line">    conv3_3 = mx.symbol.Convolution(name=<span class="string">'conv3_3'</span>, data=relu3_2 , num_filter=<span class="number">256</span>, pad=(<span class="number">1</span>,<span class="number">1</span>), kernel=(<span class="number">3</span>,<span class="number">3</span>), stride=(<span class="number">1</span>,<span class="number">1</span>), no_bias=<span class="keyword">False</span>, workspace=<span class="number">1024</span>)</span><br><span class="line">    relu3_3 = mx.symbol.Activation(name=<span class="string">'relu3_3'</span>, data=conv3_3 , act_type=<span class="string">'relu'</span>)</span><br><span class="line">    conv3_4 = mx.symbol.Convolution(name=<span class="string">'conv3_4'</span>, data=relu3_3 , num_filter=<span class="number">256</span>, pad=(<span class="number">1</span>,<span class="number">1</span>), kernel=(<span class="number">3</span>,<span class="number">3</span>), stride=(<span class="number">1</span>,<span class="number">1</span>), no_bias=<span class="keyword">False</span>, workspace=<span class="number">1024</span>)</span><br><span class="line">    relu3_4 = mx.symbol.Activation(name=<span class="string">'relu3_4'</span>, data=conv3_4 , act_type=<span class="string">'relu'</span>)</span><br><span class="line">    pool3 = mx.symbol.Pooling(name=<span class="string">'pool3'</span>, data=relu3_4 , pad=(<span class="number">0</span>,<span class="number">0</span>), kernel=(<span class="number">2</span>,<span class="number">2</span>), stride=(<span class="number">2</span>,<span class="number">2</span>), pool_type=<span class="string">'avg'</span>)</span><br><span class="line">    conv4_1 = mx.symbol.Convolution(name=<span class="string">'conv4_1'</span>, data=pool3 , num_filter=<span class="number">512</span>, pad=(<span class="number">1</span>,<span class="number">1</span>), kernel=(<span class="number">3</span>,<span class="number">3</span>), stride=(<span class="number">1</span>,<span class="number">1</span>), no_bias=<span class="keyword">False</span>, workspace=<span class="number">1024</span>)</span><br><span class="line">    relu4_1 = mx.symbol.Activation(name=<span class="string">'relu4_1'</span>, data=conv4_1 , act_type=<span class="string">'relu'</span>)</span><br><span class="line">    conv4_2 = mx.symbol.Convolution(name=<span class="string">'conv4_2'</span>, data=relu4_1 , num_filter=<span class="number">512</span>, pad=(<span class="number">1</span>,<span class="number">1</span>), kernel=(<span class="number">3</span>,<span class="number">3</span>), stride=(<span class="number">1</span>,<span class="number">1</span>), no_bias=<span class="keyword">False</span>, workspace=<span class="number">1024</span>)</span><br><span class="line">    relu4_2 = mx.symbol.Activation(name=<span class="string">'relu4_2'</span>, data=conv4_2 , act_type=<span class="string">'relu'</span>)</span><br><span class="line">    conv4_3 = mx.symbol.Convolution(name=<span class="string">'conv4_3'</span>, data=relu4_2 , num_filter=<span class="number">512</span>, pad=(<span class="number">1</span>,<span class="number">1</span>), kernel=(<span class="number">3</span>,<span class="number">3</span>), stride=(<span class="number">1</span>,<span class="number">1</span>), no_bias=<span class="keyword">False</span>, workspace=<span class="number">1024</span>)</span><br><span class="line">    relu4_3 = mx.symbol.Activation(name=<span class="string">'relu4_3'</span>, data=conv4_3 , act_type=<span class="string">'relu'</span>)</span><br><span class="line">    conv4_4 = mx.symbol.Convolution(name=<span class="string">'conv4_4'</span>, data=relu4_3 , num_filter=<span class="number">512</span>, pad=(<span class="number">1</span>,<span class="number">1</span>), kernel=(<span class="number">3</span>,<span class="number">3</span>), stride=(<span class="number">1</span>,<span class="number">1</span>), no_bias=<span class="keyword">False</span>, workspace=<span class="number">1024</span>)</span><br><span class="line">    relu4_4 = mx.symbol.Activation(name=<span class="string">'relu4_4'</span>, data=conv4_4 , act_type=<span class="string">'relu'</span>)</span><br><span class="line">    pool4 = mx.symbol.Pooling(name=<span class="string">'pool4'</span>, data=relu4_4 , pad=(<span class="number">0</span>,<span class="number">0</span>), kernel=(<span class="number">2</span>,<span class="number">2</span>), stride=(<span class="number">2</span>,<span class="number">2</span>), pool_type=<span class="string">'avg'</span>)</span><br><span class="line">    conv5_1 = mx.symbol.Convolution(name=<span class="string">'conv5_1'</span>, data=pool4 , num_filter=<span class="number">512</span>, pad=(<span class="number">1</span>,<span class="number">1</span>), kernel=(<span class="number">3</span>,<span class="number">3</span>), stride=(<span class="number">1</span>,<span class="number">1</span>), no_bias=<span class="keyword">False</span>, workspace=<span class="number">1024</span>)</span><br><span class="line">    relu5_1 = mx.symbol.Activation(name=<span class="string">'relu5_1'</span>, data=conv5_1 , act_type=<span class="string">'relu'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># style and content layers</span></span><br><span class="line">    style = mx.sym.Group([relu1_1, relu2_1, relu3_1, relu4_1, relu5_1])</span><br><span class="line">    content = mx.sym.Group([relu4_2])</span><br><span class="line">    <span class="keyword">return</span> style, content</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_executor</span><span class="params">(style, content, input_size, ctx)</span>:</span></span><br><span class="line">    out = mx.sym.Group([style, content])</span><br><span class="line">    <span class="comment"># make executor</span></span><br><span class="line">    arg_shapes, output_shapes, aux_shapes = out.infer_shape(data=(<span class="number">1</span>, <span class="number">3</span>, input_size[<span class="number">0</span>], input_size[<span class="number">1</span>]))</span><br><span class="line">    arg_names = out.list_arguments()</span><br><span class="line">    arg_dict = dict(zip(arg_names, [mx.nd.zeros(shape, ctx=ctx) <span class="keyword">for</span> shape <span class="keyword">in</span> arg_shapes]))</span><br><span class="line">    grad_dict = &#123;<span class="string">"data"</span>: arg_dict[<span class="string">"data"</span>].copyto(ctx)&#125;</span><br><span class="line">    <span class="comment"># init with pretrained weight</span></span><br><span class="line">    pretrained = mx.nd.load(<span class="string">"./model/vgg19.params"</span>)</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> arg_names:</span><br><span class="line">        <span class="keyword">if</span> name == <span class="string">"data"</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        key = <span class="string">"arg:"</span> + name</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">in</span> pretrained:</span><br><span class="line">            pretrained[key].copyto(arg_dict[name])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"Skip argument %s"</span> % name)</span><br><span class="line">    executor = out.bind(ctx=ctx, args=arg_dict, args_grad=grad_dict, grad_req=<span class="string">"write"</span>)</span><br><span class="line">    <span class="keyword">return</span> ConvExecutor(executor=executor,</span><br><span class="line">                        data=arg_dict[<span class="string">"data"</span>],</span><br><span class="line">                        data_grad=grad_dict[<span class="string">"data"</span>],</span><br><span class="line">                        style=executor.outputs[:<span class="number">-1</span>],</span><br><span class="line">                        content=executor.outputs[<span class="number">-1</span>],</span><br><span class="line">                        arg_dict=arg_dict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span><span class="params">(input_size, ctx)</span>:</span></span><br><span class="line">    style, content = get_symbol()</span><br><span class="line">    <span class="keyword">return</span> get_executor(style, content, input_size, ctx)</span><br></pre></td></tr></table></figure></p>
<p>nstyle.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment"># or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment"># distributed with this work for additional information</span></span><br><span class="line"><span class="comment"># regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment"># to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment"># "License"); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment"># with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#   http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing,</span></span><br><span class="line"><span class="comment"># software distributed under the License is distributed on an</span></span><br><span class="line"><span class="comment"># "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY</span></span><br><span class="line"><span class="comment"># KIND, either express or implied.  See the License for the</span></span><br><span class="line"><span class="comment"># specific language governing permissions and limitations</span></span><br><span class="line"><span class="comment"># under the License.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> find_mxnet</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> importlib</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.basicConfig(level=logging.DEBUG)</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io, transform</span><br><span class="line"><span class="keyword">from</span> skimage.restoration <span class="keyword">import</span> denoise_tv_chambolle</span><br><span class="line"></span><br><span class="line">CallbackData = namedtuple(<span class="string">'CallbackData'</span>, field_names=[<span class="string">'eps'</span>,<span class="string">'epoch'</span>,<span class="string">'img'</span>,<span class="string">'filename'</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_args</span><span class="params">(arglist=None)</span>:</span></span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">'neural style'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择模型，默认是VGG19</span></span><br><span class="line">    parser.add_argument(<span class="string">'--model'</span>, type=str, default=<span class="string">'vgg19'</span>,</span><br><span class="line">                        choices = [<span class="string">'vgg'</span>],</span><br><span class="line">                        help = <span class="string">'the pretrained model to use'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 内容图片的路径</span></span><br><span class="line">    parser.add_argument(<span class="string">'--content-image'</span>, type=str, default=<span class="string">'input/IMG_4343.jpg'</span>,</span><br><span class="line">                        help=<span class="string">'the content image'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 风格图片的路径</span></span><br><span class="line">    parser.add_argument(<span class="string">'--style-image'</span>, type=str, default=<span class="string">'input/starry_night.jpg'</span>,</span><br><span class="line">                        help=<span class="string">'the style image'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 停止迭代的阈值，若relative change小于这个数就停止迭代</span></span><br><span class="line">    parser.add_argument(<span class="string">'--stop-eps'</span>, type=float, default=<span class="number">.005</span>,</span><br><span class="line">                        help=<span class="string">'stop if the relative chanage is less than eps'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 内容图片在loss上的权重</span></span><br><span class="line">    parser.add_argument(<span class="string">'--content-weight'</span>, type=float, default=<span class="number">10</span>,</span><br><span class="line">                        help=<span class="string">'the weight for the content image'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 风格图片在loss上的权重</span></span><br><span class="line">    parser.add_argument(<span class="string">'--style-weight'</span>, type=float, default=<span class="number">1</span>,</span><br><span class="line">                        help=<span class="string">'the weight for the style image'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># total variation在loss上的权重</span></span><br><span class="line">    parser.add_argument(<span class="string">'--tv-weight'</span>, type=float, default=<span class="number">1e-2</span>,</span><br><span class="line">                        help=<span class="string">'the magtitute on TV loss'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最大迭代次数</span></span><br><span class="line">    parser.add_argument(<span class="string">'--max-num-epochs'</span>, type=int, default=<span class="number">1000</span>,</span><br><span class="line">                        help=<span class="string">'the maximal number of training epochs'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># </span></span><br><span class="line">    parser.add_argument(<span class="string">'--max-long-edge'</span>, type=int, default=<span class="number">600</span>,</span><br><span class="line">                        help=<span class="string">'resize the content image'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始的学习率</span></span><br><span class="line">    parser.add_argument(<span class="string">'--lr'</span>, type=float, default=<span class="number">.001</span>,</span><br><span class="line">                        help=<span class="string">'the initial learning rate'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用哪块GPU</span></span><br><span class="line">    parser.add_argument(<span class="string">'--gpu'</span>, type=int, default=<span class="number">0</span>,</span><br><span class="line">                        help=<span class="string">'which gpu card to use, -1 means using cpu'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出图像的路径</span></span><br><span class="line">    parser.add_argument(<span class="string">'--output_dir'</span>, type=str, default=<span class="string">'output/'</span>,</span><br><span class="line">                        help=<span class="string">'the output image'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每多少轮保存一次当前的输出结果</span></span><br><span class="line">    parser.add_argument(<span class="string">'--save-epochs'</span>, type=int, default=<span class="number">50</span>,</span><br><span class="line">                        help=<span class="string">'save the output every n epochs'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># </span></span><br><span class="line">    parser.add_argument(<span class="string">'--remove-noise'</span>, type=float, default=<span class="number">.02</span>,</span><br><span class="line">                        help=<span class="string">'the magtitute to remove noise'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每迭代多少轮减小一下学习率</span></span><br><span class="line">    parser.add_argument(<span class="string">'--lr-sched-delay'</span>, type=int, default=<span class="number">75</span>,</span><br><span class="line">                        help=<span class="string">'how many epochs between decreasing learning rate'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习率衰减因子</span></span><br><span class="line">    parser.add_argument(<span class="string">'--lr-sched-factor'</span>, type=int, default=<span class="number">0.9</span>,</span><br><span class="line">                        help=<span class="string">'factor to decrease learning rate on schedule'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> arglist <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> parser.parse_args()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> parser.parse_args(arglist)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PreprocessContentImage</span><span class="params">(path, long_edge)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    内容图片预处理</span></span><br><span class="line"><span class="string">    Parameter: path, str, 图片路径</span></span><br><span class="line"><span class="string">               long_edge, int, float, str(float), 图像被缩放后长边的长度</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 读取图片，使用skimage.io.imread，返回numpy.ndarray</span></span><br><span class="line">    img = io.imread(path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># img.shape前两个数分别是多少行和多少列，第三个数是channel数</span></span><br><span class="line">    logging.info(<span class="string">"load the content image, size = %s"</span>, img.shape[:<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># resize一下图片，resize后的范围在0到1内</span></span><br><span class="line">    factor = float(long_edge) / max(img.shape[:<span class="number">2</span>])</span><br><span class="line">    new_size = (int(img.shape[<span class="number">0</span>] * factor), int(img.shape[<span class="number">1</span>] * factor))</span><br><span class="line">    resized_img = transform.resize(img, new_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 乘以256恢复到原来的区间</span></span><br><span class="line">    sample = np.asarray(resized_img) * <span class="number">256</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># swap axes to make image from (224, 224, 3) to (3, 224, 224)</span></span><br><span class="line">    sample = np.swapaxes(sample, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">    sample = np.swapaxes(sample, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sub mean，这里的均值应该是ImageNet数据集在RGB三通道上的均值</span></span><br><span class="line">    sample[<span class="number">0</span>, :] -= <span class="number">123.68</span></span><br><span class="line">    sample[<span class="number">1</span>, :] -= <span class="number">116.779</span></span><br><span class="line">    sample[<span class="number">2</span>, :] -= <span class="number">103.939</span></span><br><span class="line"></span><br><span class="line">    logging.info(<span class="string">"resize the content image to %s"</span>, new_size)</span><br><span class="line">    <span class="keyword">return</span> np.resize(sample, (<span class="number">1</span>, <span class="number">3</span>, sample.shape[<span class="number">1</span>], sample.shape[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PreprocessStyleImage</span><span class="params">(path, shape)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    对风格图片的预处理</span></span><br><span class="line"><span class="string">    Parameter: path, str, 图像路径</span></span><br><span class="line"><span class="string">               shape, tuple, 长度为4的tuple，第三个元素和第四个元素是content image的size</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    img = io.imread(path)</span><br><span class="line">    resized_img = transform.resize(img, (shape[<span class="number">2</span>], shape[<span class="number">3</span>]))</span><br><span class="line">    sample = np.asarray(resized_img) * <span class="number">256</span></span><br><span class="line">    sample = np.swapaxes(sample, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">    sample = np.swapaxes(sample, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    sample[<span class="number">0</span>, :] -= <span class="number">123.68</span></span><br><span class="line">    sample[<span class="number">1</span>, :] -= <span class="number">116.779</span></span><br><span class="line">    sample[<span class="number">2</span>, :] -= <span class="number">103.939</span></span><br><span class="line">    <span class="keyword">return</span> np.resize(sample, (<span class="number">1</span>, <span class="number">3</span>, sample.shape[<span class="number">1</span>], sample.shape[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PostprocessImage</span><span class="params">(img)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    对图像的后处理</span></span><br><span class="line"><span class="string">    Parameter: img, numpy.ndarray</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    img = np.resize(img, (<span class="number">3</span>, img.shape[<span class="number">2</span>], img.shape[<span class="number">3</span>]))</span><br><span class="line">    img[<span class="number">0</span>, :] += <span class="number">123.68</span></span><br><span class="line">    img[<span class="number">1</span>, :] += <span class="number">116.779</span></span><br><span class="line">    img[<span class="number">2</span>, :] += <span class="number">103.939</span></span><br><span class="line">    img = np.swapaxes(img, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    img = np.swapaxes(img, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># clip函数是用来砍掉小于下界和大于上届的数的</span></span><br><span class="line">    img = np.clip(img, <span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">    <span class="keyword">return</span> img.astype(<span class="string">'uint8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SaveImage</span><span class="params">(img, filename, remove_noise=<span class="number">0.</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    保存图片</span></span><br><span class="line"><span class="string">    Parameter: img, numpy.ndarray</span></span><br><span class="line"><span class="string">               filename, str</span></span><br><span class="line"><span class="string">               remove_noise, float, default=0., </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    logging.info(<span class="string">'save output to %s'</span>, filename)</span><br><span class="line">    out = PostprocessImage(img)</span><br><span class="line">    <span class="keyword">if</span> remove_noise != <span class="number">0.0</span>:</span><br><span class="line">        out = denoise_tv_chambolle(out, weight=remove_noise, multichannel=<span class="keyword">True</span>)</span><br><span class="line">    io.imsave(filename, out)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_gram_symbol</span><span class="params">(input_size, style)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Parameter: input_size, tuple, length=2, 表示content image的size</span></span><br><span class="line"><span class="string">               style, mx.sym.Group，里面是style对应的层</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    _, output_shapes, _ = style.infer_shape(data=(<span class="number">1</span>, <span class="number">3</span>, input_size[<span class="number">0</span>], input_size[<span class="number">1</span>]))</span><br><span class="line">    gram_list = []</span><br><span class="line">    grad_scale = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(style.list_outputs())):</span><br><span class="line">        shape = output_shapes[i]</span><br><span class="line">        x = mx.sym.Reshape(style[i], target_shape=(int(shape[<span class="number">1</span>]), int(np.prod(shape[<span class="number">2</span>:]))))</span><br><span class="line">        <span class="comment"># use fully connected to quickly do dot(x, x^T)</span></span><br><span class="line">        gram = mx.sym.FullyConnected(x, x, no_bias=<span class="keyword">True</span>, num_hidden=shape[<span class="number">1</span>])</span><br><span class="line">        gram_list.append(gram)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># grad_scale c*h*w*c</span></span><br><span class="line">        grad_scale.append(np.prod(shape[<span class="number">1</span>:]) * shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> mx.sym.Group(gram_list), grad_scale</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_loss</span><span class="params">(gram, content)</span>:</span></span><br><span class="line">    gram_loss = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(gram.list_outputs())):</span><br><span class="line">        gvar = mx.sym.Variable(<span class="string">"target_gram_%d"</span> % i)</span><br><span class="line">        gram_loss.append(mx.sym.sum(mx.sym.square(gvar - gram[i])))</span><br><span class="line">    cvar = mx.sym.Variable(<span class="string">"target_content"</span>)</span><br><span class="line">    content_loss = mx.sym.sum(mx.sym.square(cvar - content))</span><br><span class="line">    <span class="keyword">return</span> mx.sym.Group(gram_loss), content_loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tv_grad_executor</span><span class="params">(img, ctx, tv_weight)</span>:</span></span><br><span class="line">    <span class="string">"""create TV gradient executor with input binded on img</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> tv_weight &lt;= <span class="number">0.0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    nchannel = img.shape[<span class="number">1</span>]</span><br><span class="line">    simg = mx.sym.Variable(<span class="string">"img"</span>)</span><br><span class="line">    skernel = mx.sym.Variable(<span class="string">"kernel"</span>)</span><br><span class="line">    channels = mx.sym.SliceChannel(simg, num_outputs=nchannel)</span><br><span class="line">    out = mx.sym.Concat(*[</span><br><span class="line">        mx.sym.Convolution(data=channels[i], weight=skernel,</span><br><span class="line">                           num_filter=<span class="number">1</span>,</span><br><span class="line">                           kernel=(<span class="number">3</span>, <span class="number">3</span>), pad=(<span class="number">1</span>,<span class="number">1</span>),</span><br><span class="line">                           no_bias=<span class="keyword">True</span>, stride=(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(nchannel)])</span><br><span class="line">    kernel = mx.nd.array(np.array([[<span class="number">0</span>, <span class="number">-1</span>, <span class="number">0</span>],</span><br><span class="line">                                   [<span class="number">-1</span>, <span class="number">4</span>, <span class="number">-1</span>],</span><br><span class="line">                                   [<span class="number">0</span>, <span class="number">-1</span>, <span class="number">0</span>]])</span><br><span class="line">                         .reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)),</span><br><span class="line">                         ctx) / <span class="number">8.0</span></span><br><span class="line">    out = out * tv_weight</span><br><span class="line">    <span class="keyword">return</span> out.bind(ctx, args=&#123;<span class="string">"img"</span>: img,</span><br><span class="line">                               <span class="string">"kernel"</span>: kernel&#125;)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_nstyle</span><span class="params">(args, callback=None)</span>:</span></span><br><span class="line">    <span class="string">"""Train a neural style network.</span></span><br><span class="line"><span class="string">    Args are from argparse and control input, output, hyper-parameters.</span></span><br><span class="line"><span class="string">    callback allows for display of training progress.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># input</span></span><br><span class="line">    dev = mx.gpu(args.gpu) <span class="keyword">if</span> args.gpu &gt;= <span class="number">0</span> <span class="keyword">else</span> mx.cpu()</span><br><span class="line">    content_np = PreprocessContentImage(args.content_image, args.max_long_edge)</span><br><span class="line">    style_np = PreprocessStyleImage(args.style_image, shape=content_np.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># size是内容图片的尺寸</span></span><br><span class="line">    size = content_np.shape[<span class="number">2</span>:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># model</span></span><br><span class="line">    Executor = namedtuple(<span class="string">'Executor'</span>, [<span class="string">'executor'</span>, <span class="string">'data'</span>, <span class="string">'data_grad'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 导入'model_vgg19.py'</span></span><br><span class="line">    model_module =  importlib.import_module(<span class="string">'model_'</span> + args.model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取到style和content两个mx.sym.Group，里面装着style和content层</span></span><br><span class="line">    style, content = model_module.get_symbol()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取到所有style层的gram矩阵和grad scale</span></span><br><span class="line">    gram, gscale = style_gram_symbol(size, style)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    model_executor = model_module.get_executor(gram, content, size, dev)</span><br><span class="line">    model_executor.data[:] = style_np</span><br><span class="line">    model_executor.executor.forward()</span><br><span class="line">    style_array = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(model_executor.style)):</span><br><span class="line">        style_array.append(model_executor.style[i].copyto(mx.cpu()))</span><br><span class="line"></span><br><span class="line">    model_executor.data[:] = content_np</span><br><span class="line">    model_executor.executor.forward()</span><br><span class="line">    content_array = model_executor.content.copyto(mx.cpu())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># delete the executor</span></span><br><span class="line">    <span class="keyword">del</span> model_executor</span><br><span class="line"></span><br><span class="line">    style_loss, content_loss = get_loss(gram, content)</span><br><span class="line">    model_executor = model_module.get_executor(</span><br><span class="line">        style_loss, content_loss, size, dev)</span><br><span class="line"></span><br><span class="line">    grad_array = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(style_array)):</span><br><span class="line">        style_array[i].copyto(model_executor.arg_dict[<span class="string">"target_gram_%d"</span> % i])</span><br><span class="line">        grad_array.append(mx.nd.ones((<span class="number">1</span>,), dev) * (float(args.style_weight) / gscale[i]))</span><br><span class="line">    grad_array.append(mx.nd.ones((<span class="number">1</span>,), dev) * (float(args.content_weight)))</span><br><span class="line"></span><br><span class="line">    print([x.asscalar() <span class="keyword">for</span> x <span class="keyword">in</span> grad_array])</span><br><span class="line">    content_array.copyto(model_executor.arg_dict[<span class="string">"target_content"</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># train</span></span><br><span class="line">    <span class="comment"># initialize img with random noise</span></span><br><span class="line">    img = mx.nd.zeros(content_np.shape, ctx=dev)</span><br><span class="line">    img[:] = mx.rnd.uniform(<span class="number">-0.1</span>, <span class="number">0.1</span>, img.shape)</span><br><span class="line"></span><br><span class="line">    lr = mx.lr_scheduler.FactorScheduler(step=args.lr_sched_delay,</span><br><span class="line">            factor=args.lr_sched_factor)</span><br><span class="line"></span><br><span class="line">    optimizer = mx.optimizer.NAG(</span><br><span class="line">        learning_rate = args.lr,</span><br><span class="line">        wd = <span class="number">0.0001</span>,</span><br><span class="line">        momentum=<span class="number">0.95</span>,</span><br><span class="line">        lr_scheduler = lr)</span><br><span class="line">    optim_state = optimizer.create_state(<span class="number">0</span>, img)</span><br><span class="line"></span><br><span class="line">    logging.info(<span class="string">'start training arguments %s'</span>, args)</span><br><span class="line">    old_img = img.copyto(dev)</span><br><span class="line">    clip_norm = <span class="number">1</span> * np.prod(img.shape)</span><br><span class="line">    tv_grad_executor = get_tv_grad_executor(img, dev, args.tv_weight)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> range(args.max_num_epochs):</span><br><span class="line">        img.copyto(model_executor.data)</span><br><span class="line">        model_executor.executor.forward()</span><br><span class="line">        model_executor.executor.backward(grad_array)</span><br><span class="line">        gnorm = mx.nd.norm(model_executor.data_grad).asscalar()</span><br><span class="line">        <span class="keyword">if</span> gnorm &gt; clip_norm:</span><br><span class="line">            model_executor.data_grad[:] *= clip_norm / gnorm</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> tv_grad_executor <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            tv_grad_executor.forward()</span><br><span class="line">            optimizer.update(<span class="number">0</span>, img,</span><br><span class="line">                             model_executor.data_grad + tv_grad_executor.outputs[<span class="number">0</span>],</span><br><span class="line">                             optim_state)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            optimizer.update(<span class="number">0</span>, img, model_executor.data_grad, optim_state)</span><br><span class="line">        new_img = img</span><br><span class="line">        eps = (mx.nd.norm(old_img - new_img) / mx.nd.norm(new_img)).asscalar()</span><br><span class="line"></span><br><span class="line">        old_img = new_img.copyto(dev)</span><br><span class="line">        logging.info(<span class="string">'epoch %d, relative change %f'</span>, e, eps)</span><br><span class="line">        <span class="keyword">if</span> eps &lt; args.stop_eps:</span><br><span class="line">            logging.info(<span class="string">'eps &lt; args.stop_eps, training finished'</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> callback:</span><br><span class="line">            cbdata = &#123;</span><br><span class="line">                <span class="string">'eps'</span>: eps,</span><br><span class="line">                <span class="string">'epoch'</span>: e+<span class="number">1</span>,</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">if</span> (e+<span class="number">1</span>) % args.save_epochs == <span class="number">0</span>:</span><br><span class="line">            outfn = args.output_dir + <span class="string">'e_'</span>+str(e+<span class="number">1</span>)+<span class="string">'.jpg'</span></span><br><span class="line">            npimg = new_img.asnumpy()</span><br><span class="line">            SaveImage(npimg, outfn, args.remove_noise)</span><br><span class="line">            <span class="keyword">if</span> callback:</span><br><span class="line">                cbdata[<span class="string">'filename'</span>] = outfn</span><br><span class="line">                cbdata[<span class="string">'img'</span>] = npimg</span><br><span class="line">        <span class="keyword">if</span> callback:</span><br><span class="line">            callback(cbdata)</span><br><span class="line"></span><br><span class="line">    final_fn = args.output_dir + <span class="string">'/final.jpg'</span></span><br><span class="line">    SaveImage(new_img.asnumpy(), final_fn)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    args = get_args()</span><br><span class="line">    train_nstyle(args)</span><br></pre></td></tr></table></figure></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/blog/tags/machine-learning/" rel="tag"># machine learning</a>
          
            <a href="/blog/tags/deep-learning/" rel="tag"># deep learning</a>
          
            <a href="/blog/tags/computer-vision/" rel="tag"># computer vision</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/blog/2018/02/24/leetcode-algorithms-3/" rel="next" title="leetcode algorithms #3">
                <i class="fa fa-chevron-left"></i> leetcode algorithms #3
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/blog/2018/03/01/perceptual-losses-for-real-time-style-transfer-and-super-resolution/" rel="prev" title="Perceptual Losses for Real-Time Style Transfer and Super-Resolution">
                Perceptual Losses for Real-Time Style Transfer and Super-Resolution <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNzEwMS8xMzYzNw=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/blog/uploads/avatar.jpg"
                alt="Davidham" />
            
              <p class="site-author-name" itemprop="name">Davidham</p>
              <p class="site-description motion-element" itemprop="description">Davidham的博客，写点学习笔记啥的，邮箱17120410@bjtu.edu.cn</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/blog/archives/">
              
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/blog/categories/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/blog/tags/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Davidham3" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="http://weibo.com/Davidham3" target="_blank" title="微博">
                      
                        <i class="fa fa-fw fa-globe"></i>微博</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Image-Style-Transfer-Using-Convolutional-Neural-Networks"><span class="nav-number">1.</span> <span class="nav-text">Image Style Transfer Using Convolutional Neural Networks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#大体原理"><span class="nav-number">2.</span> <span class="nav-text">大体原理</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Deep-image-representations"><span class="nav-number">3.</span> <span class="nav-text">Deep image representations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#content-representation"><span class="nav-number">3.1.</span> <span class="nav-text">content representation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#style-representation"><span class="nav-number">3.2.</span> <span class="nav-text">style representation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#style-transfer"><span class="nav-number">3.3.</span> <span class="nav-text">style transfer</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Results"><span class="nav-number">4.</span> <span class="nav-text">Results</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Trade-off-between-content-and-style-matching"><span class="nav-number">4.1.</span> <span class="nav-text">Trade-off between content and style matching</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Effect-of-different-layers-of-the-Convolutional-Neural-Network"><span class="nav-number">4.2.</span> <span class="nav-text">Effect of different layers of the Convolutional Neural Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Initialisation-of-gradient-descent"><span class="nav-number">4.3.</span> <span class="nav-text">Initialisation of gradient descent</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#implementation"><span class="nav-number">5.</span> <span class="nav-text">implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Total-variation-denoising"><span class="nav-number">5.1.</span> <span class="nav-text">Total variation denoising</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1D-signal-series"><span class="nav-number">5.1.1.</span> <span class="nav-text">1D signal series</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization-properties"><span class="nav-number">5.1.2.</span> <span class="nav-text">Regularization properties</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2D-signal-images"><span class="nav-number">5.1.3.</span> <span class="nav-text">2D signal images</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Davidham</span>

  

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Gemini</a> v6.0.0</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>
























  
  
    <script type="text/javascript" src="/blog/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=6.0.0"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=6.0.0"></script>



  
  


  <script type="text/javascript" src="/blog/js/src/affix.js?v=6.0.0"></script>

  <script type="text/javascript" src="/blog/js/src/schemes/pisces.js?v=6.0.0"></script>



  
  <script type="text/javascript" src="/blog/js/src/scrollspy.js?v=6.0.0"></script>
<script type="text/javascript" src="/blog/js/src/post-details.js?v=6.0.0"></script>



  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=6.0.0"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
