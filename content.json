{"pages":[{"title":"categories","text":"","link":"/blog/categories/index.html"},{"title":"tags","text":"","link":"/blog/tags/index.html"}],"posts":[{"title":"A Tutorial on Spectral Clustering","text":"关于谱聚类的文章，主要包含了谱聚类和拉普拉斯矩阵的内容。最近研究 GCN 的原理的时候发现了这篇论文。Von Luxburg U. A tutorial on spectral clustering[J]. Statistics and Computing, 2007, 17(4): 395-416.原文链接：A Tutorial on Spectral Clustering 2 Similarity graphs给定一组数据点 $x_1, …, x_n$ 还有数据点 $x_i$ 和 $x_j$ 之间的相似性 $s_{ij} \\geq 0$，聚类的目标是将样本点分到几个组中，组的样本相似，不同组的样本不相似。如果没有更多的信息，使用 similarity graph 表示数据是一个好的方法，$G = (V, E)$。每个顶点 $v_i$ 表示一个数据点 $x_i$。如果相似度 $s_{ij}$ 是正的，且大于一个确定的阈值，那么两个顶点相连，边的权重为 $s_{ij}$。聚类的问题可以使用相似度图重新定义为：我们想找到一个划分方案，使得不同组之间的边有很小的权重（意味着不同类簇间的样本不相似），同组内的权重较高（意味着同一类簇的样本相似）。我们首先引入一些符号和性质。 2.1 Graph notation$G = (V, E)$ 是无向图，顶点集 $V = \\lbrace v_1, …, v_n \\rbrace $。我们假设图 $G$ 是带权的，边有非负权重 $w_{ij} \\geq 0$。带权的邻接矩阵是 $W = (w_{ij})_{i,j=1,…,n}$。如果 $w_{ij} = 0$，表示顶点 $v_i$ 和 $v_j$之间没有边。因为 $G$ 是无向的，所以 $w_{ij} = w_{ji}$。顶点 $v_i \\in V$ 的度定义为： $$d_i = \\sum^n_{j = 1} w_{ij}.$$ 事实上，这个加和只会在所有和 $v_i$ 邻接的顶点上做， 因为和其他的顶点之间的边权重为0。度矩阵 $D$ 定义为对角矩阵，对角线上是度 $d_1, …, d_n$。给定顶点的子集 $A \\subset V$，它的补集 $V \\ \\backslash \\ A$ 表示为 $\\bar{A}$。定义一个指示向量 $1_A = (f_1, \\dots f_n)’ \\in \\mathbb{R}^n$，如果 $v_i \\in A$，$f_i = 1$，否则 $f_i = 0$。我们在做求和的时候，比如 $\\sum_{i \\in A} w_{ij}$， 把 $\\lbrace i \\mid v_i \\in A \\rbrace $ 简记为 $i \\in A$。对于两个不相交的集合 $A, B \\subset V$，定义： $$W(A, B) := \\sum_{i \\in A, j \\in B} w_{ij}.$$ 我们考虑两个不同的方式来描述子集 $A \\subset V$ 的“大小”： $$\\vert A \\vert := A 的顶点数\\\\\\text{vol}(A) := \\sum_{i \\in A} d_i.$$ 直观上来讲，$\\vert A \\vert$ 通过顶点数描述了 $A$ 的大小，但是 $\\text{vol}(A)$ 通过对 $A$ 中所有的边进行加和得到。如果 $A$ 中的两个结点可以通过一条路径连接，而且中间的点都在 $A$ 中，那么称子集 $A \\subset V$ 是连通的。如果子集是连通的，且顶点集 $A$ 和 $\\bar{A}$ 之间没有结点相连，那么称 $A$ 是一个连通分量。如果 $A_i \\cap A_j = \\emptyset$ 且 $A_1 \\cup \\dots \\cup A_k = V$，那么非空集合 $A_1, \\dots, A_k$ 是图的一个划分。 2.2 Different similarity graphs有一些流行的方法对顶点间的相似度或距离构建图。构建相似度图的目标是对样本之间的局部邻居关系建模。 The $\\varepsilon$-neighborhood graph: 我们把距离小于 $\\varepsilon$ 的样本连起来。因为连接起来的样本点基本是一个尺度的，考虑边的权重不会增加更多的信息。所以，$\\varepsilon$-邻居图通常是无权图。 $k$-nearest neighbor graphs: 如果 $v_j$ 是 $v_i$ 的 $k$-近邻邻居，那目标是连接 $v_i$ 和 $v_j$。但是，这个定义会得到一个有向图，因为邻居间的关系是非对称的。有两种方法变成有向。第一种是忽略边的方向，也就是用无向边连接。结果通常称为 $k$-近邻邻居图。第二种方法是如果两个顶点互为对方的 $k$-近邻邻居，那么相连。得到的图称为 mutual $k$-nearest neighbor graph。这两种图的边都是顶点的相似度。 The fully connected graph: 我们简单的连接有着正的相似度的顶点，边的权重就是相似度 $s_{ij}$。因为图应该表示局部邻居关系，这个构建方法只在相似度能体现局部邻居关系时才有效。举个相似度函数的例子，高斯相似度函数 $s(x_i, x_j) = \\exp(- \\Vert x_i - x_j \\Vert ^ 2 / (2 \\sigma^2))$，参数 $\\sigma$ 控制了邻居的宽度。这个参数和 $\\varepsilon$-邻居图中的 $\\varepsilon$ 的角色差不多。 上面提到的图是谱聚类中常用的。据我们所知，相似度图如何影响谱聚类的结果还不为所知。不同的图的表现形式我们会在第八节讨论。 3 Graph Laplacians and their basic properties谱聚类的主要工具是图拉普拉斯矩阵。有一个领域致力研究这些矩阵，称为谱图理论(e.g., see Chung, 1997)。我们这节定义不同的拉普拉斯矩阵，指出他们的重要性质。我们会仔细的对比不同的拉普拉斯矩阵。注意，事实上没有一个统一的说法说哪个矩阵就是 “graph Laplacian”。通常，每个作者都称他们使用的矩阵是拉普拉斯矩阵。因此，在读关于拉普拉斯矩阵的论文的时候需要注意。 假设 $G$ 是无向带权图，权重矩阵 $W$，$w_{ij} = w_{ji} \\geq 0$。使用一个矩阵的特征向量时，我们没有必要假设他们是归一化的。举个例子，常向量 $1$ 和他的倍数 $a1$ 在 $a = \\not 0$ 的时候被认为是相同的特征向量。特征值总时升序排列，而且会有多重性。前 $k$ 个特征值，我们指的是前 $k$ 个最小的特征值。 3.1 The unnormalized graph Laplacian非归一化的图拉普拉斯矩阵定义为： $$L = D - W.$$ 关于它的性质的论文在 Mohar(1991, 1997)。下面性质是谱聚类需要的性质： Proposition 1 (Properties of $L$) 拉普拉斯矩阵满足以下性质： 对于每个向量 $f \\in \\mathbb{R}^n$，我们有： $$f’Lf = \\frac{1}{2} \\sum^n_{i,j=1} w_{ij}(f_i - f_j)^2.$$ $L$ 是对称且半正定的。 $L$ 最小的特征值是 $0$，对应的特征向量是常向量 $1$。 $L$ 有 $n$ 个非负实数特征值 $0 = \\lambda_1 \\leq \\lambda_2 \\leq \\dots \\lambda_n$。 Proof.Part (1)：由 $d_i$ 的定义得： $$\\begin{aligned}f’Lf &amp;= f’Df - f’Wf = \\sum^n_{i=1}d_i f^2_i - \\sum^n_{i,j=1} f_i f_j w_{ij}\\\\&amp;=\\frac{1}{2} \\Bigg( \\sum^n_{i=1} d_i f^2_i - 2 \\sum^n_{i,j=1} f_i f_j w_{ij} + \\sum^n_{j=1} d_j f^2_j \\Bigg) = \\frac{1}{2} \\sum^n_{i,j=1} w_{ij} (f_i - f_j)^2.\\end{aligned}$$ Part (2)：$L$ 的对称性是因为 $W$ 和 $D$ 是对称的。半正定是 Part (1) 的结果，对于任意的 $f \\in \\mathbb{R}^n$，$f’Lf \\geq 0$。 Part (3)：显而易见。 Part (4)：由(1) 和 (3) 推出。 注意：非归一化的拉普拉斯矩阵不依赖于邻接矩阵 $W$ 的对角线上的元素。邻接矩阵非对角线上的元素得到非归一化的拉普拉斯矩阵。图中的自连接不会改变对应的拉普拉斯矩阵。（这里说白了就是，拉普拉斯矩阵非对角线位置上的元素是邻接矩阵对应位置的元素的相反数，如果顶点加自连接，那么度矩阵就会对应地增加，D-W在对角线上还是一样的数，不会变） 非归一化的拉普拉斯矩阵的特征值和特征向量可以用于描述图的很多性质，参见 Mohar(1991, 1997)。对于谱聚类来说一个重要的性质是： Proposition 2 (Number of connected components and the spectrum of $L$) 图 $G$ 是无向非负权重的图。拉普拉斯矩阵的特征值 $0$ 的多重性 $k$ 等于图中的连通分量 $A_1, \\dots, A_k$ 的数量。特征值 $0$ 的特征空间通过指示向量 $1_{A_1}, \\dots, 1_{A_k}$ 生成。 Proof. 我们先以 $k = 1$ 为例，也就是说只有一个连通图。假设 $f$ 是特征值 $0$ 对应的特征向量。我们知道： $$0 = f’Lf = \\sum^n_{i,j=1} w_{ij} (f_i - f_j)^2.$$ 因为权重 $w_{ij}$ 是非负的，如果所有的项 $w_{ij} (f_i - f_j)^2$ 都消失了，这个和就会很小。因此，如果两个顶点相连（权重大于0），那么 $f_i$ 需要等于 $f_j$。我们可以发现，对于所有顶点 $f$ 需要是一个相同的常数，且这些点可以通过一条路径相连。此外，因为无向图内连通分量所有的顶点可以通过一条路径相连，$f$ 对于整个连通分量来说需要是一个常数。在只有一个连通分量的图中，我们因此只有一个常向量 $1$ 作为特征向量，对应的特征值为 $0$，显然这个向量就是这个连通分量的指示向量。 现在考虑 $k$ 个连通分量。为了不失一般性，我们假设顶点是根据连通分量排序的。这样，邻接矩阵 $W$ 有一个块对角形式，对于矩阵 $L$ 也是如此： $$L = \\begin{pmatrix} L_1 &amp; &amp; &amp; \\\\ &amp; L_2 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; L_k\\end{pmatrix}$$ 注意：块 $L_i$ 是一个关于它自己的拉普拉斯矩阵，也就是对应第 $i$ 个子图的拉普拉斯矩阵。在这个对角都是块矩阵的例子中，我们知道 $L$ 的谱是所有的 $L_i$ 的谱的并集，对应的 $L$ 的特征向量是 $L_i$ 的特征向量，其他方块的位置都是0.因为每个 $L_i$ 是一个连通图的拉普拉斯矩阵，我们知道每个 $L_i$ 在第 $i$ 个连通分量上，有特征值 $0$，且多重性为 $1$，对应的特征向量常向量。因此，矩阵 $L$ 的特征值 $0$ 的个数就等于连通分量数，而且对应的特征向量是连通分量的指示向量。 3.2 The normalized graph Laplacians在文献中有两个归一化的拉普拉斯矩阵。两个矩阵紧密相连，定义如下： $$\\begin{aligned}&amp; L_{sym} := D^{-1/2} L D^{-1/2} = I - D^{-1/2} W D^{-1/2}\\\\&amp; L_{rw} := D^{-1} L = I - D^{-1} W.\\end{aligned}$$ 我们将第一个矩阵表示为 $L_{sym}$，因为它是一个对称阵，第二个矩阵表示为 $L_{rw}$，因为它和随机游走有关。接下来我们总结一下这两个矩阵的性质。关于归一化的拉普拉斯矩阵的引用在 Chung (1997)。 Proposition 3 (Properties of $L_{sym}$ and $L_{rw}$) 归一化的拉普拉斯矩阵满足以下性质： 对于每个 $f \\in \\mathbb{R}^n$，我们有： $$f’ L_{sym} f = \\frac{1}{2} \\sum^n_{i,j=1} w_{ij} \\Bigg( \\frac{f_i}{\\sqrt{d_i}} - \\frac{f_j}{\\sqrt{d_j}} \\Bigg)^2.$$ $\\lambda$ 是 $L_{rw}$ 的一个特征值，对应的特征向量 $u$，当且仅当 $\\lambda$ 是 $L_{sym}$ 的一个特征值且对应的特征向量 $w = D^{1/2}u$。 $\\lambda$ 是 $L_{rw}$ 的一个特征值，对应的特征向量 $u$，当且仅当 $\\lambda$ 和 $u$ 是 generalized eigenproblem $Lu = \\lambda Du$ 的解。 $0$ 是 $L_{rw}$ 的特征值，常向量 $1$ 是特征向量。$0$ 是 $L_{sym}$ 的特征值且特征向量是 $D^{1/2}1$。 $L_{sym}$ 和 $L_{rw}$ 是半正定的，有 $n$ 个非负的实数特征值 $0 = \\lambda_1 \\leq \\dots \\leq \\lambda_n$。","link":"/blog/2019/11/08/a-tutorial-on-spectral-clustering/"},{"title":"Convolution on Graph: A High-Order and Adaptive Approach","text":"重新定义了卷积的定义，利用$k$阶邻接矩阵，定义考虑$k$阶邻居的卷积，利用邻接矩阵和特征矩阵构建能同时考虑顶点特征和图结构信息的卷积核。在预测顶点、预测图、生成图三个任务上验证了模型的效果。原文链接：Graph Convolution: A High-Order and Adaptive Approach 摘要我们提出了两个新的模块为图结构数据而设计：k阶卷积器和自适应滤波模块。重要的是，我们的框架(HA-GCN)是一种通用框架，可以在顶点和图上适应多种应用，还有图生成模型。我们的实验效果很好。在顶点分类和分子属性预测上超越了state-of-the-art。生成了超过32%的真实分子，在材料设计和药物筛选上很有效。 引言图卷积网络通常应用在两个学习任务上：·以顶点为中心：预测任务与顶点相关。GCN对图中的每个顶点输出一个特征向量，有效地反映顶点属性和邻居结构。举个例子，社交网络中，向量用于顶点分类和链路预测。有时也和顶点表示学习有关。·以图为中心：预测任务和图相关。举个例子，化学领域中，分子可以被看作是一个图，原子作顶点，化学键是边。根据分子的物理和化学性质，构建图卷积网络对分子进行有意义地编码。这些任务对很多生活中的应用如材料设计、药物筛选很重要。在这种情况下，图卷积通常对图进行编码，使用编码进行图的预测。 和我们的工作最相关的是Kipf &amp; Welling 2016a，他们的卷积只考虑了一阶邻居。我们的高阶操作器有着到达k阶邻居的高效设计。此外，我们还引入了自适应模块动态地基于局部图的连接和顶点属性调整权重。对比Li et al., 2015，他们将LSTM引入图中，我们的自适应模块可以解释成Xu et al., 2015提出的注意力机制。不像前人设计的模型要么是以顶点为重，要么是以图为中心，我们的HA-GCN框架是个通用的模型。除此以外，我们用HA-GCN构建了针对分子生成的图生成模型，比state-of-the-art的效果提高了很多。 贡献有两点：·引入两个模块，构建新的图卷积网络架构HA-GCN·提出了可以应用到以顶点为中心、图为中心的通用框架和图生成模型。在所有的任务上获得了state-of-the-art的效果。 PreliminariesThe Graph Model一个图$\\mathcal{G}$表示为一个对$(V, E)$，$V = \\lbrace v_1, …, v_n \\rbrace $是顶点集，$E \\in V \\times V$是边集。我们不区分有向和无向，因为我们的模型都能做。每个图可以表示为一个$n \\times n$的邻接矩阵$A$，如果$v_i$到$v_j$有边，$A_{i,j} = 1$否则为$0$。基于邻接矩阵，我们可以得到距离函数$d(v_i, v_j)$表示$v_i$到$v_j$的距离（最短距离）。此外，我们认为每个顶点$v_i$与一个特征向量$X_i \\in \\mathcal{R}^m$相关，我们使用$X = (X^T_1, X^T_2, …, X^T_n) \\in \\mathcal{R}^{n \\times m}$表示特征矩阵。 Graph Convolutional Networks (GCNs)首先，一个顶点$v_j$在图$\\mathcal{G}$上的卷积操作可以表示为：$$L_{conv}(j) = \\sum_{i \\in \\mathcal{N}_j} w_{ij} X_i + b_j$$其中$X_i \\in R_m$是顶点$v_i$的输入特征，$b_j$是偏置，$w_{ij}$是权重，对$j$是非平稳且变化的。集合$\\mathcal{N}_j$表示scope of convolution。对于传统的应用，CNN通常设计成低维度的网格且对每个顶点有着相同的连接。举个例子，图像可以看作二维网格，图$\\mathcal{G}$通过邻接的像素组成。$\\mathcal{N}_j$可以简单的定义为一个围绕在像素$j$的固定大小的block或window。 在更一般的图上，可以将$\\mathcal{N}_j$定义为顶点$v_j$的邻接顶点的集合。比如，在Duvenaud et al. 2015的工作中，fingerprint(FP)卷积操作器的核心是计算邻居的均值，也就是对于所有的$(i, j)$，$w_{ij} = 1$。通过邻接矩阵$A$，我们可以将这个操作写为$$\\tag{1}L_{FP} = AX.$$ $A$和特征矩阵$X$的乘积得到一个所有邻居顶点特征的均值。Kipf &amp; Welling 2016a提出的node-GCN使用线性组合与非线性变换得到的均值为：$$\\tag{2}L_{node-GCN} = \\sigma(AXW).$$ 权重矩阵$W$，函数$\\sigma(\\cdot)$分别是特征$X$上的线性组合与非线性变换。 Bruna et al., 2013与Defferrard et al., 2016提采用了不同的方式在图的拉普拉斯矩阵的谱上做了卷积。$H$为拉普拉斯矩阵，正交分解为$H = U \\Lambda U^T$($U$是正交矩阵，$\\Lambda$是对角矩阵)。与其在式2中加入权重矩阵，谱卷积考虑的是$H$上的一个参数化的卷积操作；$$\\tag{3}L_{spectral} = U g_\\theta U^T X.$$ 这里$g_\\theta(\\cdot)$是一个多项式函数，element-wisely应用在对角矩阵$\\Lambda$上。 在讨论谱图卷积的优点时，作者提到$k$阶多项式多项式$g_\\theta(\\cdot)$就是图的$k$阶局部，也就是说卷积会到达$k$阶邻居。对比式1和式2一阶邻居的均值，这能使信息在图上快速的传播。然而，考虑到$U \\Lambda^n U^T = A^n$，多项式$g_\\theta(\\cdot)$的选择并没有给$k$阶邻居一个明确的卷积操作，因为在卷积中不是所有的邻居都是占相同的份量。这使得我们提出了我们的高阶卷积操作。这些卷积的其他问题是，他们在图中是不变的。因此几乎不能捕获到使用卷积时不同地方的不同。这使得我们提出了自适应模块，成功的考虑了局部特征和图结构。（这里看的云里雾里的。。。） High-Order and Adaptive Graph Convolutional Network (HA-GCN)K-th Order Graph Convolution定义顶点$v_j$的$k$阶邻居：$\\mathcal{N}_j = \\lbrace v_i \\in V \\mid d(v_i, v_j) \\leq k \\rbrace $。可以通过对邻接矩阵$A$连乘得到$k$阶邻居。Proposition 1. $A$是图$\\mathcal{G}$的邻接矩阵，$A^k$的第$i$行第$j$列表示的是顶点$i$到顶点$j$的$k$步路径的个数。我们定义$k$阶卷积如下：$$\\tag{4}\\tilde{L}^{(k)}_{gconv} = (W_k \\circ \\tilde{A}^k)X + B_k,$$ 其中$$\\tag{5}\\tilde{A}^k = \\min\\lbrace A^k + I, 1\\rbrace .$$ 其中$\\circ$和$\\min$分别表示element-wise矩阵乘法和最小值。$W_k \\in \\mathcal{R}^{n \\times n}$是权重矩阵，$B_k \\in \\mathcal{R}^{n \\times m}$是偏置矩阵。$\\tilde{A}^k$是通过将$A^k + I$砍到1获得的。在$A^k$上增加单位阵是为了让图上的每个顶点都有自连接。砍到1是因为如果$A^k$有大于1的元素，砍到1确实会得到k阶邻居的卷积。卷积的输入$\\tilde{L}^{(k)}_{gconv}$是邻接矩阵$A \\in \\lbrace 0, 1\\rbrace ^{n \\times n}$和特征矩阵$X \\in \\mathcal{r}^{n \\times m}$。输出的维度和$X$一样。如同名字所示，卷积操作$\\tilde{L}^{(k)}_{gconv}$取一个顶点的$k$阶邻居的特征向量作为输入，输出他们的加权平均。 式4的操作优雅地实现了我们在图上$k$阶邻居的idea，和传统的卷积一样，是kernel size为$k$的卷积。一方面，它可以看作是式2从1阶邻居到高阶的高效的泛化。另一方面，卷积器与式3的谱图卷积关系紧密，因为谱图中的$k$阶多项式也可以被看作是一种范围为$k$阶邻居$\\mathcal{N}_j$的操作。 Adaptive Filtering Module基于式4，我们引入图卷积的自适应滤波器模块。它根据一个顶点的特征以及邻居的连接过滤卷积的权重。以化学中分子的图为例，在预测分子性质时，benzene rings比alkyl chains更重要。没有自适应模块，图卷积在空间上不变，而且不能按预期的那样工作。自适应滤波器的引入使得网络自适应地找到卷积目标并且更好的捕获局部的不一致。 自适应滤波器的想法来源于注意力机制Xu et al., 2015，他们在生成输出序列中对应的词时，自适应地的了有趣的像素。也可以看作是一种门的变体，这个门有选择的让信息通过LSTM。从技术上来讲，我们的自适应滤波器是一个在权重矩阵$W_k$上的非线性操作器$g$：$$\\tag{6}\\tilde{W_k} = g \\circ W_k,$$ 其中$\\circ$表示element-wise矩阵乘法。事实上，操作器$g$是由$\\tilde{A}^k$和$X$共同决定的，反映了顶点特征与图的连接，$$g = f_{adp}(\\tilde{A}^k, X).$$ 我们考虑了函数$f_{adp}$的两个选项：$$\\tag{7}f_{adp/prod} = \\mathrm{sigmoid}(\\tilde{A}^kXQ)$$和$$\\tag{8}f_{adp/lin} = \\mathrm{sigmoid}(Q \\cdot [\\tilde{A}^k, X]).$$ 这里，$[\\cdot, \\cdot]$表示矩阵拼接。第一个操作器通过$A$和$X$的内积考虑了顶点特征和图连接的的交互，第二个通过线性变换也实现了这个目的。事实上，我们发现线性自适应滤波器(8)比(7)在大多数任务上表现的更好。因此，我们在实验部分会采用并且记录线性的表现。自适应滤波器为了顶点的权重选择而设计出来，因此我们用了一个sigmoid非线性激活使它二值化。参数矩阵$Q$会让$f_{adp}$的输出的维度与矩阵$A$对齐。不像当前已有的动态滤波器只从顶点或边的特征中生成权重，我们的自适应滤波器模块通过同时考虑顶点特征与图的连通性有着更全面的考虑。 The Framework of HA-GCN通过在高阶卷积式4中加入自适应模块式6，我们得到HA的定义为：$$\\tilde{L}^{(k)}_{HA} = (\\tilde{W}_k \\circ \\tilde{A}^k)X = B_k$$ 图1给了卷积器和HA-GCN框架的可视化。图1a展示了对于一个顶点，$k = 2$时的$\\tilde{L}^{(k)}_{HA}$：自适应滤波器$g$的底层加到权重矩阵$W_1$和$W_2$上，得到了自适应权重$\\tilde{W}_1$和$\\tilde{W}_2$（橙色和绿色线）；第二层把自适应权重和对应的邻接矩阵拼到一起为了卷积使用。图1b强调了卷积是在图上每个顶点都做的，并且是一层一层的。需要注意的是高阶卷积器和自适应权重可以和其他的神经网络架构/操作一起使用，比如全连接层，池化层，非线性变换。我们将我们的HA操作的图卷积层命名为HA-GCN。 在所有的卷积层之后，我们将那些来自不同阶的卷积层的输出特征拼接起来：$$L_{HA} = [\\tilde{L}^{(1)}_{HA}, …, \\tilde{L}^{(K)}_{HA}].$$ HA-GCN的架构以特征矩阵$X \\in \\mathcal{R}^{n \\times m}$（$n$是图的顶点数，$m$是顶点特征的维数）作为输入，输出一个维度为$n \\times (mK)$的矩阵，特征的维数会乘以$K$倍。现在，我们将详细描述如何将HA-GCN应用到各种各样的问题上。 以顶点为中心的预测：HA-GCN的卷积之后，每个顶点和一个特征向量有关。特征向量可以用于顶点分类或回归。与网络表示学习也密切相关。使用以顶点为中心的设定，意味着我们可以给每个顶点学习出一个向量，向量反映了那个顶点周围的图的局部结构。我们的HA-GCN也给图中的每个顶点输出了一个向量。在这个场景下，HA-GCN可以看成是一个监督的图表示学习框架。 以图为中心的预测：为了解决不同尺度的图，输入的邻接矩阵和特征矩阵在底部和右侧加入了为0的padding。以顶点为中心和以图为中心的一个小不同是：以顶点为中心的任务中，一部分有标签/值的顶点作为训练，其他的作验证和输出，而以图为中心的任务，数据集是一组图（可能尺度不同），分为训练/验证/测试集。HA-GCN在这两种情况都能工作，HA卷积层中的参数是$(n^2)$，$n$是图的size（或是那些图中最大的size）。HA-GCN在顶点为中心的任务中比在图为中心的任务更容易过拟合，我们会在后面的实验部分描述这个问题。 图生成模型：从一组图$\\bar{\\mathcal{G}} = \\lbrace \\mathcal{G}_1, …, \\mathcal{G}_N \\rbrace $中学习一个概率模型，通过这个模型我可以生成之前没见过但是和$\\bar{\\mathcal{G}}$中相似的图。通过variational auto-encoder(Kingma and Welling 2013)和adversarial auto-encoder(Makhzani et al., 2015)，图卷积网络可以适用于生成模型的任务甚至是判别模型。 一个自编码器总是由两部分组成：一个编码器和一个解码器。编码器将输入数据$X \\in \\mathcal{X}$映射到一个编码向量$Y \\in \\mathcal{Y}$，解码器将$\\mathcal{Y}$映射回$\\mathcal{X}$。我们称编码空间$\\mathcal{Y}$为隐藏空间。为了使他成为一个生成模型，我们通常假设隐藏空间中有一个概率分布（比如高斯分布）。图生成模型能让我们生成分子的连续表达，通过搜索隐藏空间生成新的化学结构，可以用来指导材料设计和药物筛选。 实验Node-centric learningcitation graphs上的监督文档分类，每个图包含文档的bag-of-words特征向量，还有文档之间的引用连接。我们把这个网络当成无向图，构建一个二值对称邻接矩阵$A$。每个文档有一个类标，目标是从文档的特征和引用的图对文档标签预测。统计数据(Sen et al., 2008) Dataset Nodes Edges Classes Features Citeseer 3327 4732 6 3703 Cora 2708 5429 7 1433 Pubmed 19717 44338 210 5414 训练和架构：我们使用和Kipf &amp; Welling同样的GCN网络结构，除了将他们的一阶图卷积层换成了我们的HA层。我们使用gcn_{1,…,k}$表示$1$阶到$k$阶的图卷积层。$\\mathrm{fc}k$表示有$k$个隐藏单元的全连接层。 Name Architectures GCN gcn_{1}-fc128-gcn_{1}-fc1-softmax gcn_{1, 2} gcn{1, 2}-fc128-gcn{1, 2}-fc1-softmax adp_gcn_{1, 2} adp_gcn{1, 2}-fc128-adp_gcn{1, 2}-fc1-softmax 为了比较不同模型的性能，我们将数据集随机划分成训练/验证/测试集，比例为$7:1.5:1.5$，记录测试集的预测精度，表1。超参数：dropout rate $0.7$，L2 regularization $0.5 \\cdot 10^{-8}$，hidden units $128$。从顶点表示学习角度看，前三个是半监督的模型，后四个是半监督的模型。这也解释了为什么后面的模型效果更好。我们的二姐邻居HA图卷积在精度上提升了2%。自适应模块没能继续提升。这是因为自适应模块是为了对不同的图生成不同的滤波器权重。然而，在顶点为中心的任务中，只有一个图，卷积权重直接就学出来了。因此自适应模块在顶点为中心的任务中是冗余的。 Graph-centric learning预测分子图。目标是给定分子图，预测分子性质。我们使用Duvenaud et al., 2015描述的数据集，评估三种属性：Solubility, Drug efficacy, Organic photovolatic efficiency。训练和架构：l1_gcn和l2_gcn分别表示有一个和两个卷积层的卷积神经网络。我们记录了RMSE（表2）。 Name Architectures l1_gcn gcn_{1,2,3}-ReLU-fc64-ReLU-fc16-ReLU-fc1 l1_adp_gcn adp_gcn{1,2,3}-ReLU-fc64-ReLU-fc16-ReLU-fc1 l2_gcn [gcb_{1,2,3}-ReLU]*2-fc64-ReLU-fc16-ReLU-fc1 l2_adp_gcn [adp_gcn_{1,2,3}-ReLU]*2-fc64-ReLU-fc16-ReLU-fc1 node-GCN就是没有自适应滤波模块的一阶HA-GCN。对比node-GCN,l1_gcn,l2_gcn，可以看到我们的卷积层的效果。有自适应滤波器的比没有的效果好。 还有一部分图生成模型，就不说了。","link":"/blog/2018/07/16/convolution-on-graph-a-high-order-and-adaptive-approach/"},{"title":"Collaborative Filtering for Implicit Feedback Datasets","text":"ICDM 2008. 推荐系统：协同过滤在隐反馈数据上的应用，这个算法在GitHub上有人实现了，性能很强。这是我的阅读笔记，把论文当中的主要部分抽出来了。原文链接：Collaborative Filtering for Implicit Feedback Datasets IntroductionIn this part, this paper introduce 4 important characteristics for implicit feedback: No negative feedbackFor example, a user that did not watch a certain show might have done so because she dislikes the show or was not availale to watch it. So by observing the users behavior, we can infer which items they probably like and thus chose to consume. However, it’s hard to reliably infer which item a user did not like. Implicit feedback is inherently noiseFor example, we may view purchases behavior for an individual, but this does not necessarily indicate a positive view of thhe product. The item may have been purchased as a gift. Or a television is on a particular channel and a particular time, but the viewer might be asleep. The numerical value of explicit feedback indicate preference, whereas the numerical value of implicit feedback indicates confidenceNumerical values of implicit feedback describe the frequency of actions, e.g., how much time the user watched a certain show, how frequently a user is buying a certain item, etc. A larger value is not indicating a higher preference. Evaluation of implicit-feedback recommender requires appropriate measuresFor example, if we gather data on television viewing, it’s unclear how to evaluate a show that has been watched more than once, or how to compare two shows that are on at the same time, and hence cannot both be watched by the user. preliminariesnotions:users $u, v$items $i, j$observations $r_{ui}$, associate users and items. For explicit feedback datasets, those values would be ratings that indicate the preference by user $u$ and item $i$. For implicit datasets, $r_{ui}$ can indicate observations for user actions. For example, $r_{ui}$ can indicate the number of times $u$ purchased item $i$ or the time $u$ spent on webpage $i$. previous workNeighborhood modelsIts original form is user-oriented, see [1] for a good analysis.Later, an analogous item-oriented approach [2,3] became popular. In those methods, a rating is estimated using known ratings made by the same user on similar items. In addition, item-oriented methods are more amenable to explaining the reasoning behind predictions. This is because users are familiar with items previously preferred by them, but usually do not know those allegedly like minded users.Central to most item-oriented approaches is a similarity measure between items, where $s_{ij}$ denotes the similarity of $i$ and $j$. Frequently, it is based on the Pearson correlation coeffcient. Our goal is to predict $r_{ui}$--the unobserved value by user $u$ for item $i$. Using the similarity maesure, we identify the $k$ items rated by $u$, which are most similar to $i$. This set of $k$ neighbors is denoted by $S^k(i;u)$. The predicted value of $r_{ui}$ is taken as a weighted average of the ratings for neighboring items:$$\\hat{r}_{ui} = \\frac{\\sum_{j\\in S^k(i;u)}s_{ij}r_{uj}}{\\sum_{j\\in S^k(i;u)}s_{ij}}$$Some enhancements of this scheme are well practiced for explicit feedback, such as correcting for biases caused by varying mean ratings of different users and items.All item-oriented models share a disadvantage in regards to implicit feedback - they do not provide the flexibility to make a distinction between user preferences and thhe confidence we might have in those preferences. Latent factor modelsLatent factor models comprise an alternative approach to CF with the more holistic goal to uncover latent features that explain observed ratings; example include pLSA\\cite{ref4}, neural networks\\cite{ref5}, and Latent Dirichlet Allocation\\cite{ref6}. We will focus on models that are induced by Singular Value Decomposition(SVD) of the user-item observations matrix. Many of the recent works, applied to explicit feedback datasets, suggested modeling directly only the observed ratings, while avoiding overfitting through an adequate regularized model, such as:$$\\min \\limits_{x_,y_} \\sum \\limits_{r_{w,i}is known} (r_{ui}-x^T_uy_i)^2+\\lambda (\\lVert x_u\\rVert^2+\\lVert y_i \\rVert^2)$$Here, $\\lambda$ is used for regularizing the model. Parameters are often learnt by stochastic gradient descent; Our modelFirst, we need to formalize the notion of confidence which the $r_{ui}$ variables measure. To this end, let us introduce a set of binary variables $p_{ui}$, which indicates the preference of user $u$ to item $i$. The $p_{ui}$ values are derived by binarizing the $r_{ui}$ values:$$p_{ui}=\\begin{cases}1 &amp; r_{ui}&gt;0\\\\0 &amp; r_{ui}=0\\end{cases}$$In other words, if a user $u$ consumed item $i$($r_{ui}&gt;0$), then we have an indication that $u$ likes $i$($p_{ui}=1$). On the other hand, if $u$ never comsumed $i$, we believe no preference($p_{ui}=0$).We will have different confidence levels also among items that are indicated to be preferred by the user. In general, as $r_{ui}$ grows, we have a stronger indication that the user indeed like thhe item. Consequently, we introduce a set of variables, $c_{ui}$, which measure our confidence in observing $p_{ui}$. A plausible choice for $c_{ui}$ would be:$$c_{ui} = 1 + \\alpha r_{ui}$$This way, we have some minimal confidence in $p_{ui}$ for every user-item pair, but as we observe more evidence for positive preference, our confidence in $p_{ui}=1$ increases accordingly. The rate of increase is controlled by the constant $\\alpha$. In our experiments, setting $\\alpha = 40$ was found to produce good results.Our goal is to find a vector $x_u\\in \\mathbb{R}^f$ for each user $u$, and a vector $y_i\\in \\mathbb{R}^f$ for each item $i$ that will factor user preferences. These vectors will be known as the user-factors and the item-factors, respectively. Preferences are assumed to be the inner products: $p_{ui}=x^T_uy_i$. Essentially, the vectors strive to map users and items into a common latent vector space where they can be directly compared. This is similar to matrix factorization techniques which are popular for explicit feedback data, with two important distinction: (1) We need to account for the varying confidence levels, (2) Optimization should account for all possible $u, i$ pairs, rather than only these corresponding to observed data. Accordingly, factors are computed by minimizing the following cost function:$$\\min \\limits_{x_, y_}\\sum \\limits_{u,i}c_{ui}(p_{ui}-x^T_uy_i)^2+\\lambda(\\sum\\limits_{u}\\lVert x_u\\rVert^2+\\sum\\limits_{i}\\lVert y_i\\rVert^2)$$The $\\lambda(\\sum\\limits_{u}\\lVert x_u\\rVert^2+\\sum\\limits_{i}\\lVert y_i\\rVert^2)$ term is necessary for regularizing the model such that it will not overfit the training data. [1]. Herlocker J L, Konstan J A, Borchers A, et al. An algorithmic framework for performing collaborative filtering[C]. international acm sigir conference on research and development in information retrieval, 1999: 230-237.[2]. Linden G, Smith B, York J C, et al. Amazon.com recommendations: item-to-item collaborative filtering[J]. IEEE Internet Computing, 2003, 7(1): 76-80.[3]. Sarwar B M, Karypis G, Konstan J A, et al. Item-based collaborative filtering recommendation algorithms[J]. international world wide web conferences, 2001: 285-295.[4]. Hofmann T. Latent semantic models for collaborative filtering[J]. ACM Transactions on Information Systems, 2004, 22(1): 89-115.[5]. Salakhutdinov R, Mnih A, Hinton G E, et al. Restricted Boltzmann machines for collaborative filtering[C]. international conference on machine learning, 2007: 791-798.[6]. Blei D M, Ng A Y, Jordan M I, et al. Latent Dirichlet Allocation[C]. neural information processing systems, 2002, 3(0): 601-608.","link":"/blog/2018/01/18/collaborative-filtering-for-implicit-feedback-datasets/"},{"title":"Convolutional Sequence to Sequence Learning","text":"ICML 2017. Facebook 2017年的卷积版seq2seq。卷积加注意力机制，外加GLU，训练速度很快，因为RNN训练时依靠上一个元素的隐藏状态，CNN可以并行训练。原文链接：Convolutional Sequence to Sequence Learning Abstract流行的序列到序列的学习方法将输入的序列通过循环神经网络映射到一个变长输出序列。我们提出了一个完全基于卷积神经网络的架构。对比循环神经网络模型，在训练过程中对所有元素的运算都可以并行，而且可以充分利用GPU资源，优化过程也会变得更加容易，因为非线性单元的个数是固定的，而且与输入长度无关。我们使用的门控线性单元（GLU）可以帮助梯度传播，我们在每个解码层上部署了一个独立的注意力模块。我们算法表现在WMT’14英语-德语和WMT’14英语-法语两个数据集上，都要比Wu等人的深度LSTM拥有更高的精度，且在GPU和CPU上训练速度都快了一个量级。 1. Introduction序列到序列模型在很多任务中都大获成功，如机器翻译、语音识别（Sutskever et al., 201v4; Chorowski et al., 2015），文本摘要（Rush et al., 2015; Nallapati et al., 2016; Shen et al., 2016）等等。当今主流的方法将输入序列使用一个双向循环神经网络进行编码，并且用另一个循环神经网络生成一个变长输出序列，这两个模型通过soft-attention（Bahdanau et al., 2014; Luong et al., 2015）相连。在机器翻译中，这个架构已经比传统的基于短语的模型要好出很多了（Sennrich et al., 2016b; Zhou et al., 2016; Wu et al., 2016）。 尽管卷积神经网络有很多优点，但是很少应用在序列建模中（Waibel et al., 1989; LeCun &amp; Bengio, 1995）。对比循环神经网络中的循环层，卷积是对定长的内容生成表示，然而，有效的卷积长度可以通过简单的堆叠卷积层变得逐渐增加。这就使得我们可以精确地控制要建模的依赖关系的最大长度。卷积神经网络不需要依赖于之前时间步的计算，因此可以在序列中的任意一个地方进行并行运算。RNN需要维持一个含有整个过去信息的隐藏状态，导致对序列进行计算时不能并行。 多层卷积神经网络在整个输入序列上构建了层次表示，在这个层次表示中，底层是临近的输入元素交互，高层是离得较远的元素交互。层级结构相比于链式结构的循环神经网络，提供了一条更短的路径来捕获长范围的依赖关系。比如，我们可以用一个滑动窗在 $n$ 个单词上，使用复杂度为 $O(\\frac{n}{k})$ 的卷积操作，卷积核宽度为 $k$，来获取单词间的关系，提取到一个特征表示，如果用循环神经网络，复杂度为 $O(n)$。卷积神经网络的输入会被放入一个有着固定数目的卷积核和非线性单元的网络中，然而循环神经网络对第一个单词进行了 $n$ 次操作和非线性变换后，对最后一个单词只进行了一组操作。固定数目的非线性操作也可以简化学习过程。 最近在卷积神经网络应用于序列建模的工作，像 Bradbury et al.(2016)，他们在一连串的卷积层间引入了循环池化。Kalchbrenner et al.(2016) 解决了没有注意力机制的神经机器翻译的问题。然而，这些方法的表现没有一个在大型的数据集上超越当前最先进的技术。门控卷积在之前已经被 Meng et al.(2015) 用于了机器翻译，但是他们的评估方法受限于小数据集，而且模型与传统的基于计数的模型相串联。有一部分是卷积层的架构在大数据集上展现了更好的效果但是他们的解码器仍然是循环的。 在这篇文章中，我们提出了一个针对序列到序列的模型，这个模型完全是基于卷积的。我们的模型使用了门控线性单元（Dauphin et al., 2016）和残差连接（He et al., 2015a）。我们在每个解码层也使用了注意力机制，而且显示出每个注意力层只增加了一点点可以忽略不计的开销。这些方法的融合可以让我们处理大规模的数据集。 我们在几个大型的机器翻译数据集上评估了我们的方法，并且和文章中现有的最好的架构们进行了对比与总结。在 WMT’16 英语-罗马尼亚语数据集上，我们的算法是最好的，比之前最好的结果要好 1.9 BLEU。在 WMT’14 英语-德语上，我们比 Wu et al.(2016) 的 strong LSTM 好 0.5 BLEU。在 WMT’14 英语-法语上，我们比 Wu et al.(2016) 的 likelihood trained system 好 1.6 BLEU。除此以外，我们的模型可以翻译从未见过的句子，速度比 Wu et al.(2016) 的算法在 GPU 和 CPU 上都快出一个数量级。 2. Recurrent Sequence to Sequence Learning序列到序列建模又称基于循环神经网络的编码器-解码器架构（Sutskever et al., 2014; Bahdanau et al., 2014）。编码器 RNN 处理输入序列 $\\mathbf{x} = (x_1, …, x_m)$ $m$ 个元素，返回状态表示 $\\mathbf{z} = (z_1, …, z_m)$。解码器 RNN 接受 $\\bf{z}$ 并且从左到右生成输出序列 $\\mathbf{y} = (y_1, …, y_n)$，一次一个元素。为了生成输出 $y_{i+1}$，解码器基于之前的隐藏状态 $h_i$ ，前一个目标单词 $y_i$ 的嵌入表示 $g_i$，还有一个源自编码器输出 $\\bf{z}$ 的条件输入 $c_i$，计算一个新的隐藏状态 $h_{i+1}$。基于这个大体的规则，各种各样的编码-解码架构被相继提出，他们之间主要差别是条件输入和RNN的类型不同。 没有注意力机制的模型通过对所有的 $i$ 设定 $c_i = z_m$，只考虑最后的编码状态 $z_m$（Cho et al., 2014），或是简单地将第一个解码器的隐藏状态初始化为 $z_m$（Sutskever et al., 2014），在后面这种情况中没有用到 $c_i$。带注意力机制的架构（Bahdanau et al., 2014; Luong et al., 2015）在每个时间步，计算 $(z_1, …, z_m)$ 的加权和得到 $c_i$。求和时每一项的权重称为注意力分数，可以使网络在输出序列时关注于输入序列的不同部分。注意力分数本质上是通过比较每个编码器状态 $z_j$ 和之前的解码器隐藏状态 $h_i$ 与最后的预测结果 $y_i$ 的线性组合，计算得到；结果通过归一化得到了在输入序列上的一个分布。 当前流行的编解码器使用的模型是长短时记忆网络（LSTM; Hochreiter &amp; Schmidhuber, 1997）和门控循环单元（GRU; Cho et al., 2014）。这两个都是使用门控机制对Elman的RNN（Elman, 1990）进行了扩展，门控机制使得模型可以记得前一时间步获得的信息，以此来对长期依赖进行建模。大多数最近的方法也依赖于双向编码器对过去和未来的上下文环境同时地构建表示（Bahdanau et al., 2014; Zhou et al., 2016; Wu et al., 2016）。通常带有很多层的模型会依赖于残差网络的残差连接（He et al., 2015a; Zhou et al., 2016; Wu et al., 2016）。 3. A Convolutional Architecture接下来我们介绍一个序列到序列的完全卷积架构。我们使用卷积神经网络替代 RNN 来计算中间的编码器状态 $\\bf{z}$ 和解码器状态 $\\bf{h}$。 3.1. Position Embeddings首先，我们将输入序列 $\\mathbf{x} = (x_1, …, x_m)$ 嵌入到一个分布的空间内 $\\mathbf{w} = (w_1, …, w_m)$中，其中 $w_j \\in \\mathbb{R}^f$ 是嵌入矩阵 $\\mathcal{D} \\in \\mathbb{R}^{V \\times f}$ 的一列。我们也通过嵌入输入元素 $\\mathbf{p} = (p_1, …, p_m)$ 的绝对位置使模型对顺序敏感，其中 $p_j \\in \\mathbb{R}^f$。这两者做加和获得输入元素的表示 $\\mathbf{e} = (w_1+p_1, …, w_m+p_m)$。我们对解码器输出的元素也做相似的工作来生成输出元素表示 $\\mathbf{g} = (g_1, …, g_n)$，然后再传入解码网络。位置嵌入在我们的架构中很有用，因为他们能让模型知道当前正在处理的输入或输出序列中的哪个部分。 3.2. Convolutional Block Structure编码器和解码器网络共享一个简单的块状结构，这个块状结构会基于固定长度的输入元素计算出中间状态。我们将解码器的第 $l$ 个块的输出记为 $\\mathbf{h}^l = (h^l_1, …, h^l_n)$，编码器的第 $l$ 个输出记为 $\\mathbf{z}^l=(z^l_1, …, z^l_m)$；我们交替地称块和层。每个块包含一个一维卷积加一个非线性单元。对于一个有着一个块和宽度为 $k$ 的卷积核的解码网络来说，每个结果状态 $h^l_i$ 包含了过去 $k$ 个输入元素的信息。堆叠多个块增加了在一个状态中可以表示的输入元素的数量。比如，堆叠6个宽度为 $k=5$ 的块可以得到输入空间为25个元素的表示，也就是每个输出依赖于25个输入。非线性单元允许网络利用整个输入空间，或者在必要时关注更少的元素。 每个卷积核的参数为 $W \\in \\mathbb{R}^{2d \\times kd}$，$b_w \\in \\mathbb{R}^{2d}$，接收的输入为 $X \\in \\mathbb{R}^{k \\times d}$，输入是 $k$ 个输入元素嵌入到 $d$ 维空间得到的向量的拼接，然后将他们映射到一个有着输入元素维数两倍的输出元素 $Y \\in \\mathbb{R}^{2d}$ 上；后续的层对前一层的 $k$ 个输出元素进行操作。我们选择门控线性单元（GLU; Dauphin et al., 2016）作为非线性激活单元，这是一个在卷积 $Y=[A \\ B] \\in \\mathbb{R}^{2d}$ 的输出上实现的一个简单的门控机制： $$v([A B])=A \\otimes \\sigma(B)$$ 其中 $A, B \\in \\mathbb{R}^d$ 是非线性层的输入，$\\otimes$ 是元素对元素相乘，输出 $v([A B]) \\in \\mathbb{R}^d$ 是 $Y$ 的大小的一半。门 $\\sigma(B)$ 控制当前的上下文环境中哪个输入 $A$ 是相关的。一个相似的非线性单元由 Oord et al. (2016b) 提出，他们在 $A$ 上加了 $tanh$ 但是 Dauphin et al. (2016) 展示了GLUs在语言模型中表现的更好。 为了让深度卷积网络可行，我们在每个卷积的输入和块的输出间加入了残差连接（He et al., 2015a）。 $$h^l_i=v(W^l[h^{l-1}_{i-k/2}, …, h^{l-1}_{i+k/2}] + b^l_w) + h^{l-1}_i$$ 对于编码器网络，我们确信卷积层的输出通过在每层增加 padding 可以匹配输入长度。然而，对于解码器网络我们需要注意对于解码器来说没有可用的未来信息（Oord et al., 2016a）。详细来说就是，我们在输入的左右两侧都加了 $k-1$ 个 0 作为 padding，并且从卷积输出的末端删除了 $k$ 个元素。 我们也在嵌入的大小 $f$ 和 卷积的输出，也就是大小为 $2d$ 的空间中做了线性映射关系。我们在将嵌入表示输入到编码器的时候，对 $\\bf{w}$ 使用了这样的变换，也对编码器输出 $z^y_j$，解码器在 softmax $\\bf{h^L}$ 之前的最后一层，以及计算注意力值之前的解码器的每一层$h^l$。 最后，我们计算了在 $T$ 个可能的下一个目标元素 $y_{i+1}$ 上的分布，通过将解码器最上面的输出做权重为 $W_o$ 偏置为 $b_o$ 的线性组合得到： $$p(y_{i+1} \\mid y_1, …, y_i, \\mathbf{x}) = \\text{softmax} (W_oh^L_i+b_o) \\in \\mathbb{R}^T$$ 3.3. Multi-step Attention 我们对于\b每一个解码层引入了一个分开的注意力机制。为了计算注意力，我们融合了当前解码器状态$h^l_i$和前一个目标元素$g_i$的嵌入表示：$$d^l_i=W^l_dh^l_i + b^l_d + g_i$$解码器层$l$的注意力$a^l_{ij}$的状态$i$和源元素$j$是通过解码器状态的汇总$d^l_i$和最后一个编码器块$u$的每个输出$z^u_j$的点乘得到：$$a^l_{ij}=\\frac{\\exp(d^l_i \\cdot z^u_j)}{\\sum^m_{t=1}\\exp(d^l_i \\cdot z^u_t)}$$对当前解码器层的条件输入$c^l_i$是编码器输出的加权求和，也就是输入元素嵌入$e_j$（图1，中右侧）：$$c^l_i=\\sum^m_{j=1}a^l_{ij}(z^u_j+e_j)$$这与循环神经网络的方法稍有不同，在循环神经网络中，注意力和$z^u_j$的\b加权求和是同时计算的(This is slightly different to recurrent approaches which compute both the attention and the weighted sum over $z^u_j$ only)。我们发现加入$e_j$更有效，而且它与键值记忆网络相似，后者的键是$z^u_j$，值是$z^u_j+e_j$（Miller et al., 2016）。编码器输出$z^u_j$潜在地表达了大量的输入上下文，$e_j$提供了一个在做预测时有效的输入元素的点信息。一旦$c^l_i$被计算得出，它就会被简单的加到对应解码层$h^l_i$的输出上。对比单步注意力机制（Bahdanau et al., 2014; Luong et al., 2015; Zhou et al., 2016; Wu et al., 2016）来说，这可以被看作是多步“跳跃”（Sukhbaater et al., 2015）的注意力机制。特别地，第一层的注意力决定了被输入到第二层的一个有效的源上下文，第二层在计算注意力的时候考虑了这个信息。解码器也可以直接获取注意力的前$k-1$步历史，因为条件输入$c^{l-1}_{i-k}, …, c^{l-1}_{i}$是$h^{l-1}_{i-k}, …, h^{l-1}_i$的一部分，而后者是$h^l_i$的输入。对于循环神经网络来说，前几步的输入在隐藏状态中，并且需要经过多个非线性单元后还保存下来，但是对于卷积的网络来说，考虑已经被加入的前几步信息更加简单。总的来说，我们的注意力机制考虑了我们之前已经加入的哪个单词，并且每个时间步都实现了多次跳跃注意力机制。在后记$C$中，我们绘制了对于深层解码器的注意力分数，显示出了不同层，被考虑的源\b输入的不同位置。我们的卷积架构与RNN相比可以批量的计算一个序列的所有元素的注意力（图1，中间）。我们分别地计算了每个解码器层。 正则化的策略我们\b通过小心的权重初始化稳定了学习过程$\\text{\\S3.5}$，并且通过缩放网络的部分使透过网络中的方差不会剧烈的变化。特别地，我们也对残差块的输出和注意力进行缩放来保持激活后的方差。我们将输入的加和和一个残差块的输出乘以了$\\sqrt{0.5}$，来使和的方差减半。这假设了被加数有着相同的方差，虽说不一定总是能这样，但是在使用的时候还是很有效的。通过注意力机制生成的条件输入$c^l_i$是$m$个向量的加权求和，我们通过缩放$m\\sqrt{1/m}$抵消了在方差上的一个变化；我们假设注意力分数是均匀分布的，对输入乘以了$m$来使他从原来的大小放大。一般我们不这么干，但是我们发现在实际情况中表现得还挺好。对于带有多个注意力机制的卷积解码器，我们根据我们的使用的注意力机制的数量对编码层的梯度进行缩放；我们排除了源单词的嵌入。我们发现这样可以稳定训练过程，因为如果不这样编码器就会接受到很大的梯度。 初始化在对不同层的输出进行加和的时候，比如残差连接，对激活单元进行归一化时需要很小心的权重初始化。我们的初始化策略是受到了归一化的启发：保持网络前向和反向传播时激活后的值的方差。所有的嵌入表示从一个均值为0，标准差为0.1的正态分布中初始化得到。对于输出不直接输入到门控线性单元的层，我们从$\\mathcal{N}(0, \\sqrt{1/n_l})$中初始化权重，其中$n_l$是每个神经元输入连接数。这确保了一个均匀分布输入的方差是保持不变的。对于通过GLU激活的层来说，我们提出了一个通过adapting the derivations in（He et al., 2015b; Glorot &amp; Bengio, 2010; Appendix A）的初始化机制。如果GLU的输入服从均值为0的分布，并且有充分小的方差，那么我们可以用输入方差的四分之一来近似输出方差（Appendix A.1）。因此，我们初始化权重，使得GLU激活的输入有着4倍的\b输入的方差。这可以通过对$\\mathcal{N}(0, \\sqrt{r/n_l})$采样初始化得到。偏置项在网络构建时均匀的设置为0。我们在某些层的输入使用了dropout，以便输入保持一个概率$p$。这可以看作是一个伯努利随机变量\b取值为$1/p$的概率为$p$，其他为0（Srivastava et al., 2014）。dropout的应用会使得方差被$1/p$缩放。我们的目标是通过使用大的权重来初始化各个层来恢复进入的方差(We aim to restore the incoming variance by initializing the respective layers with larger weights)。特别地，我们使用$\\mathcal{N}(0, \\sqrt{4p/n_l})$初始化那些输出会输入至GLU的层，使用$\\mathcal{N}(0, \\sqrt{p/n_l})$来初始化其他的。","link":"/blog/2018/05/22/convolutional-sequence-to-sequence-learning/"},{"title":"Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting","text":"NIPS 2015. 将 FC-LSTM 中的全连接换成了卷积，也就是将普通的权重与矩阵相乘，换成了卷积核对输入和隐藏状态的卷积，为了能捕获空间信息，将输入变成了4维的矩阵，后两维表示空间信息。两个数据集：Moving-MNIST 和 雷达云图数据集。原文链接：Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting Abstract降雨量预测的目标是预测未来一个局部区域再相对短的一个时间段的降雨密度。之前几乎没有研究从机器学习角度研究这个重要而且很有挑战的问题。在这篇论文中，我们将降雨量预测问题定义成一个时空序列预测问题，输入和预测都是时空序列。通过扩展 fully connected LSTM (FC-LSTM)，在输入到隐藏与隐藏到隐藏的变换都加入卷积结构，我们提出了 convolutional LSTM (ConvLSTM)，用它做了一个端到端的模型来预测降雨量。实验表明我们的 ConvLSTM 网络比 FC-LSTM 以及最先进的 ROVER 算法在降水量预测上能更好地捕获时空关系。 1. Introduction2. Preliminaries2.1 Forumulation of Precipitation Nowcasting Problem降水量预测的目标是使用之前的雷达声波序列预测一个区域（如香港、纽约、东京）未来定长（时间）的雷达图。在实际应用中，雷达图通常从气候雷达每6到10分钟获得一次，然后预测未来1到6小时，也就是说，预测6到60帧。从机器学习的角度来看，这个问题可以看作是时空序列预测问题。 假设我们在一个空间区域观测了一个动态系统，由一个 $M \\times N$ 的网格组成，$M$ 行 $N$ 列。在网格的每个单元格内部随着时间的变化，有 $P$ 个测量值。因此，观测值在任意时刻可以表示成一个张量 $\\mathcal{X} \\in \\mathbf{R}^{P \\times M \\times N}$，其中 $\\mathbf{R}$ 表示观测到的特征的定义域。如果我们周期性的记录观测值，我们可以得到一个序列 $\\hat{\\mathcal{X}_1}, \\hat{\\mathcal{X}_2}, …, \\hat{\\mathcal{X}_t}$。这个时空序列预测问题是给定前 $J$ 个观测值，预测未来长度为 $K$ 的序列： $$\\tag{1}\\tilde{\\mathcal{X}}_{t+1}, …, \\tilde{\\mathcal{X}}_{t+K} = \\mathop{\\arg \\max}_{\\mathcal{X}_{t+1}, …\\mathcal{X}_{t+K}} p(\\mathcal{X}_{t+1}, …, \\mathcal{X}_{t+K} \\mid \\hat{\\mathcal{X}}_{t-J+1}, \\hat{\\mathcal{X}}_{t-J+2}, …, \\hat{\\mathcal{X}}_t)$$ 对于降雨量预测，每个时间戳的观测值是一个2D雷达地图。如果我们将地图分到平铺且不重合的部分，将每个部分内的像素看作是它的观测值（图1），预测问题就会自然地变成一个时空序列预测问题。 我们注意到我们的时空序列预测问题与一步时间序列预测问题不同，因为我们问题的预测目标是一个包含时间和空间结构的序列。尽管长度为$K$的序列中的自由变量的数量可以达到$O(M^KN^KP^K)$，实际上我们可以挖掘可能的预测值的空间结构，减小维度，使问题变得容易处理。 2.2 Long Short-Term Memory for Sequence Modeling这篇论文，我们使用 FC-LSTM 的公式[11] $$\\tag{2}\\begin{aligned}i_t &amp;= \\sigma ( W_{xi} x_t + W_{hi} h_{t-1} + W_{ci} \\circ c_{t-1} + b_i) \\\\f_t &amp;= \\sigma ( W_{xf} x_t + W_{hf} h_{t-1} + W_{cf} \\circ c_{t-1} + b_f) \\\\c_t &amp;= f_t \\circ c_{t-1} + i_t \\circ \\mathrm{tanh}(W_{xc} x_t + W_{hc} h_{t-1} + b_c) \\\\o_t &amp;= \\sigma ( W_{xo} x_t + W_{ho} h_{t-1} + W_{co} \\circ c_t + b_o ) \\\\h_t &amp;= o_t \\circ \\mathrm{tanh}(c_t)\\end{aligned}$$ 多个 LSTM 可以堆叠，对复杂的结构在时间上拼接。 3 The Model尽管 FC-LSTM 层已经在时间关系上表现的很有效了，但是在空间数据上有很多的冗余。为了解决这个问题，我们提出了 FC-LSTM 的扩展，在输入到隐藏，以及隐藏到隐藏的变换上有了卷积的结构。通过堆叠多个 ConvLSTM 层，形成一个 encoding-forecasting 结构，我们可以构建一个不仅可以处理降雨量预测问题，还可以处理更一般的时空序列预测问题的模型。 3.1 Convolutional LSTMFC-LSTM 的主要问题是处理时空数据的时候，它的全连接层没有对空间信息进行编码。为了解决这个问题，我们的设计中的一个特征就是，所有的输入 $\\mathcal{X}_1, …, \\mathcal{X}_t$，细胞状态 $\\mathcal{C}_1, …, \\mathcal{C}_t$，隐藏状态 $\\mathcal{H}_1, … \\mathcal{H}_t$，还有门 $i_t, f_t, o_t$ 都是三维的张量，而且后两维都是空间维（行和列）。为了更好的理解输入和状态，我们可以把它们想象成在空间网格站立的向量。ConvLSTM 通过输入和局部邻居的上一个状态决定了一个特定细胞的未来状态。通过在输入到隐藏，隐藏到隐藏中使用一个卷积操作器就可以轻松的实现（图2）。ConvLSTM的重要的公式如（3）所示（下面的公式），$\\ast$表示卷积操作，$\\circ$表示Hadamard积： $$\\tag{3}\\begin{aligned}i_t &amp;= \\sigma( W_{xi} \\ast \\mathcal{X}_t + W_{hi} \\ast \\mathcal{H}_{t-1} + W_{ci} \\circ \\mathcal{C}_{t-1} + b_i )\\\\f_t &amp;= \\sigma( W_{xf} \\ast \\mathcal{X}_t + W_{hf} \\ast \\mathcal{H}_{t-1} + W_{cf} \\circ \\mathcal{C}_{t-1} + b_f )\\\\\\mathcal{C}_t &amp;= f_t \\circ \\mathcal{C}_{t-1} + i_t \\circ \\mathrm{tanh}(W_{xc} \\ast \\mathcal{X}_t + W_{hc} \\ast \\mathcal{H}_{t-1} + b_c)\\\\o_t &amp;= \\sigma( W_{xo} \\ast \\mathcal{X}_t + W_{ho} \\ast \\mathcal{H}_{t-1} + W_{co} \\circ \\mathcal{C}_t + b_o )\\\\\\mathcal{H}_t &amp;= o_t \\circ \\mathrm{tanh}(\\mathcal{C}_t)\\end{aligned}$$ 如果我们将状态看作是移动物体的隐藏表示，带有一个更大的变换卷积核的 ConvLSTM 应该能捕获更快的移动，而小的核能捕获慢的移动。同时，如果我们用[16]的角度来看，由式2表示的传统的 FC-LSTM 的输入、细胞输出以及隐藏状态可以看作是一个后两维都是1的三维张量。这样的话，FC-LSTM实际上是所有特征都站立在一个单独细胞上 ConvLSTM 的一个特例。 为了确保状态和输入有相同个数的行和列，需要在卷积操作之前加入 padding 操作。这里隐藏状态在边界点的 padding 可以看作是使用 state of the outside world 来计算。通常，在第一个输入来之前，我们将 LSTM 的所有状态初始化为0，对应未来的 “total ignorance”。相似地，如果我们在隐藏状态上用 zero-padding，我们实际上将 state of the outside world 设定为0，而且假设没有关于外部的先验知识。通过在状态上加 padding，我们可以不同地对待边界点，很多时候这都是有用的。举个例子，假设我们的系统正在观测一个被墙环绕的移动的球。尽管我们不能看到这些墙，但是我们可以通过观察球的一次次反弹推测它们的存在，如果边界点像内部的点一样有相同的状态变化动态性，那这就几乎不可能了。 3.2 Encoding-Forecasting Structure就像 FC-LSTM，ConvLSTM 可以使用块对复杂的结构建模。对于我们的时空序列预测问题，我们使用图3这样的结构，由两个网络组成，一个编码网络，一个预测网络。就像[21]，预测网络的初始状态和细胞输出从编码网络的最后一个状态复制过来。两个网络都通过堆叠多个 ConvLSTM 层构成。因为我们的预测目标与输入的维度相同，我们将预测网络所有的状态拼接，放到一个 $1 \\times 1$ 的卷积层中，生成最后的预测结果。 我们使用像[23]一样的观点解释这个结构。编码 LSTM 将整个输入序列压缩到一个隐藏状态的张量中，预测 LSTM 解压了这个状态，给出了最后的预测： $$\\tag{4}\\begin{aligned}\\tilde{\\mathcal{X}_{t+1}}, …, \\tilde{\\mathcal{X}_{t+K}} &amp;= \\mathop{\\arg \\max}_{\\mathcal{X}_{t+1}, …, \\mathcal{X}_{t+K}} p(\\mathcal{X}_{t+1}, …, \\mathcal{X}_{t+K} \\mid \\hat{\\mathcal{X}}_{t-J+1}, \\hat{\\mathcal{X}}_{t-J+2}, …, \\hat{\\mathcal{X}}_t) \\\\&amp;\\approx \\mathop{\\arg \\max}_{\\mathcal{X}_{t+1}, …, \\mathcal{X}_{t+K}} p(\\mathcal{X}_{t+1}, …, \\mathcal{X}_{t+K} \\mid f_{encoding} (\\hat{\\mathcal{X}}_{t-J+1}, \\hat{\\mathcal{X}}_{t-J+2}, …, \\hat{\\mathcal{X}}_t)) \\\\&amp;\\approx g_{forecasting}(f_{encoding}(\\hat{\\mathcal{X}}_{t-J+1}, \\hat{\\mathcal{X}}_{t-J+2}, …, \\hat{\\mathcal{X}}_t))\\end{aligned}$$ 这个结构与[21]中的 LSTM 未来预测模型相似，除了我们的输入和输出元素都是三维的张量，保留了所有的空间信息。因为网络堆叠了多个 ConvLSTM 层，它由很强的表示能力可以在复杂的动态系统中给出预测，如降雨量预测问题。 4. Experiments我们将我们的模型 ConvLSTM 与 FC-LSTM 在一个人工生成的 Moving-MNIST 数据集上做了对比，对我们的模型进行一个初步的了解。我们使用了不同的层数以及不同的卷积核大小，也研究了一些 “out-of-domain” 的情况，如[21]。为了验证我们的模型在更有挑战的降雨量预测问题上的有效性，我们构建了一个新的雷达声波图，在几个降雨量预测的指标上，比较了我们的模型和当前最先进的 ROVER 算法。结果显示这两个数据集上： ConvLSTM 比 FC-LSTM 在处理时空关系时更好 隐藏状态到隐藏状态的卷积核的尺寸大于1对于捕获时空运动模式来说很重要 深、且参数少的模型能生成更好的结果 ConvLSTM 比 ROVER 在降雨量预测上表现的更好。","link":"/blog/2018/09/27/convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting/"},{"title":"Deep Residual Learning for Image Recognition","text":"CVPR 2015，ResNet，原文链接：Deep Residual Learning for Image Recognition Deep Residual Learning for Image RecongnitionproblemsWhen deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error, as reported in [11, 42] and thoroughly verified by our experiments. Fig. 1 shows a typical example. Figure 1. Training error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer “plain” networks. The deeper network has higher training error, and thus test error. Similar phenomena on ImageNet is presented in Fig. 4. The degradation (of training accuracy) indicates that not all systems are similarly easy to optimize. Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time). Deep Residual LearningResidual LearningLet us consider $\\mathcal{H}(x)$ as an underlying mapping to be fit by a few stacked layers (not necessarily the entire net), with $x$ denoting the inputs to the first of these layers. If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, $i.e.$, $\\mathcal{H}(x)-x$ (assuming that the input and output are of the same dimensions). So rather than expect stacked layers to approximate $\\mathcal{H}(x)$, we explicitly let these layers approximate a residual function $\\mathcal{F}(x):=\\mathcal{H}(x)-x$. The original function thus becomes $\\mathcal{F}(x)+x$. Although both forms should be able to asymptotically approximate the desired functions (as hypothesized), the ease of learning might be different. Identity Mapping by Shortcuts$$y = \\mathcal{F}(x, {W_i})+x$$Here $x$ and $y$ are the input and output vectors of the layers considered. The function $\\mathcal{F}(x, W_i)$ represents the residual mapping to be learned. For the example in Fig. 2 that has two layers, $\\mathcal{F} = W_2\\sigma (W_1x)$ in which $\\sigma $ denotes ReLU and the bias are omitting for simplifying notations. The operation $\\mathcal{F}+x$ is performed by a shortcut connection and element-wise addition. We adopt the second nonlinearity after the addtion (i.e., $\\sigma(y)$, see Fig.2). Figure2. Residual learning: a building block. The dimensions of $x$ and $\\mathcal{F}$ must be equal in Eqn.(1). If this is not the case (e.g., when changing the input/output channels), we can perform a linear projection $W_s$ by the shortcut connections to match the dimensions:$$y = \\mathcal{F}(x, {W_i}) + W_sx$$We can also use a square matrix $W_s$ in Eqn.(1). But we will show by experiments that the identity mapping is sufficient for addressing the degradation problem and is economical, and thus $W_s$ is only used when matching dimensions.We also note that although the above notations are about fully-connected layers for simplicity, they are applicable to convolutional layers. The function $\\mathcal{F}(x, {W_i})$ can represent multiple convolutional layers. The element-wise addition is performed on two feature maps, channel by channel. Residual NetworkThe identity shortcuts can be directly used when the input and output are of the same dimensions (solid line shortcuts in Fig.3). When the dimensions increase (dotted line shortcuts in Fig.3), we consider two options: (A) The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. This option introduces no extra parameter; (B) The projection shortcut in Eqn.(2) is used to match dimensions (done by $1 \\times 1$ convolutions). For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2. Figure3. Example network architectures for ImageNet. Left: the VGG-19 model. Middle: a plain network with 34-parameter layers. Right: a residual network with 34 parameter layers. The dotted shortcuts increase dimensions. Table 1 shows more details and other variants. ImplementationOur implementation for ImageNet follows the practice in [Imagenet classificationwith deep convolutional neural networks] and [Very deep convolutional networks for large-scale image recognition]. The Image is resized with its shorter side randomly sampled in $[256, 480]$ for scale agumentation. A $224 \\times 224$ crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted. The standard color augmentation in Imagenet classificationwith deep convolutional neural networks is used. We adopt batch normalization (BN) right after each convolution and before activation. We initialize the weights as in Delving deep into rectifiers:Surpassing human-level performance on imagenet classification and train all plain/residual nets from scratch. We use SGD with a mini-batch size of 256. The learning rate starts from 0.1 and is divided by 10 when the error plateaus, and the models are trained from up to $60 \\times 10^4$ iterations. We use a weight decay of 0.0001 and a momentum of 0.9. We do not use dropout, following the practice in Batch normalization: Accelerating deepnetwork training by reducing internal covariate shift. In testing, for comparison studies we adopt the standard 10-crop testing.[Imagenet classificationwith deep convolutional neural networks] For best results, we adopt the fully-convolutional form as in Very deep convolutional networks for large-scale image recognition and Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, and average the scores at multiple scales (images are resized such that the shorter side is in $\\lbrace 224, 256, 384, 480, 640\\rbrace $. ImageNet classificationDeeper Bottleneck ArchitectureNext we describe our deeper nets for ImageNet. Because of concerns on the training time that we can afford, we modify the building block as a bottleneck design. For each residual function $\\mathcal{F}$, we use a stack of 3 layers instead of 2 (Fig. 5). The three layers are $1 \\times 1$, $3 \\times 3$, and $1 \\times 1$ convolutions, where the $1 \\times 1$ layers are responsible for reducing and then increasing (restoring) dimensions, leaving the $3 \\times 3$ layer a bottleneck with smaller input/output dimensions. Fig. 5 shows an example, where both designs have similar time complexity.The parameter-free indentity shortcuts are particularly important for the bottleneck architectures. If the identity CIFAR-10 and AnalysisThe plain/residual architectures follow the form in Fig.3(middle/right). The network inputs are $32 \\times 32$ images, with the per-pixel mean subtracted. The first layer is $3 \\times 3$ convolutions. Then we use a stack of $6n$ layers with $3 \\times 3$ convolutions on the feature maps of sizes $\\lbrace 32, 16, 8\\rbrace $ respectively, with $2n$ layers for each feature map size. The numbers of filters are $\\lbrace 16, 32, 64\\rbrace $ respectively, with $2n$ layers for each feature map size. The subsampling is performed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax.When shortcut connections are used, they are connected to the pairs of $3 \\times 3$ layers(totally $3n$ shortcuts). On this dataset we use identity shortcuts in all cases (i.e., option A), so our residual models have exactly the same depth, width and number of parameters as the plain counterparts.We use a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization in Delving deep into rectifiers: Surpassing human-level performance on imagenet classification and BN in Accelerating deep network training by reducing internal covariate shift but with no dropout. These models are trained with a mini-batch size of 128 on two GPUs. We start with a learning rate of 0.1, divide it by 10 at 32k and 48k iterations, and terminate training at 64k iterations, which is determined on a 45k/5k train/val split. We follow the simple data augmentation in Deeply-supervised nets for training: 4 pixels are padded on each side, and a $32 \\times 32$ crop is randomly sampled from the padded image or its horizontal flip. For testing, we only evaluate the single view of the original $32 \\times 32$ image.We compare $n=\\lbrace 3, 5, 7, 9\\rbrace $, leading to 20, 32, 44, and 56-layer networks.","link":"/blog/2018/03/04/deep-residual-learning-for-image-recognition/"},{"title":"Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction","text":"AAAI 2017, ST-ResNet，网格流量预测，用三个相同结构的残差卷积神经网络对近邻时间、周期、趋势（远期）分别建模。与 RNN 相比，RNN 无法处理序列长度过大的序列。三组件的输出结果进行集成，然后和外部因素集成，得到预测结果。原文地址：Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction Abstract对于交通管理和公共安全来说，预测人流很重要，但这个问题也很有挑战性，因为收到很多复杂的因素影响，如区域内的交通、事件、天气。我们提出了一个基于深度学习的模型 ST-ResNet，对城市内的每个区域的人流的进出一起预测。我们基于时空数据独一的属性，设计了一个端到端的结构。我们使用残差神经网络框架对时间近邻、周期、趋势属性建模。对每个属性，我们设计了残差卷积的一个分支，每个分支对人流的空间属性建模。ST-ResNet 基于数据动态地聚合三个残差神经网络的输出，给不同的分支和区域分配权重。聚合结果还融合了外部因素，像天气或日期。实验在北京和纽约两个数据集上开展。 Introduction对于交通管理和公共安全来说，预测人流很重要（Zheng et al. 2014）。举个例子，2015年新年夜，上海有大量人群涌入一个区域，导致 36 人死亡。2016年六月中旬，数百名 Pokemon Go 玩家冲入纽约中央公园，为了抓一只特别稀有的怪，导致严重的踩踏事故。如果可以预测一个区域的人流，这样的悲剧可以通过应急措施避免，像提前做交通管控，发布预警，疏散人群等。 我们在这篇文章中预测两类人流（Zhang et al. 2016)：如图 1（a）所示，流入和流出。流入是在给定时间段，从其他区域进入到一个区域的交通运载量。流出表示给定时段内，从一个区域向其他区域的交通运载量。两个流量都是区域间的人口流动。了解这个对风险评估和交通管理有很大帮助。流入/流出可以通过行人数量、邻近道路车辆数、公共运输系统的人数、或是所有的都加起来。图 1（b）展示了一个例子。我们可以使用手机信号测量行人数，$r_2$ 的流入和流出分别为 3 和 1。类似地，使用车辆 GPS 轨迹，分别是 0 和 3。 然而，同时预测城市每个区域人口的流入和流出是很有难度的，有 3 个复杂的因素： 空间依赖。区域 $r_2$ 的流入（图1（a））受邻近区域（像 $r_1$）和遥远区域流出的影响。$r_2$ 的流出也受其他区域（$r_3$）流入的影响。$r_2$ 的流入也影响其自身。 时间依赖。一个区域的人流受到近期和远期时间影响。举个例子，早上8点发生的交通拥堵可能会影响到 9 点。此外，早高峰的交通状况可能在接连的几天都是相似的，每 24 小时一次。而且随着冬天的到来，早高峰时间可能越来越晚。温度下降，日初变晚会使人们起床时间变晚。 外部影响。一些像天气和事件的外部因素可能会显著地改变城市内不同区域的人口流动。 为了解决这些问题，我们提出了一个深度深空残差网络 (ST-ResNet) 对每个区域的流入和流出同时预测。我们的贡献有 4 点： ST-ResNet 使用基于卷积的残差神经网络对城市内两个邻近的和遥远的区域的空间依赖建模，同时还确信了模型的预测精度不会因为模型的深度增加而降低。 我们将人口流动的时间属性分为三种，时间近邻、周期、趋势。ST-ResNet 使用三个残差网络对这些属性建模。 ST-ResNet 动态地聚合三个上述网络的输出，给不同的分支和区域分配权重。聚合还融合了外部因素。 我们使用北京出租车的轨迹数据和气象数据，纽约自行车轨迹数据。结果表示我们的方法比 6 个 baseline 都好。 Preliminaries简要回顾人流预测问题（Zhang el al. 2016; Hoang, Zheng, and Singh 2016），介绍残差学习（He et al. 2016）。 Formulation of Crowd Flows ProblemDefinition 1(Region (Zhang et al. 2016)) 根据不同粒度级和语义，一个地点的定义有很多。我们根据经纬度将城市划分成 $I \\times J$ 个网格，一个网格表示一个区域，如图 2(a)。 **Definition 2(Inflow/outflow (Zhang et al. 2016)) $\\mathbb{P}$ 是第 t 时段的轨迹集合。对于第 $i$ 行第 $j$ 列的网格，时段 $t$ 流入和流出的人流分别定义为： $$x^{in,i,j}_t = \\sum_{T_r \\in \\mathbb{P}} \\vert \\lbrace k &gt; 1 \\mid g_{k-1} \\not \\in (i, j) \\wedge g_k \\in (i,j) \\rbrace \\vert\\\\x^{out,i,j}_t = \\sum_{T_r \\in \\mathbb{P}} \\vert \\lbrace k \\geq 1 \\mid g_k \\in (i,j) \\wedge g_{k+1} \\not \\in (i,j) \\rbrace \\vert$$ 其中 $T_r: g_1 \\rightarrow g_2 \\rightarrow \\cdots \\rightarrow g_{\\vert T_r \\vert}$ 是 $\\mathbb{P}$ 中的轨迹，$g_k$ 是地理坐标；$g_k \\in (i,j)$ 表示点 $g_k$ 落在 $(i, j)$ 内；$\\vert · \\vert$ 表示集合基数。 时段 $t$ ，所有区域的流入和流出可以表示成 $\\mathbf{X}_t \\in \\mathbb{R}^{2 \\times I \\times J}$，$(\\mathbf{X}_t)_{0,i,j}=x^{in,i,j}_t, (\\mathbf{X}_t)_{1,i,j} = x^{out,i,j}_t$。流入矩阵如图2(b)。 空间区域可以表达成一个 $I \\times J$ 的区域，有两类流动，所以观测值可以表示为 $\\mathbf{X} \\in \\mathbb{R}^{2 \\times I \\times J}$。 Problem 1 给定历史观测值 $\\lbrace \\mathbf{X}_t \\mid t = 0,\\dots,n-1 \\rbrace$，预测 $\\mathbf{X}_n$。 Deep Residual Learning$$\\tag{1}\\mathbf{X}^{(l+1)} = \\mathbf{X}^{(l)} + \\mathcal{F}(\\mathbf{X}^{(l)})$$ Deep Spatio-Temporal Residual Networks 图 3 展示了 ST-ResNet的架构，4 个部分分别对时间近邻、周期、远期、外部因素建模。如图 3 所示，首先将流入和流出作为两个通道放到矩阵中，使用定义 1 和 2 引入的方法。我们将时间轴分为三个部分，表示近期时间、邻近历史、远期历史。三个时段的两通道的流动矩阵分别输入上述模型，对三种时间属性建模。这三个组件结构相同，都是残差网络。这样的结构捕获邻近和遥远区域间的空间依赖。外部组件中，我们手动的从数据集中提取了特征，如天气、事件等，放入两层全连接神经网络中。前三个组件的输出基于参数矩阵融合为 $\\mathbf{X}_{Res}$，参数矩阵给不同的区域不同的组件分配权重。$\\mathbf{X}_{Res}$ 然后与外部组件 $\\mathbf{X}_{Ext}$ 集成。最后，聚合结果通过 Tanh 映射到 $[-1, 1]$，在反向传播会比 logistic function 收敛的更快 (LeCun et al. 2012)。 Structures of the First Three Components如图 4。 Convolution 一个城市通常很大，包含很多距离不同的区域。直观上来说，邻近区域的人流会影响其他区域，可以通过 CNN 有效地处理，CNN 也被证明在层级地捕获空间信息方面很强 (LeCun et al. 1998)。而且，如果两个遥远地方通过地铁或高速公路连接，那么这两个区域间就有依赖关系。为了捕获任何区域的空间依赖，我们需要设计一个很多层的 CNN 模型，因为一个卷积层只考虑空间近邻，受限于它卷积核的大小。同样的问题在视频序列生成任务中也有，当输入和输出有同样的分辨率的时候(Mathieu, Couprie, and LeCun 2015)。为了避免下采样导致的分辨率损失引入了几种方法，同时还保持遥远的依赖关系(Long, Shelhamer, and Darrell 2015)。与传统的 CNN 不同的是，我们没有使用下采样，而是只使用卷积 (Jain et al. 2007)。如图 4(a)，图中有 3 个多级的 feature map，通过一些卷积操作相连。一个高层次的结点依赖于 9 个中间层次的结点，这些又依赖于低层次的所有结点。这意味着一个卷积可以很自然地捕获空间近邻依赖，堆叠卷积可以更多地捕获遥远的空间依赖。 图 3 的近邻组件使用了一些 2 通道流动矩阵对近邻时间依赖建模。令最近的部分为 $[\\mathbf{X}_{t-l_c}, \\mathbf{X}_{t-(l_c-1)}, \\dots, \\mathbf{X}_{t-1}]$，也称为近邻依赖序列。我们将他们沿第一个轴（时间）拼接，得到一个张量 $\\mathbf{X}^{(0)}_c \\in \\mathbb{R}^{2l_c \\times I \\times J}$，然后使用卷积（图 3 中的 Conv1）： $$\\tag{2}\\mathbf{X}^{(1)}_c = f(W^{(1)}_c \\ast \\mathbf{X}^{(0)}_c + b^{(1)}_c)$$ 其中 $\\ast$ 表示卷积；$f$ 是激活函数；$W^{(1)}_c, b^{(1)}_c$ 是参数。 Residual Unit. 尽管有 ReLU 职业那个的激活函数和正则化技巧，深度卷积网络在训练上还是很难。但我们仍然需要深度神经网络捕获非常大范围的依赖。对于典型的流量数据，假设输入大小是 $32 \\times 32$，卷积核大小是 $3 \\times 3$，如果我们想对城市范围的依赖建模，至少需要连续 15 个卷积层。为了解决这个问题，我们使用残差学习(He et al. 2015)，在训练超过 1000 层的网络时很有效。 在我们的 ST-ResNet(如图 3)，我们在 Conv1 上堆叠 $L$ 个残差单元如下： $$\\tag{3}\\mathbf{X}^{(l+1)}_c = \\mathbf{X}^{(l)}_c + \\mathcal{F}(\\mathbf{X}^{(l)}_c; \\theta^{(l)}_c), l = 1, \\dots, L$$ $\\mathcal{F}$ 是残差函数，即 ReLU + Convolution，如图 4(b)。我们还在 ReLU 之前加了 Batch Normalization。在第 $L$ 个残差单元前，我们使用了一个卷积层，图 3 中的 Conv2。2 个卷积和 $L$ 个残差单元，图 3 中的近邻组件的输出是 $\\mathbf{X}^{(l+2)}_c$。 同样的，使用上面的操作，我们可以构建 周期 和 趋势 组件，如图 3。假设时段 $p$ 有 $l_p$ 个时间间隔。那么 时段 依赖序列是 $[\\mathbf{X}_{t-l_p \\cdot p}, \\mathbf{X}_{t-(l_p - 1) \\cdot p}, \\dots, \\mathbf{X}_{t-p}]$。使用式 2 和 式 3 那样的卷积和 $L$ 个残差单元，周期 组件的输出是 $\\mathbf{X}^{(L + 2)}_p$。同时，趋势 组件的输出是 $\\mathbf{X}^{(L+2)}_q$，输入是 $[\\mathbf{X}_{t-l_q \\cdot q}, \\mathbf{X}_{t-(l_q - 1) \\cdot q}, \\dots, \\mathbf{X}_{t-q}]$，$l_q$ 是趋势依赖序列的长度，$q$ 是趋势跨度。需要注意的是 $p$ 和 $q$ 是两个不同类型的周期。在实际的实现中，$p$ 等于一天，描述的是日周期，$q$ 是一周，表示周级别的趋势。 The Structure of the External Component交通流会被很多复杂的外部因素所影响，如天气或事件。图 5(a) 表示假期（春节）时的人流和平时的人流很不一样。图 5(b) 表示相比上周的同一天，突然而来的大雨会减少此时办公区域的人流。令 $E_t$ 为特征向量，表示预测的时段 $t$ 的外部因素。我们的实现中，我们主要考虑天气、假期事件、元数据（工作日、周末）。详细情况见表 1。为了预测时段 $t$ 的交通流，假期事件和元数据可以直接获得。然而，未来时段 $t$ 的天气预报不知道。可以使用时段 $t$ 的天气预报，或是 $t-1$ 时段的天气来近似。我们在 $E_t$ 上堆叠两个全连接层，第一层可以看作是每个子因素的嵌入层。第二层用来从低维映射到和 $\\mathbf{X}_t$ 一样的高维上。图 3 中外部组件的输出表示为 $\\mathbf{X}_{Ext}$，参数是$\\theta_{Ext}$。 Fusion我们先用一个参数矩阵融合前三个组件，然后融合外部组件。 图6(a)和(d)展示了表1展示的北京轨迹数据的比例曲线，x轴是两个时段的时间差，y轴是任意两个有相同时间差的流入的平均比例。两个不同区域的曲线在时间序列上表现出了时间联系，也就是近期的流入比远期的流入更相关，表现出了事件近邻性。两条曲线有两个不同的形状，表现出不同区域可能有不同性质的近邻性。图6(b)和(e)描绘了7天所有时段的流入。我们可以观察到两个区域明显的日周期性。在办公区域，工作日的峰值比周末的高很多。住宅区在工作日和周末有相似的峰值。图6(c)和(f)描述了2015年3月到2015年6月一个特定时段(9:00pm-9:30pm)的流入。随着时间的推移，办公区域的流入逐渐减少，住宅区逐渐增加。不同的区域表现出了不同的趋势。总的来说，两个区域的流入受到近邻、周期、趋势三部分影响，但是影响程度是不同的。我们也发现其他区域也有同样的性质。 综上，不同区域受近邻、周期、趋势的影响，但是影响程度不同。受这些观察的启发，我们提出了一个基于矩阵参数的融合方法。 Parametric-matrix-based fusion. 我们融合图 3 中前三个组件： $$\\tag{4}\\mathbf{X}_{Res} = \\mathbf{W}_c \\odot \\mathbf{X}^{(L+2)}_c + \\mathbf{W}_p \\odot \\mathbf{X}^{(L+2)}_p + \\mathbf{W}_q \\odot \\mathbf{X}^{(L+2)}_q$$ $\\odot$ 是哈达玛乘积，$\\mathbf{W}$ 是参数，分别调整三个组件的影响程度。 Fusing the external component. 我们直接地将前三个组件的输出和外部组件融合，如图3。最后，时段 $t$ 的预测值，表示为 $\\hat{\\mathbf{X}}_t$ 定义为： $$\\tag{5}\\hat{\\mathbf{X}}_t = \\mathrm{tanh}(\\mathbf{X}_{Res} + \\mathbf{X}_{Ext})$$ 我们的 ST-ResNet 可以从三个流动与朕和外部因素特征通过最下滑 MSE 来训练： $$\\tag{6}\\mathcal{L}(\\theta) = \\Vert \\mathbf{X}_t - \\hat{\\mathbf{X}}_t \\Vert^2_2$$ Algorithm and Optimization算法1描述了 ST-ResNet 的训练过程。首先从原始序列构造训练实例。然后通过反向传播，用 Adam 算法训练。 ExperimentsSettingsDatasets. 我们使用表 1 中展示的两个数据集。每个数据集都包含两个子集，轨迹和天气。 TaxiBJ: 轨迹数据是出租车 GPS 数据和北京的气象数据，2013年7月1日到10月30日，2014年5月1日到6月30日，2015年5月1日到6月30日，2015年11月1日到2016年4月1日。使用定义2，我们获得两类人流。我们选择最后四周作为测试集，之前的都为训练集。 BikeNYC: 轨迹数据是2014年NYC Bike系统中取的，从4月1日到9月30日。旅行数据包含：持续时间、起点终点站点ID，起始终止时间。在数据中，最后10天选做测试集，其他选做训练集。 Baselines. 我们对比了6个baselines：HA, ARIMA, SARIMA, VAR, ST-ANN, DeepST.","link":"/blog/2019/03/08/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/"},{"title":"Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning","text":"AAAI 2018。这篇论文很有趣，讲的是 GCN 堆得过多了之后，效果会变差的问题。作者分析了一下为什么会变差，主要是因为 GCN 的本质实际上是对每个结点的邻居特征和自身特征做线性组合，权重和邻接矩阵相关，所以对于顶点分类问题来说，如果堆得层数多了，就会让一个结点的特征聚合越来越多邻居的特征，让大家都变得相似，从而使得类间的相似度增大，自然分类效果就差了。作者提出了两个方法解决这个问题，算训练上的 trick 吧。原文链接：Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning Abstract机器学习中很多有趣的问题正在用深度学习工具来重新审视。对于基于图的半监督学习问题，最新的一个重要进展就是图卷积神经网络 (GCNs)，这个模型可以很好的将顶点局部特征和图的拓扑结构整合进卷积层内。尽管 GCN 模型和其他的 state-of-the-art 方法相比效果更好，但是它的机理目前还不是很清楚，而且需要很多的标记数据用于验证以及模型选择。 在这篇论文中，我们深入了 GCN 模型，解决了它的底层限制。首先，我们发现 GCN 模型的图卷积实际上是一个拉普拉斯平滑的特殊形式，这是 GCN 工作的关键原因，但是这也会给很多卷积层带来潜在的危害。其次，为了克服 GCN 层数少的限制，我们提出了协同训练和自训练方法来训练 GCNs。我们的方法显著地提升了 GCNs 在标记样本少的情况下的学习，并且让他们避免了使用额外的标记用来验证。大量的实验证明了我们的理论和方案。 1 Introduction深度学习中的突破使得人工智能和机器学习中正在发生范式变化。一方面，很多老问题通过深度神经网络重新审视，很多原来看起来在任务中无法完成的巨大进步现在也在发生着，如机器翻译和计算机视觉。另一方面，像几何深度学习 (Bronstein et al. 2017) 这样的技术正在发展，可能会将深度神经模型泛化到新的或非传统的领域。 众所周知，深度学习模型一般需要大量的标记数据，在很多标记训练数据代价很大的场景就无法满足这样的要求。为了减少用于训练的数据的数量，最近的研究开始关注 few-shot learning (Lake, Salakhutdinov, and Tenenbaum 2015; Rezende et al. 2016)——从每个类只有很少的样本中学习一个分类模型。和 few-shot learning 相近的是半监督学习，其中有大量的未标记样本可以用来和很少量的标记样本一起用于训练。 很多研究者已经证实了如果使用恰当，在训练中利用未标记样本可以显著地提升学习的精度 (Zhu and Goldberg 2009)。关键问题是最大化未标记样本的结构和特征信息的有效利用。由于强力的特征抽取能力和深度学习近些年的成功案例，已经有很多人使用基于神经网络的方法处理半监督学习，包括 ladder network (Rasmus et al. 2015), 半监督嵌入 (Weston et al. 2008)，planetoid (Yang, Cohen, and Salakhutdinov 2016)，图卷积网络 (Kipf and Welling 2017)。 最近发展的图卷积神经网络 (GCNNs) (Defferrard, Bresson, and Vandergheynst 2016) 是一个将欧氏空间中使用的卷积神经网络 (CNNs) 泛化到对图结构数据建模的成功尝试。在他们的初期工作 (Kipf and Welling 2017)，Kifp and Welling 提出了一个 GCNNs 的简化类型，称为图卷积网络 (GCNs)，应用于半监督分类。GCN 模型很自然地将图结构数据的连接模式和特征属性集成起来，而且比很多 state-of-the-art 方法在 benchmarks 上好很多。尽管如此，它也有很多其他基于神经网络的模型遇到的问题。用于半监督学习的 GCN 模型的工作机理还不清楚，而且训练 GCNs 仍然需要大量的标记样本用于调参和模型选择，这就和半监督学习的理念相违背。 在这篇论文中，我们弄清楚了用于半监督学习的 GCN 模型。特别地，我们发现 GCN 模型中的图卷积是拉普拉斯平滑的一种特殊形式，这个平滑可以混合一个顶点和它周围顶点的特征。这个平滑操作使得同一类簇内顶点的特征相似，因此使分类任务变得简单，这使为什么 GCNs 表现的这么好的关键原因。然而，这也会带来 over-smoothing 的问题。如果 GCN 有很多卷积层后变深了，那么输出的特征可能会变得过度平滑，且来自不同类簇的顶点可能变得无法区分。这种混合在小的数据集，且只有很少的卷积层上发生的很快，就像图2展示的那样。而且，给 GCN 模型增加更多的层也会使它变得难以训练。 然而，一个浅层的 GCN 模型，像 Kipf &amp; Welling 2017 使用的两层 GCN 有它自身的限制。除此以外它还需要很多额外的标记用来验证，它也会遇到卷积核局部性等问题。当只有少数标记的时候，一个浅层的 GCN 模型不能有效的将标记传播到整个图上。如图1所示，GCNs 的表现会随着训练集的减少急速下降，甚至有500个额外标记用来验证。 为了克服限制并理解 GCN 模型的全部潜能，我们提出了一种协同训练方法和一个自训练方法来训练 GCNs。通过使用随机游走模型来协同训练一个 GCN，随机游走模型可以补充 GCN 模型在获取整个图拓扑结构上的能力。通过自训练一个 GCN，我们可以挖掘它的特征提取能力来克服它的局部特性。融合协同训练和自训练方法可以从本质上提升 GCN 模型在半监督学习上只有少量标记的效果，而且使它不用使用额外的标记样本用来验证。如图1所示，我们的方法比 GCNs 好了一大截。 总而言之，这篇论文的关键创新有：1) 对半监督学习的 GCN 模型提供了新的视角和新的分析；2) 提出了对半监督学习的 GCN 模型提升的解决方案。 2 Preliminaries and Related Works首先，我们定义一些符号。图表示为 $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$，其中 $\\mathcal{V}$ 是顶点集，$\\vert \\mathcal{V} \\vert = n$，$\\mathcal{E}$ 是边集。在这篇论文中，我们考虑的是无向图。$A = [a_{ij}] \\in \\mathbb{R}^{n \\times n}$ 是邻接矩阵，且为非负的。$D = \\mathrm{diag}(d_1, d_2, …, d_n)$ 表示度矩阵，$d_i = \\sum_j a_{ij}$ 是顶点 $i$ 的度。图拉普拉斯矩阵 (Chung 1997) 定义为 $L := D - A$，归一化的图拉普拉斯矩阵的两个版本分别定义为：$L_{sym} := D^{-\\frac{1}{2}} L D^{-\\frac{1}{2}}$ 和 $L_{rw} := D^{-1}L$。 Graph-Based Semi-Supervised Learning 这篇论文中我们考虑的问题是图上的半监督分类任务。给定一个图 $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E}, X)$，其中 $X = \\mathrm{[x_1, x_2, …, x_n]^T} \\in R^{n \\times c}$ 是特征矩阵，$\\mathrm{x}_i \\in R^c$ 是顶点 $i$ 的 $c$ 维特征向量。假设给定了一组顶点 $\\mathcal{V}_l$ 的标记，目标是预测其余顶点 $\\mathcal{V}_u$ 的标记。 基于图的半监督学习在过去的二十年成为了一个流行的研究领域。通过挖掘图或数据的流形结构，是可以通过少量标记进行学习的。很多基于图的半监督学习方法形成了类簇假设 (cluster assumption) (Chapelle and Zien 2005)，假设了一个图上临近的顶点倾向于有共同的标记。顺着这条路线的研究包括 min-cuts (Blum and Chawla 2001) 和 randomized min-cuts (Blum et al. 2004)，spectral graph transducer (Joachims 2003)，label propagation (Zhu, Ghahramani, and Lafferty 2003) and its variants (Zhou et al. 2004; Bengio, Delalleau, and Le Roux 2006)，modified adsorption (Talukdar and Crammer 2009)，还有 iterative classification algorithm (Sen et al. 2008)。 但是图只表示数据的结构信息。在很多应用，数据的样本是以包含信息的特征向量表示，而不是在图中表现。比如，在引文网络中，文档之间的引用链接描述了引用关系，但是文档是由 bag-of-words 向量表示的，这些向量描述的内容是文档的内容。很多半监督学习方法寻求对图结构和数据的特征属性共同建模。一个常见的想法是使用一些正则项对一个监督的学习器进行正则化。比如，manifold regularization (LapSVM) (Belkin, Niyogi, and Sindhwani 2006) 使用一个拉普拉斯正则项对 SVM 进行正则化。深度半监督嵌入 (Weston et al. 2008) 使用一个基于嵌入的正则项对深度神经网络进行正则化。Planetoid (Yang, Cohen, and Salakhutdinov 2016) 也通过共同地对类标记和样本的上下文预测对神经网络进行正则化。 Graph Convolutional Networks 图卷积神经网络 (GCNNs) 将传统的卷积神经网络泛化到图域中。主要有两类 GCNNs (Bronstein et al. 2017): spatial GCNNs 和 spectral GCNNs。空间 GCNNs 将卷积看作是 “patch operator”，对每个顶点使用它的邻居信息构建新的特征向量。谱 GCNNs 通过对图信号 $\\bf{s} \\in \\mathcal{R}^n$ 在谱域上进行分解，然后使用一个在谱成分上的谱卷积核 $g_\\theta$ (是 $L_{sym}$ 的特征值的一个函数) (Bruna et al. 2014; Sandryhaila and Moura 2013; Shuman et al. 2013)。然而这个模型需要计算出拉普拉斯矩阵的特征向量，这对于大尺度的图来说是不太实际的。一种缓解这个问题的方法是通过将谱卷积核 $g_\\theta$ 通过切比雪夫多项式趋近到 $K^{th}$ 阶 (Hammond, Vandergheynst, and Gribonval 2011)。在 (Defferrard, Bresson, and Vandergheynst 2016)，Defferrard et al. 使用这个构建了 $K$ 阶 ChebNet，卷积定义为： $$\\tag{1}g_\\theta \\star \\mathbf{s} \\approx \\sum^K_{k=0} \\theta’_k T_k (L_{sym}) \\mathbf{s},$$ 其中 $\\bf{s} \\in \\mathcal{R}^n$ 是图上的信号，$g_\\theta$是谱滤波器，$\\star$ 是卷积操作，$T_k$ 是切比雪夫多项式，$\\theta’ \\in \\mathcal{R}^K$ 是切比雪夫系数向量。通过这种趋近，ChebNet 域谱无关。 在 (Kipf and Welling 2017) 中，Kipf and Welling 将上面的模型通过让 $K = 1$ 进行了简化，将 $L_{sym}$ 的最大特征值趋近为2.在这种形式中，卷积变成： $$\\tag{2}g_\\theta \\star \\mathbf{s} = \\theta(I + D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}) \\mathbf{s},$$ 其中 $\\theta$ 是切比雪夫系数。然后对卷积矩阵使用一种正则化的技巧： $$\\tag{3}I + D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} \\rightarrow \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}},$$ 其中 $\\tilde{A} = A + I$，$\\tilde{D} = \\sum_j \\tilde{A}_{ij}$. 将卷积泛化到带有 $c$ 个通道的图信号上，也就是 $X \\in \\mathcal{R}^{n \\times c}$ (每个顶点是一个 $c$ 维特征向量)，使用 $f$ 谱卷积核，简化后的模型的传播规则是： $$\\tag{4}H^{(l + 1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{(l)} \\Theta^{(l)}),$$ 其中，$H^{(l)}$ 是第 $l$ 层的激活值矩阵，$H^{(0)} = X$，$\\Theta^{(l)} \\in \\mathcal{R}^{c \\times f}$ 是第 $l$ 层可训练的权重矩阵，$\\sigma$ 是激活函数，比如 $ReLU(\\cdot) = max(0, \\cdot)$。 这个简化的模型称为图卷积网络 (GCNs)，是我们这篇论文关注的重点。 Semi-Supervised Classification with GCNs 在 Kipf and Welling 2017 中，GCN 模型以一种优雅的方式做半监督分类任务。模型是一个两层 GCN，在输出时使用一个 softmax： $$\\tag{5}Z = \\mathrm{softmax}(\\hat{A} ReLU (\\hat{A} X \\Theta^{(0)}) \\Theta^{(1)} ),$$ 其中 $\\hat{A} = \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}$，$\\mathrm{softmax}(x_i) = \\frac{1}{\\mathcal{Z}} exp(x_i)$，$\\mathcal{Z} = \\sum_i exp(x_i)$。损失函数是所有标记样本上的交叉熵： $$\\tag{6}\\mathcal{L} := - \\sum_{i \\in \\mathcal{V}_l} \\sum^F_{f=1} Y_{if} \\mathrm{ln} Z_{if},$$ 其中 $\\mathcal{V}_l$ 是标记顶点的下标，$F$ 是输出特征的维数，等价于类别数。$Y \\in \\mathcal{R}^{\\vert \\mathcal{V}_l \\vert \\times F}$ 是标记矩阵。权重参数 $\\Theta^{(0)}$ 和 $\\Theta^{(1)}$ 可以通过梯度下降训练。 GCN 模型在卷积中自然地融合了图的结构和顶点的特征，未标记的顶点的特征和临近的标记顶点的混合在一起，然后通过多个层在网络上传播。GCNs 在 Kipf &amp; Welling 2017 中比很多 state-of-the-art 方法都好很多，比如在引文网络上。 3 Analysis尽管它的性能很好，但是用于半监督学习的 GCN 模型的机理还没有弄明白。在这部分我们会走近 GCN 模型，分析它为什么好使，并指出它的限制。 Why GCNs Work 我们将 GCN 和最简单的全连接神经网络 (FCN) 进行比较，传播规则是： $$\\tag{7}H^{(l + 1)} = \\sigma(H^{(l)} \\Theta^{(l)}).$$ GCN 和 FCN 之间的唯一一个区别是图卷积矩阵 $\\hat{A} = \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}$ (式5)用在特征矩阵 $X$ 的左边。我们在 Cora 数据集上，每类 20 个 标签，做了半监督分类的测试。如表1所示。即便是只有一层的 GCN 也比一层的 FCN 好很多。 Laplacian Smoothing. 考虑一个一层的 GCN。实际有两步： 从矩阵 $X$ 通过一个图卷积得到新的特征矩阵 $Y$： $$\\tag{8}Y = \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}X.$$ 将新的特征矩阵 $Y$ 放到一个全连接层。很明显，图卷积是性能提升的关键。 我们来自己的检查一下图卷积。假设我们给图中的每个结点增加一个自连接，新的图的邻接矩阵就是 $\\tilde{A} = A + I$。输入特征的每个通道的拉普拉斯平滑 (Taubin 1995) 定义为： $$\\tag{9}\\hat{\\mathrm{y}}_i = (1 - \\gamma) \\mathrm{x}_i + \\gamma \\sum_j \\frac{\\tilde{a}_{ij}}{d_i} \\mathrm{x}_j \\quad (\\text{for} \\quad 1 \\leq i \\leq n),$$ 其中 $0 &lt; \\gamma &lt; 1$ 是控制当前结点的特征和它的邻居的特征之间的权重。我们可以将拉普拉斯平滑写成矩阵形式： $$\\tag{10}\\hat{Y} = X - \\gamma \\tilde{D}^{-1} \\tilde{L} X = (I - \\gamma \\tilde{D}^{-1} \\tilde{L})X,$$ 其中 $\\tilde{L} = \\tilde{D} - \\tilde{A}$。通过设定 $\\gamma = 1$，也就是只使用邻居的特征，可得 $\\hat{Y} = \\tilde{D}^{-1} \\tilde{A} X$，也就是拉普拉斯平滑的标准形式。 现在如果我们把归一化的拉普拉斯矩阵 $\\tilde{D}^{-1} \\tilde{L}$ 替换成对阵的归一化拉普拉斯矩阵 $\\tilde{D}^{-\\frac{1}{2}} \\tilde{L} \\tilde{D}^{-\\frac{1}{2}}$，让 $\\gamma = 1$，可得 $\\hat{Y} = \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} X$，这恰好就是式8中的图卷积。我们因此称图卷积是一种特殊形式的拉普拉斯平滑——对称拉普拉斯平滑。注意，平滑仍然会包含顶点特征，因为每个顶点有一个自连接，还有它自己的邻居。 拉普拉斯平滑计算了顶点的新的特征，也就是顶点自身和邻居的加权平均。因为同一类簇的顶点倾向于连接的更紧密，这使得分类任务变得更简单。因为我们可以从表1看出只使用一次平滑就很有效了。 Multi-layer Structure. 我们可以从表1看出尽管两层的 FCN 比 一层的 FCN 有了些许的提升，两层的 GCN 却比 一层的 GCN 好了很多。这是因为在第一层的激活值上再使用平滑使得同一个类簇中的顶点特征变得更像四了，使分类任务更简单。 When GCNs Fail 我们已经证明了图卷积本质上就是一种拉普拉斯平滑。那 GCN 中应该放多少层呢？当然不是越多越好。GCN 层多了会不好训练。而且重复使用拉普拉斯平滑可能会混合不同类簇中的顶点的特征，使得他们区分不清。我们来举个例子。 我们在 Zachary 的 karate club dataset (Zachary 1977) 上跑几个层数不同的模型。这个数据集有 34 个结点，两类，78 条边。GCN 的参数像 (Glorot and Bengio 2010) 中的一样随机初始化。隐藏层的维数是 16，输出层的维度是2。每个结点的特征向量是一个 one-hot 向量。每个 GCN 的输出绘制在图2中。我们可以看到图卷积的影响（图2a）。使用两次平滑，分类效果相对较好。再次使用平滑，点就会混合（图2c，2d，2e）。因为这是个小的数据集，两类之间的顶点有很多连接，所以很快就发生了混合。 接下来，我们会证明重复使用拉普拉斯平滑，顶点的特征以及图的每个连通分量会收敛到相同的值。对于对称的拉普拉斯平滑，他们收敛到的值与顶点度数的二分之一次幂成正比。 假设图 $\\mathcal{G}$ 有 $k$ 个连通分量 $\\lbrace C_i\\rbrace ^k_{i=1}$，对于第 $i$ 个连通分量的指示向量表示为 $\\mathbf{1}^{(i)} \\in \\mathbb{R}^n$。这个向量表示顶点是否在分量 $C_i$中，即： $$\\tag{11}\\mathbf{1}^{(i)}_j = \\begin{cases}1, v_j \\in C_i,\\\\0, v_j \\notin C_i\\end{cases}$$ Theorem 1. 如果一个图没有二分的连通分量，那么对于任意 $\\mathrm{w} \\in \\mathbb{R}^n$，$\\alpha \\in (0, 1]$， $$\\lim_{m \\rightarrow + \\infty} (I - \\alpha L_{rw})^m \\mathrm{w} = [\\mathbf{1}^{(1)}, \\mathbf{1}^{(2)}, …, \\mathbf{1}^{(k)}]\\theta_1,$$ $$\\lim_{m \\rightarrow + \\infty} (I - \\alpha L_{sym})^m \\mathrm{w} = D^{-\\frac{1}{2}}[\\mathbf{1}^{(1)}, \\mathbf{1}^{(2)}, …, \\mathbf{1}^{(k)}]\\theta_2,$$ 其中 $\\theta_1 \\in \\mathbb{R}^k, \\theta_2 \\in \\mathbb{R}^k$，也就是他们分别收敛到 $\\lbrace \\mathbf{1}^{(i)}\\rbrace ^k_{i=1}$ 和 $\\lbrace D^{-\\frac{1}{2}} \\mathbf{1}^{(i)} \\rbrace ^k_{i=1}$。 Proof. $L_{rw}$ 和 $L_{sym}$ 有相同的 $n$ 个特征值，不同的特征向量 (Von Luxbury 2007)。如果一个图没有二分的连通分量，那么特征值就会在 $[0, 2)$ 区间内 (Chung 1997)。后面就没看懂了。。。 4. SolutionsCo-Train a GCN with a Random Walk Model 使用一个 partially absorbing random walks (Wu et al. 2012) 来捕获网络的全局结构。方法就是计算归一化的吸收概率矩阵 $P = (L + \\alpha \\Lambda)^{-1}$，$P_{i, j}$ 是从顶点 $i$ 出发被吸收到顶点 $j$ 的概率，表示 $i$ 和 $j$ 有多大的可能性属于同一类。然后我们对每类 $k$，计算可信向量 $\\mathbf{p} = \\sum_{j \\in S_k} P_{:, j}$，其中 $\\mathbf{p} \\in \\mathbb{R}^n$，$p_i$ 是顶点 $i$ 属于类 $k$ 的概率。最后，找到 $t$ 个最可信的顶点把他们加到训练集的类 $k$ 中。 GCN Self-Training另一种方法就是先训练一个 GCN，然后使用这个 GCN 去预测，根据预测结果的 $\\text{softmax}$ 分数选择可信的样本，加入到训练集中。","link":"/blog/2018/10/31/deeper-insights-into-graph-convolutional-networks-for-semi-supervised-learning/"},{"title":"DeepSTN+: Context-aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis","text":"AAAI 2019，网格流量预测，对比ST-ResNet，抛出三个问题，卷积捕获的空间范围小、人口流动和区域的功能相关、之前的融合机制不好。改了一下残差卷积，给 POI 信息增加了时间维度，多组件的信息提前融合，减少了参数，稳定模型训练。原文链接：DeepSTN+: Context-aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis Abstract人口流量预测在城市规划、交通管控中的很多应用中都很重要。目的是预测流入和流出流量。我们提出了 DeepSTN+，一个基于深度学习的卷积模型，预测超大城市的人口流量。首先，DeepSTN+ 使用 ConvPlus 结构对大范围的空间依赖建模。此外，POI 分布和时间因素相融合来表达区域属性的影响，以此引入人口流动的先验知识。最后，我们提出了一个有效的融合机制来稳定训练过程，提升了结果。基于两个真实数据集的大量实验结果表明我们模型的先进性，和 state-of-the-art 比高了 8% ~ 13% 左右。 Introduction如图 1 所示，人口流量预测是在给定历史流量信息的前提下，预测城市内每个区域的流入和流出流量。最近，为了解决这个问题，基于深度学习的模型被相继提出，获得了很好的效果。Deep-ST 是第一个使用卷积网络捕获空间信息的模型。ST-ResNet 用卷积模块替换了卷积。通过融合金字塔型的 ConvGRU 模型和周期表示，Periodic-CRN 设计成了捕获人口流动周期性的模型。 这些方法仍然不够有效且不精确： 1) 不能捕获区域间的空间依赖。 由于现代城市中高级的运输系统的存在，人们可以通过地铁或出租车在短时间内移动到很远的地方。因此，区域间的大范围空间依赖在人口移动中逐渐扮演重要的角色。现存的工作使用多层卷积网络来建模。然而，它们只能一步一步地捕获近邻的空间依赖，不能直接地捕获大范围的空间依赖。2) 忽略了人口流动的区域功能的影响。 人口移动是发生在物理世界中的，会直接受到区域属性的影响。举个例子，人们通常早上从家出发到公司，晚上回来。显然，区域的功能（属性）包含了关于人类移动的先验知识。然而，现存的解决方案没有考虑过区域的属性。3) 冗余以及不稳定的神经网络结构。 ST-ResNet 利用了三个独立分支，每个分支都是残差卷积单元，用来处理不同的输入，在模型的结尾用一个线性操作融合三个输出。但是，最后的融合机制导致不同组件间的交互产生了缺陷，这个缺陷导致了网络内产生了无效的参数和不稳定的性质。 总结一下，模型应该考虑大范围的空间依赖，区域的影响，更有效的融合机制这三点因素。我们提出的 DeepSTN+ 解决了上述挑战。我们设计了一个 ConvPlus 结构直接地捕获大范围空间依赖。ConvPlus 放在残差单元前面作为一个全局特征提取器提取出区域间的全局特征。其次，我们设计了一个 SemanticPlus 结构来学习人口在区域间移动的先验知识。用静态的 POI 分布作为输入，SemanticPlus 利用时间因素给不同时间上不同的 POI 分配权重。最后，我们引入早融合和多尺度融合机制来减少训练参数，捕获不同级别特征间的复杂关系。这样，我们的系统可以对更复杂的空间关联性建模，获得更好的效果，我们的贡献有以下几点： 我们设计了一个新的残差单元，ResPlus 单元用来替换原始的残差单元。我们指出了典型的卷积模型不能有效地捕获大范围依赖。ResPlus 包含了一个 ConvPlus 结构，可以捕获人流间的大范围空间依赖。 我们设计了一个 SemanticPlus 结构来建模不同区域的影响，学习人口流动的先验知识。我们在模型头部使用早融合机制，在结尾使用多尺度融合机制，提升了模型的精度和稳定性。 我们在两个数据集上开展了大量的实验，对比了 5 个 baselines，结果显示我们的模型在预测人口流动的错误上减少了 8% ~ 13%。 Preliminaries这部分，我们首先介绍人口流量预测问题，简要回顾 ST-ResNet。 Problem Formulation**Definition 1 (Region (Zhang et al. 2016)) 为了表示城市的区域，我们基于经纬度将城市划分成 $H \\times W$ 个区域，所有的网格有相同大小且表示一个区域。 **Definition 2 (Inflow/outflow (Zhang et al. 2016)) 为了表示城市内的人口流动，我们定义了区域 $(h, w)$ 在时段 $i$ 的流入和流出流量： $$x^{h,w,in}_{i} = \\sum_{T_{r_k} \\in \\mathbb{P}} \\vert \\lbrace j &gt; 1 \\mid g_{j-1} \\not \\in (h, w) \\And g_j \\in (h, w) \\rbrace \\vert,\\\\ x^{h,w,out}_{i} = \\sum_{T_{r_k} \\in \\mathbb{P}} \\vert \\lbrace j \\geq 1 \\mid g_{j-1} \\in (h, w) \\And g_j \\not \\in (h, w) \\rbrace \\vert.$$ 这里 $\\mathbb{P}$ 表示时段 $i$ 的轨迹集合。$T_r: g_1 \\rightarrow g_2 \\rightarrow \\cdots \\rightarrow g_{\\vert T_r \\vert}$ 是 $\\mathbb{P}$ 中的一条轨迹，$g_j$ 是坐标；$g_j \\in (h, w)$ 表示点 $g_j$ 在网格 $(h, w)$ 内，反之亦然；$\\vert \\cdot \\vert$ 表示集合的基数。 Crowd Flow Prediction: 给定历史观测值 $\\lbrace \\mathbf{X}_i \\mid i=1,2,\\cdots, n-1 \\rbrace$，预测 $\\mathbf{X}_n$。 ST-ResNet 包含四个组件，closeness, period, trend 和 外部因素单元。每个组成部分通过一个分支的残差单元或全连接层预测出一个流量地图。然后模型使用一个线性组合作为末端融合方式融合这些预测值。ST-ResNet 的外部因素包含了天气、假期事件、元数据。 卷积神经网络的卷积核通常很小，意味着他们不能直接捕获远距离的空间依赖。然而，大范围的空间依赖在城市中很重要。另一方面，ST-ResNet 忽略了人口流动的在位置上的影响。此外，ST-ResNet 的末端融合机制导致了模型交互上的缺点以及参数的低效，还有模型的不稳定的问题。 Our Model图 2 展示了我们模型的框架。主要有三个部分：流量输入、SemanticPlus 和 ResPlus 单元。流量慎入包含 closeness, period, terend，由于数据的时间范围限制可以减少为 closeness, period。SemanticPlus 包含 POI 分布和时间信息。ResPlus 单元可以捕获远距离空间依赖。每个区域的流入和流出流量通过每小时或者每半小时统计得到流量地图的时间序列。这些流量地图通过 Min-Max 归一化处理到 $[-1, 1]$。如图 2 所示，人口分布地图通过近期时间、近邻历史、远期历史选择后作为输入放入模型。不同类型的 POI 分布通过 Min-Max 归一化到 $[0, 1]$。如图 2 做部分所示，POI 分布地图通过时间信息赋予了不同的权重。之后，POI 信息和人流信息通过早融合后放入堆叠的 ResPlus 单元中。最后，ResPlus 单元不同级别的特征融合后进入卷积部分，然后通过 Tanh 映射到 $[-1, 1]$。下面会介绍细节。 ResPlus很多处理人口流量预测的深度学习模型主要包含两个部分：基于 RNN 的结构，像 ConvLSTM 和 Periodic-CRN，以及基于 CNN 的结构，如 Deep-ST 和 ST-ResNet。但是，训练基于 RNN 结构的模型费时。因此我们选用基于 CNN 的结构 ST-ResNet 作为我们的基础模型。 在这篇论文中，我们设计 ConvPlus 来捕获城市内远距离的空间依赖。如图 3，ResPlus 单元使用一个 ConvPlus 和一个典型卷积。我们尝试了 Batch Normalization 和 Dropout，为了简介没有在图里面画出来。 典型卷积的每个通道对应一个卷积核。卷积核使用这些核来计算地图上的互相关系数，比如捕获梯度上的特征。卷积核的大小一般很小。在 ST-ResNet 和 DeepSTN+ 里面，卷积核的大小是 $3 \\times 3$。但是城市中存在着远距离的依赖。人们可能坐地铁去上班。我们称这类关系叫远距离空间依赖关系。这种关系使得堆叠卷积难以有效地捕获这个关系。 如图 3 左部分所示，在 ConvPlus 结构中，我们将典型卷积的一些通道分离来捕获每个区域的远距离空间依赖。然后用一个全连接层直接捕获每两个区域之间的远距离空间依赖，在这层前面用一个池化层来减少参数。因此，在 ConvPlus 的输出有两类通道。ConvPlus 的输出有着和普通卷积一样的输出，可以用于下一个卷积的输入。 图 4 展示了两个不同区域的空间依赖热力图，分别是红色和黄色的星。这些目标区域不仅有区域上的依赖，还有一些和远处区域的远距离依赖。这也显示出不同的区域和地图上的其他区域有不一样的关系，这很难通过堆叠卷积有效地捕获。 因为 ConvPlus 有两类不同的输出通道，我们在 ResPlus 单元中使用 ConvPlus + Conv 而不是 ConvPlus + ConvPlus。没有 SemanticPlus 的 DeepSTN+ 形式化为： $$\\widehat{\\mathbf{X}} = f_{Res}(f_{EF}(\\mathbf{X}^c + \\mathbf{X}^p + \\mathbf{X}^t)),$$ 三个 $\\mathbf{X}$ 表示三种类型的历史地图——closeness, period, trend。$\\widehat{\\mathbf{X}}$ 表示预测出的流量地图。$+$ 表示拼接操作。$f_{EF}$ 表示用来早融合不同类型信息的卷积函数，$f_{Res}$ 表示一个堆叠的 ResPlus 单元。 SemanticPlusPOI 在人口流动上有很强烈的影响，这些影响随时间变化而变化。因此，我们继承这个先验知识到模型内。我们手机了包括类型、数量、位置的 POI 信息。然后统计每个网格内 POI 的数量，使用一个一维向量表示每种 POI 的分布。图 5 展示了北京的流量分布地图和餐饮分布地图。它们的分布很相似，并且互相关系数有 0.87，暗示了它们之间的潜在关系。 我们使用一个时间向量来表示每个人口流量地图的时间。时间向量包含两个部分：一个 one-hot 向量表示一天中的各个时间，如果时段按小时走，那长度就是 24；另一个 one-hot 向量表示是一周中的哪天，长度是 7。一个时间向量拼接了这两个向量。 为了建模对流量地图有变化的时间影响的 POI 信息，我们将时间向量转换为 POI 的影响强度。我们使用大小为 $PN \\times H \\times W$ 的 $\\mathbf{X}^s$ 来表示 POI 地图（$PN$ 表示 POI 的类数，$H$ 和 $W$ 是网格的行数和列数，一个向量 $\\bf{I}$ 用来表示时间向量，大小为 $PN$ 的向量 $\\bf{R}$ 表示 POI 的影响强度。因此，我们有带有时间权重的 POI 分布，形式化如下： $$\\mathbf{S} = \\mathbf{X}^s \\ast \\mathbf{R} = \\mathbf{X}^s \\ast f_t(\\mathbf{I})$$ 函数 $f_t()$ 将时间向量转换为表示 POI 影响强度的向量。$\\ast$ 表示每个 POI 分布地图会被附上一个权重，表示 POI 的影响强度。我们假设同一类在不同的区域的 POI 有相同的时间模式。因此，一个类别的 POI 分布地图会有相同的权重。图 6 展示了娱乐和居住区的影响强度。影响强度在一周内随时间的变化而变化，每天存在着一些典型的模式。很多人早上去上班，工作结束后回家，所以每天早上和下午住宅区有明显的两个峰。对比居住区，娱乐区的影响相对稳定。 Fusion三组件应该用更复杂的融合方式，而不是线性组合。这些带有 POI 信息的流量信息也有复杂的交互。为了建模这种相互影响，我们使用早融合而不是末端融合使得不同的信息能更早的融合起来。早融合减少了大约三分之二的参数。此外，ST-ResNet 有些时候不能收敛。我们发现这个问题可以通过早融合减少参数来简化模型解决。考虑到不同层的特征有不同的函数，我们在模型末端设定了一个多尺度的融合机制。这里我们形式化描述整个网络： $$\\widehat{\\mathbf{X}} = f_{con}(f_{Res}(f_{EF}(\\mathbf{X}^c + \\mathbf{X}^p + \\mathbf{X}^t + \\mathbf{S}))),$$ 函数 $f_{EF}$ 表示一个早融合使用的卷积操作，在早融合之前压缩了通道数。函数 $f_{con}$ 表明了最后的多尺度融合，表示卷积层后的一个拼接层。$\\bf{S}$ 表示 SemanticPlus 的输出，即 带有时间权重的 POI 分布。 Training算法 1 描述了训练过程。前 7 行是构建训练集和 POI 信息，模型通过 Adam 训练（8-12 行） Performance Evaluation这部分，我们在两个数据集上不同城市的不同类型的流量上做了大量的实验，为了回答三个研究问题： 我们的提出的 DeepSTN+ 是否比现存的方法好？ ResPlus, SemanticPlus, 早融合是怎么提升预测结果的？ DeepSTN+ 的超参数如何影响预测结果？ Datasets表 1 包含了数据。每个数据有两个子集：流量轨迹和 POI 信息。 MobileBJ: 数据是中国一个很流行的社交网络应用商提供的，时间范围是4 月 1 日到 4 月 30 日。记录了用户请求区域服务时的位置。我们用定义 2 转换成了网格流量。我们选择最后一周的数据作为测试集，前面的作为训练集。表 2 展示了这个数据集的 17 类 POI 信息。 BikeNYC: NYC 的自行车数据，2014 年，4 月 1 日到 9 月 30 日。数据包含了旅途时长，出发和到达站的 ID，起始和结束时间。最后 14 天的数据用来测试，其他的训练。我们选了 9 类 POI 信息。 Baselines HA VAR ARIMA ConvLSTM ST-ResNet Metrics and Parameters RMSE $$RMSE = \\sqrt{\\frac{1}{T} \\sum^T_{i=1} \\Vert \\mathbf{X}_i - \\widehat{X}_i \\Vert^2_2},$$ MAE $$MAE = \\frac{1}{T} \\sum^T_{i=1} \\vert \\mathbf{X}_i - \\widehat{\\mathbf{X}}_i \\vert,$$ RMSE 作为 loss function。 表 3 展示了不同的参数设置。","link":"/blog/2019/05/08/deepstn-context-aware-spatial-temporal-neural-network-for-crowd-flow-prediction-in-metropolis/"},{"title":"Diffusion-Convolutional Neural Networks","text":"NIPS 2016。DCNNs，写的云里雾里的，不是很懂在干什么。。。就知道是融入了转移概率矩阵，和顶点的特征矩阵相乘，算出每个顶点到其他所有顶点的 $j$ 步转移的特征与转移概率的乘积，成为新的顶点表示，称为diffusion-convolutional representation，然后乘以一个卷积核，套一个激活，卷积就定义好了。应用还是在顶点分类与图分类上。原文链接：Diffusion-Convolutional Neural Networks。 摘要我们提出了diffusion-convolutional neural networks(DCNNs)，是图结构数据上的新模型。引入diffusion-convolution操作，可以从图结构数据中得到基于扩散性质的表示，用于顶点分类。DCNN还有几个性质，关于一个图数据的隐含表示，还有多项式时间复杂度的预测以及高效的GPU实现。通过多个真实数据集的实验，DCNN表现出了在关系顶点分类任务上超越概率关系模型以及kernel-on-graph的结果。 1 引言处理结构化数据很难。一方面是找到正确的方式表示并挖掘数据的结构可以提升预测的性能；另一方面，找到这样的表示很难，增加结构信息到模型中会急剧地增加预测和学习的复杂度。 我们的工作是为一类结构化数据设计一个灵活的模型，这个模型增强预测能力且避免复杂度的增加。为了完成这个模型，我们通过引入”diffusion-convolution”操作将卷积神经网络扩展到图结构数据上。简单地说，并不是像标准卷积操作一样扫描一个矩形的参数，diffusion-convolution操作通过扩散性的过程扫描图结构输入中每个顶点，构建一个隐含表示。 我们的受到的启发是：捕获了图扩散性的表示在预测上可以提供比图本身更好的结果。图扩散性可以表达成矩阵的幂序列，提供了一个简单的机制来包含关于实体的上下文信息，这些实体可以在多项式时间复杂度内计算出来，并在GPU上实现。 我们提出了diffusion-convolutional神经网络(DCNN)，在图数据的各种各样的分类任务上测试了性能。在分类任务中很多技术包含了结构的信息，比如概率关系模型和核方法；DCNN提供了一个补充的方法，在顶点分类上获得了巨大的提升。 DCNN的优势：·精度： DCNN比其他方法在顶点分类任务上精度更高，图分类上表现的也不错。·灵活性： DCNN提供了图数据的灵活表示，使用简单的处理对顶点特征、边特征以及结构信息进行编码。DCNN可以用于很多分类任务，包括顶点分类，边分类，图分类。·速度： DCNN的预测可以表示成一系列多项式时间复杂度的tensor操作，模型可以在GPU上实现。 2 模型长度为 $T$ 的一个图的集合 $\\mathcal{G} = \\lbrace G_t \\mid t \\in 1…T \\rbrace $。每个图 $G_t = (V_t, E_t)$ 由顶点 $V_t$ 和边 $E_t$ 组成。顶点一起表示为一个 $N_t \\times F$ 的特征矩阵 $X_t$，其中 $N_t$ 是 $G_t$ 的顶点数，边 $E_t$ 通过一个 $N_t \\times N_t$ 的邻接矩阵 $A_t$ 编码，通过这个我们可以计算出一个度归一化的转移矩阵 $P_t$，这个矩阵给出了从顶点 $i$ 一步转移到 $j$ 的概率。图 $G_t$ 没有限制，有向无向，带权不带权都可以。对于我们的任务来说，要么是顶点、边有标签 $Y$，要么是图有标签 $Y$，不同情况下 $Y$ 的维度不同。 如果 $T=1$，也就是只有一个图，标签是顶点或边，那么预测标签 $Y$ 就转换为了半监督分类问题了；如果输入中没有边的表示，就变成了标准的监督问题。如果 $T&gt;1$，标签是每个图的标签，那就是监督图分类问题。 DCNN接受 $\\mathcal{G}$ 作为输入，返回一个 $Y$ 的hard prediction或是条件概率分布 $\\mathbb{P}(Y \\mid X)$。每个实体（顶点、图、或边）被转换为扩散卷积表示，由 $F$ 个特征上 $H$ 步扩散的维度为 $H \\times F$ 的实数矩阵定义，每个实体是由 $H \\times F$ 的实数矩阵 $W^c$ 和一个非线性可微分函数 $f$ 计算激活定义的。所以对于顶点分类任务，图 $t$ 的扩散卷积(diffusion-convolutional)表示 $Z_t$，是一个 $N_t \\times H \\times F$ 的tensor，如图1a所示；对于图或边分类任务，$Z_t$ 是一个 $H \\times F$ 或 $N_t \\times H \\times F$ 的矩阵，如图1b和图1c。(原文里面写的是 $M_t \\times H \\times F$，我觉得是写错了) 术语”diffusion-convolution”的意思是唤起卷积神经网络的特征的特征：feature learning, parameter tying, invariance。DCNN核心操作是从顶点和他们的特征映射到从那个顶点开始的扩散过程的结果上。不同于标准的CNN，DCNN参数根据搜索的深度而不是他们在网格中的位置而绑定起来。扩散卷积表示对于顶点的index是不变的，而不是他们的位置；换句话说，两个同质的图的扩散卷积激活会是一样的。不像标准的CNN，DCNN没有池化。 顶点分类 $P^\\ast_t$ 是一个 $N_t \\times H \\times N_t$ 的tensor，包含了 $P_t$ 的幂序列，对顶点 $i$，$j$ 步，图 $t$ 的特征 $k$ 的扩散卷积的激活值 $Z_{tijk}$ 是：$$\\tag{1}Z_{tijk} = f(W^c_{jk} \\cdot \\sum^{N_t}_{l=1}P^*_{tijl}X_{tlk})$$ 激活使用矩阵形式可以写成：$$\\tag{2}Z_t = f(W^c \\odot P^\\ast_t X_t)$$ 其中 $\\odot$ 表示element-wise乘法；图1a。模型只有 $O(H \\times F)$ 个参数，使得隐扩散卷积表示的参数数量与输入大小无关。 模型通过一个连接 $Z$ 和 $Y$ 的dense layer完成。对于 $Y$ 的hard prediction，表示为 $\\hat{Y}$，可以通过最大的激活值得到，条件概率分布 $\\mathbb{P}(Y \\mid X)$ 可以通过使用softmax得到：$$\\tag{3}\\hat{Y} = \\arg\\max(f(W^d \\odot Z))$$$$\\tag{4}\\mathbb{P}(Y \\mid X) = \\mathrm{softmax}(f(W^d \\odot Z))$$ 图分类 DCNN可以通过在顶点上取均值激活扩展成图分类$$\\tag{5}Z_t = f(W^c \\odot 1^T_{N_t} P^\\ast_t X_t / N_t)$$其中 $1_{N_t}$ 是一个 $N_t \\times 1$ 的向量，如图1b所示。 学习 DCNN使用随机梯度下降训练。每轮，顶点的index随机的分到几个batches中。每个batch的error通过取taking slices of the graph definition power series，然后正向、反向、梯度上升更新权重。也使用了windowed early stopping，如果validation error大于前几轮的平均值，就stop。","link":"/blog/2018/07/19/diffusion-convolutional-neural-networks/"},{"title":"Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting","text":"ICLR 2018，DCRNN，模型借鉴了Structured Sequence Modeling With Graph Convolutional Recurrent Networks (ICLR 2017 reject)里面的DCRNN，将该模型应用于了交通预测上。而且后者的论文使用的卷积是Defferrard提出的图卷积，这篇论文中使用的是扩散卷积，这种扩散卷积使用的是随机游走，与Diffusion-Convolutional Neural Networks (NIPS 2016)的扩散卷积还不一样。构造出来的DCRNN使用了Structured Sequence Modeling With Graph Convolutional Recurrent Networks (ICLR 2017 reject)两种形式中的模型2，即使用扩散卷积学习出空间表示后，放入GRU中进行时间上的建模。原文链接：Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting 摘要交通预测的挑战：1. 对路网复杂的空间依赖关系， 2. 路况变换与非线性的时间动态性， 3. 长期预测的困难性。我们提出了在有向图上对交通流以扩散形式进行建模的方法，介绍了 Diffusion Convolutional Recurrent Neural Network (DCRNN)，用于交通预测的深度学习框架，同时集成了交通流中的空间与时间依赖。DCRNN 使用图上的双向随机游走捕获了空间依赖，使用编码解码框架以及 scheduled sampling 捕获时间依赖。我们在两个真实的交通数据集上评估了模型，比 state-of-the-art 强了12%-15%。 1 引言对一个在动态系统中运行的学习系统来说，时空预测是一个很关键的任务。自动驾驶、电网优化、供应链管理等都是它的应用。我们研究了一个重要的任务：路网上的交通预测，这是智能交通系统中的核心部分。目标是给定历史车速与路网数据，预测未来的车速。 任务有挑战性的原因是复杂的时空依赖关系以及长期预测的上的难度。一方面，交通数据序列表现出了强烈的时间动态性(temporal dynamics)。反复的事件如高峰期或交通事故导致了数据的非平稳性，使得长期预测很困难。另一方面，路网上的监测器包含了复杂但是唯一的空间联系(spatial correlations)。图1展示了一个例子。路1和路2是相关联的，但是路1和路3没有关联。尽管路1和路3在欧氏空间中很近，但是他们表现出了不同的形式。此外，未来的车速更容易受到下游交通的影响，而非上游。这就意味着交通上的空间结构不是欧氏空间的，而是有向的。 交通预测已经研究了几十年，有两个主要类别：知识驱动的方法和数据驱动的方法。在运输和操作研究中，知识驱动的方法经常使用排队论，模拟交通中的用户行为(Cascetta, 2013)。时间序列社区中，数据驱动的方法如 Auto-Regressive Integrated Moving Average(ARIMA) 模型，Kalman filtering 还是很流行的(Liu et al., 2011; Lippi et al., 2013)。然而，简单的时间序列模型通常依赖平稳假设，这经常与实际交通数据不符。最近开始在交通预测上应用深度学习模型 (Lv et al., 2015; Yu et al., 2017b) ，但是没有考虑空间结构。Wu &amp; Tan 2016和Ma et al. 2017 使用 CNN 对空间关系进行建模，但是在欧氏空间中的。Bruna et al. 2014，Defferrard et al. 2016 研究了图卷积，但是只能处理无向图。 我们使用一个有向图来表示 pair-wise spatial correlations。图的顶点是sensors，边是权重，通过路网上 sensor 之间的距离得到。我们使用扩散卷积 (diffusion convolution) 操作来捕获空间依赖关系，以扩散性是对交通流的动态性建模。提出了 Diffusion Convolutional Recurrent Neural Network (DCRNN)，整合了 diffusion convolution 和 sequence to sequence 架构以及 scheduled sampling 技术。在真实数据集上衡量模型时，DCRNN 比state-of-the-art好很多。· 我们研究了交通预测问题，在有向图上对交通的空间依赖以扩散形式建模。提出了 diffusion convolution，有着直观的解释以及高效的计算。· 我们提出了 Diffusion Convolutional Recurrent Neural Network (DCRNN)，使用 diffusion convolution，sequence to sequence，scheduled sampling 同时对时间和空间依赖关系进行捕获的方法。DCRNN 不限于运输领域，可以应用到其他的时空预测问题上。· 做了很多实验，效果很好。 2 Methodology2.1 Traffic Forecasting Problem$N$ 个sensors。检测器网络表示成带权有向图 $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E}, \\boldsymbol{W})$，$\\mathcal{V}$ 是顶点集，$\\vert \\mathcal{V} \\vert = N$，$\\mathcal{E}$ 是边集，$\\boldsymbol{W} \\in \\mathbb{R}^{N \\times N}$ 是带权邻接矩阵，表示顶点相似性（如路网距离的一个函数）。图信号矩阵$\\boldsymbol{X} \\in \\mathbb{R}^{N \\times P}$，$P$ 是每个顶点的特征数。$\\boldsymbol{X}^{(t)}$ 表示时间 $t$ 观测到的图信号，交通预测问题目的是学习一个函数 $h(\\cdot)$，将 $T’$ 个历史的图信号映射到未来的 $T$ 个图信号上，给定图 $\\mathcal{G}$:$$[\\boldsymbol{X}^{(t-T’+1)}, …, \\boldsymbol{X}^{(t)}; \\mathcal{G}] \\xrightarrow{h(\\cdot)} [\\boldsymbol{X}^{(t+1)}, …, \\boldsymbol{X}^{(t+T)}]$$ 2.2 Spatial Dependency Modeling扩散形式以 $\\mathcal{G}$ 上的随机游走来刻画，重启概率 $\\alpha \\in [0, 1]$，状态转移矩阵 $\\boldsymbol{D}^{-1}_O \\boldsymbol{W}$。这里，$\\boldsymbol{D}_{\\boldsymbol{O}} = \\mathrm{diag}(\\boldsymbol{W1})$ 是出度的对角矩阵，$\\mathbf{1} \\in \\mathbb{R}^N$ 表示所有都为1的向量。多个时间步之后，Markov process 会收敛到平稳分布 $\\mathcal{P} \\in \\mathbb{R}^{N \\times N}$上，第 $i$ 行 $\\mathcal{P}_{i,:} \\in \\mathbb{R}^N$ 表示从顶点 $v_i \\in \\mathcal{V}$ 扩散的可能性，也就是对顶点 $v_i$ 的 proximity。下面的引理是平稳分布的闭式解。 Lemma 2.1 (Teng et al., 2016) 扩散过程的平稳分布可以表示为图上的无限随机游走的带权组合，可以通过以下式子计算：$$\\tag{1}\\mathcal{P} = \\sum^\\infty_{k=0} \\alpha(1 - \\alpha)^k (\\boldsymbol{D}^{-1}_O \\boldsymbol{W})^k$$其中 $k$ 是diffusion step。实际上，我们使用有限的 $K$ 阶扩散过程，给每一步分配一个可训练的权重。我们也融入反向扩散过程，因为双向扩散可以让模型更灵活地去捕获上游和下游交通带来的影响。 Diffusion Convolution 图信号 $\\boldsymbol{X} \\in \\mathbb{R}^{N \\times P}$ 和滤波器 $f_\\theta$ 的扩散卷积操作的结果是：$$\\tag{2}\\boldsymbol{X}_{:,p} \\star_{\\mathcal{G}} f_\\theta = \\sum^{K-1}_{k=0} (\\theta_{k,1} (\\boldsymbol{D}^{-1}_O \\boldsymbol{W})^k + \\theta_{k,2}(\\boldsymbol{D}^{-1}_I \\boldsymbol{W}^T)^k) \\boldsymbol{X}_{:,p} \\ \\ \\ \\ \\mathrm{for} \\ \\ p \\in \\lbrace 1, …, P \\rbrace$$其中 $\\theta \\in \\mathbb{R}^{K \\times 2}$ 表示卷积核参数，$\\boldsymbol{D}^{-1}_O \\boldsymbol{W}$ 和 $\\boldsymbol{D}^{-1}_I \\boldsymbol{W}^T$ 表示扩散过程和反向扩散的转移概率矩阵。一般，计算卷积是很耗时的。然而，如果 $\\mathcal{G}$ 是稀疏的，式2可以通过递归的复杂度为 $O(K)$ 的sparse-dense矩阵乘法高效的计算，总时间复杂度为 $O(K \\vert \\mathcal{E} \\vert) \\ll O(N^2)$。附录B有详细的描述。 Diffusion Convolutional Layer 式2定义的卷积操作，我们可以构建一个扩散卷积层，将 $P$ 维特征映射到 $Q$ 维输出上。将参数表示为 $\\mathbf{\\Theta} \\in \\mathbb{R}^{Q \\times P \\times K \\times 2} = [ \\boldsymbol{\\theta} ]_{q, p}$，其中 $\\mathbf{\\Theta}_{q,p,:,:} \\in \\mathbb{R}^{K \\times 2}$ 是第 $p$ 个输入和 $q$ 个输出的参数。扩散卷积层为：$$\\tag{3}\\boldsymbol{H}_{:,q} = \\boldsymbol{a}(\\sum^P_{p=1} \\boldsymbol{X}_{:,p} \\star_{\\mathcal{G}} f_{\\mathbf{\\Theta}_{q,p,:,:}}) \\ \\ \\ \\ \\mathrm{for} \\ q \\in \\lbrace 1, …, Q \\rbrace$$其中，$\\boldsymbol{X} \\in \\mathbb{R}^{N \\times P}$ 是输入，$\\boldsymbol{H} \\in \\mathbb{R}^{N \\times Q}$ 是输出，$\\lbrace f_{\\mathbf{\\Theta}_{q,p,:,:}} \\rbrace $ 是滤波器，$a$ 是激活函数。扩散卷积层学习图结构数据的表示，我们可以使用基于随机梯度的方法训练它。 Relation with Spectral Graph Convolution: 扩散卷积是定义在有向和无向图上的。当使用在无向图上时，我们发现很多现存的图结构卷积操作，包括流行的普图卷积，ChebNet，可以看作是一个扩散卷积的特例。令 $\\boldsymbol{D}$ 表示度矩阵，$\\boldsymbol{L} = \\boldsymbol{D}^{-\\frac{1}{2}}(\\boldsymbol{D} - \\boldsymbol{W}) \\boldsymbol{D}^{-\\frac{1}{2}}$ 是图归一化的拉普拉斯矩阵，接下来的Proposition解释了连接。 Proposition 2.2. 谱图卷积的定义：$$\\boldsymbol{X}_{:,p} \\star_{\\mathcal{G}} f_\\boldsymbol{\\theta} = \\Phi F(\\boldsymbol{\\theta}) \\Phi^T \\boldsymbol{X}_{:,p}$$特征值分解 $\\boldsymbol{L} = \\Phi \\Lambda \\Phi^T$，当图 $\\mathcal{G}$是无向图时，$F(\\boldsymbol{\\theta}) = \\sum^{K-1}_0 \\theta_k \\Lambda^k$，等价于图的扩散卷积。证明见后记C。 2.3 Temporal Dynamics Modeling我们利用 RNN 对时间依赖建模。我们使用 GRU，简单有效的 RNN 变体。我们将 GRU 中的矩阵乘法换成了扩散卷积，得到了我们的扩散卷积门控循环单元 Diffusion Convolutional Gated Recurrent Unit(DCGRU).$$\\boldsymbol{r}^{(t)} = \\sigma(\\mathbf{\\Theta}_r \\star_\\mathcal{G} [\\boldsymbol{X}^{(t)}, \\boldsymbol{H}^{(t-1)}] + \\boldsymbol{b}_r) \\\\\\boldsymbol{u}^{(t)} = \\sigma( \\mathbf{\\Theta}_u \\star_\\mathcal{G} [\\boldsymbol{X}, \\boldsymbol{H}^{(t-1)}] + \\boldsymbol{b}_u) \\\\\\boldsymbol{C}^{(t)} = \\mathrm{tanh}(\\mathbf{\\Theta}_C \\star_\\mathcal{G} [\\boldsymbol{X}^{(t)}, (\\boldsymbol{r}^{(t)} \\odot \\boldsymbol{H}^{(t-1)})] + \\boldsymbol{b}_c) \\\\\\boldsymbol{H}^{(t)} = \\boldsymbol{u}^{(t)} \\odot \\boldsymbol{H}^{(t-1)} + (1 - \\boldsymbol{u}^{(t)}) \\odot \\boldsymbol{C}^{(t)}$$其中 $\\boldsymbol{X}^{(t)}, \\boldsymbol{H}^{(t)}$ 表示时间 $t$ 的输入和输出，$\\boldsymbol{r}^{(t)}, \\boldsymbol{u}^{(t)}$ 表示时间 $t$ 的reset gate和 update gate。$\\star_\\mathcal{G}$ 表示式2中定义的混合卷积，$\\mathbf{\\Theta}_r, \\mathbf{\\Theta}_u, \\mathbf{\\Theta}_C$ 表示对应的滤波器的参数。类似 GRU，DCGRU 可以用来构建循环神经网络层，使用 BPTT 训练。 在多步预测中，我们使用 Sequence to Sequence 架构。编码解码器都是 DCGRU。训练时，我们把历史的时间序列放到编码器，使用最终状态初始化解码器。解码器生成预测结果。测试时，ground truth 替换成模型本身生成的预测结果。训练和测试输入的分布的差异会导致性能的下降。为了减轻这个问题的影响，我们使用了 scheduled sampling (Bengio et al., 2015)，在训练的第 $i$ 轮时，模型的输入要么是概率为 $\\epsilon_i$ 的 ground truth，要么是概率为 $1 - \\epsilon_i$ 的预测结果。在训练阶段，$\\epsilon_i$ 逐渐的减小为0，使得模型可以学习到测试集的分布。 图2展示了 DCRNN 的架构。整个网络通过 BPTT 循环生成目标时间序列的最大似然得到。DCRNN 可以捕获时空依赖关系，应用到多种时空预测问题上。 3 Related Work运输领域和运筹学中交通预测是传统问题，主要依赖于排队论和仿真(Drew, 1968)。数据驱动的交通预测方法最近受到了很多的关注，详情可以看近些年的 paper (Vlahogianni et al., 2014)。然而，现存的机器学习模型要么有着很强的假设（如 auto-regressive model ）要么不能考虑非线性的时间依赖（如 latent space model Yu et al. 2016; Deng et al. 2016）。深度学习模型为解决时间序列预测问题提供了新的方法。举个例子，在 Yu et al. 2017b; Laptev et al. 2017 的工作中，作者使用深度循环神经网络研究时间序列预测问题。卷积神经网络已经被应用到交通预测上。Zhang et al. 2016; 2017 将路网转换成了 2D 网格，使用传统的 CNN 预测人流。Cheng et al. 2017 提出了DeepTransport，通过对每条路收集上下游邻居路段对空间依赖建模，在这些邻居上分别使用卷积操作。 最近，CNN 基于谱图理论已经泛化到任意的图结构上。图卷积神经网络由 Bruna et al. 2014 首次提出，在深度神经网络和谱图理论之间建立了桥梁。Defferrard et al. 2016 提出了 ChebNet，使用快速局部卷积滤波器提升了 GCN。Kipf &amp; Welling 2017 简化了 ChebNet，在半监督分类任务上获得了 state-of-the-art 的表现。Seo et al. 2016 融合了 ChebNet 和 RNN 用于结构序列建模。Yu et al. 2017a 对检测器网络以无向图的形式，使用 Chebnet 和卷积序列模型 (Gehring et al. 2017) 进行建模做预测。这些提及的基于谱的理论的限制之一是，他们需要图是无向的，来计算有意义的谱分解。从谱域到顶点域，Atwood &amp; Towsley 2016 提出了扩散卷积神经网络 (DCNN)，以图结构中每个顶点的扩散过程定义了卷积。Hechtlinger et al. 2017 提出了 GraphCNN 对每个顶点的 $p$ 个最近邻邻居进行卷积，将卷积泛化到图上。然而，这些方法没有考虑时间的动态性，主要处理的是静态图。 我们的方法不同于这些方法，因为问题的设定不一样，而且图卷积的公式不同。我们将 sensor network 建立成一个带权有向图，比网格和无向图更真实。此外，我们提出的卷积操作使用双向图随机游走来定义，集成了序列到序列模型以及 scheduled sampling ，对长时间的时间依赖建模。 4 Experiments我们在两个数据集上做了实验：（1）METR-LA 这个交通数据集包含了洛杉矶高速公路线圈收集的数据 (Jagadish et al., 2014)。我们选择了207个检测器，收集了从2012年3月1日到2012年6月30日4个月的数据用于实验。（2）PEMS_BAY 这个交通数据集由 California Transportation Agencies(CalTrans)Performance Measurement System (PeMS) 收集。我们选了 Bay Area 的325个检测器，收集了从2017年1月1日到2017年5月31日6个月的数据用于实验。两个数据集监测器的分布如图8所示。 这两个数据集，我们将车速聚合到了5分钟的窗口内，使用了 Z-Score normalization。70%的数用于训练，20%用于测试，10%用于验证。为了构建检测器网络，我们计算了任意两个 sensor 的距离，使用了 thresholded Gaussian kernel 来构建邻接矩阵(Shuman et al., 2013)。$W_{ij} = \\exp{(-\\frac{\\mathrm{dist}(v_i, v_j)^2}{\\sigma^2})} \\ \\text{if} \\ \\text{dist}(v_i, v_j) \\leq \\mathcal{\\kappa}, \\mathrm{otherwise} \\ 0$，其中 $W_{ij}$ 表示了检测器 $v_i$ 和 $v_j$ 之间的权重，$\\mathrm{dist}(v_i, v_j)$ 表示检测器 $v_i$ 到 $v_j$ 之间的距离。$\\sigma$ 表示距离的标准差，$\\kappa$ 表示阈值。 4.1 Experimental SettingsBaselines 1. $\\rm{HA}$：历史均值，将交通流建模成周期性过程，使用之前的周期的加权平均作为预测。2. $\\mathrm{ARIMA}_{kal}$：Auto-Regressive Integrated Moving Average model with Kalman filter，广泛地应用于时间序列预测上。3. $\\rm{VAR}$: Vector Auto-Regression(Hamilton, 1994)。4. $\\rm{SVR}$：Support Vector Regression，使用线性支持向量机用于回归任务。5. Feed forward Neural network (FNN)：前向传播神经网络，两个隐藏层，L2正则化。6. Recurrent Neural Network with fully connected LSTM hidden units (FC-LSTM)(Sutskever et al., 2014). 所有的神经网络方法都是用 Tensorflow 实现，使用 Adam 优化器，学习率衰减。使用 Tree-structured Parzen Estimator(TPE)(Bergstra et al., 2011) 在验证集上选择最好的超参数。DCRNN 的详细参数设置和 baselines 的超参数设置见附录E。 4.2 Traffic Forecasting Performance Comparison表1展示了不同的方法在15分钟，30分钟，1小时在两个数据集上预测的对比。这些方法在三种常用的 metrics 上进行了评估，包括1. MAE， 2. MAPE（Mean Absolute Percentage Error）， 3. RMSE。这些 metrics 中的缺失值被排除出去。这些公式在后记E.2。我们观察到这两个数据集上有以下现象：1. RNN-based methods，包括FC-LSTM和DCRNN，一般比其他的方法表现得好，这强调对时间依赖的建模的重要性。2. DCRNN在所有的 forecasting horizons 中的所有 metrics 上都获得了最好的表现，这说明对空间依赖建模的有效性。3. 深度学习模型，包括 FNN，FC-LSTM，DCRNN 在长期预测上，倾向于比线性的 baseline 有更好的结果。比如，1小时。这是因为随着 horizon 的增长，时间依赖变得更加非线性。此外，随着历史均值不依赖短期数据，它的表现对于 forecasting horizon 的小增长是不变的。 需要注意的是，METR-LA（Los Angeles，有很复杂的交通环境）数据比 PEMS-BAY 更有挑战性，所以我们将 METR-LA 的数据作为以下实验的默认数据集。 4.3 Effect of Spatial Dependency Modeling为了继续深入对空间依赖建模的影响，我们对比了 DCRNN 和以下变体： 1. DCRNN-NoConv，这个通过使用单位阵替换扩散卷积（式2）中的转移矩阵，忽略了空间依赖。这就意味着预测只能通过历史值预测。 2. DCRNN-UniConv，扩散卷积中只使用前向随机游走；图3展示了这三个模型使用大体相同数量的参数时的学习曲线。没有扩散卷积，DCRNN-NoConv 有着更大的 validation error。此外，DCRNN获得了最低的 validation error，说明了使用双向随机游走的有效性。这个告诉我们双向随机游走赋予了模型捕获上下游交通影响的能力与灵活性。 为了研究图的构建方法的影响，我们构建了一个无向图，$\\widehat{W}_{ij} = \\widehat{W}_{ji} = \\max(W_{ij}, W_{ji})$，其中 $\\widehat{\\boldsymbol{W}}$ 是新的对称权重矩阵。然后我们使用了 DCRNN 的一个变体，表示成 GCRNN，使用 ChebNet 卷积的序列到序列学习，并用大体相同的参数数量。表2展示了 DCRNN 和 GCRNN 在 METR-LA 数据集上的对比。DCRNN 都比 GCRNN 好。这说明有向图能更好的捕获交通检测器之间的非对称关系。图4展示了不同参数的影响。$K$ 大体对应了卷积核感受野的大小，单元数对应了卷积核数。越大的 $K$ 越能使模型捕获更宽的空间依赖，代价是增加了学习的复杂度。我们观测到随着 $K$ 的增加，验证集上的误差先是快速下降，然后微微上升。改变不同数量的单元也会有相似的情况。 4.4 Effect of Temporal Dependency Modeling为了衡量时间建模的影响，包括序列到序列框架以及 scheduled sampling 技术，我们设计 DCRNN 的三种变体：1. DCNN：我们拼接历史的观测值为一个固定长度的向量，将它放到堆叠的扩散卷积层中，预测未来的时间序列。我们训练一个模型只预测一步，将之前的预测结果放到模型中作为输入，使用多步前向预测。2. DCRNN-SEQ：使用编码解码序列到序列学习框架做多步预测。3. DCRNN：类似 DCRNN-SEQ ，除了增加了 scheduled sampling。 图5展示了这四种方法针对 MAE 的对比。我们观察到：1. DCRNN-SEQ 比 DCNN 好很多，符合了对时间建模的重要性。2. DCRNN 达到了最好的效果，随着预测 horizon 的增加，它的先进性变得越来越明显。这主要是因为模型在训练的时候就在处理多步预测时出现的误差，因此会很少的受到误差反向传播的影响。我们也训练了一个总是将输出作为输入扔到模型中的模型。但是它的表现比这三种变体都差，这就强调了 scheduled sampling 的重要性。 4.5 模型的解释性为了更好的理解模型，我们对预测结果和学习到的滤波器进行性了可视化。图6展示了预测1小时的效果。我们观察到了以下情况：1. DCRNN 在交通流速度中存在小的震荡时，用均值生成了平滑的预测结果（图6a）。这反映了模型的鲁棒性。2. DCRNN 比 baseline 方法（如FC-LSTM）更倾向于精确的预测出突变。图6b展示了 DCRNN 预测了高峰时段的起始和终止。这是因为 DCRNN 捕获了空间依赖，能够利用邻居检测器速度的变换来精确预测。图7展示了以不同顶点为中心学习到的滤波器的样例。星表示中心，颜色表示权重。我们可以观察到权重更好的在中心周围局部化，而且权重基于路网距离进行扩散。更多的可视化在附录F。 5 Conclusion我们对路网上的交通预测做了时空上的建模，提出了 diffusion convolutional recurrent neural network，可以捕获时空依赖。特别地，我们使用双向随机游走，对空间依赖建模，使用循环神经网络捕获时间的动态性。还继承了编码解码架构和 scheduled sampling 技术来提升长期预测的性能。在两个真实的数据集上评估了性能，我们的方法比 baselines 好很多。未来的工作，1. 使用提出的网络解决其他的时空预测问题；2. 对不断演化的图结构的时空依赖关系建模。 AppendixB Efficient Calculation Of Equation式2可以分解成两个有相同时间复杂度的部分，一部分是 $\\boldsymbol{D}^{-1}_O \\boldsymbol{W}$，另一部分是 $\\boldsymbol{D}^{-1}_I \\boldsymbol{W}^T$。因此我们只研究第一部分的时间复杂度。 令 $T_k(x) = (\\boldsymbol{D}^{-1}_O \\boldsymbol{W})^k \\boldsymbol{x}$，式2的第一部分可以重写为：$$\\tag{4}\\sum^{K-1}_{k=0} \\theta_k T_k (X_{:,p})$$因为 $T_{k+1}(x) = \\boldsymbol{D}^{-1}_O \\boldsymbol{W} T_k(\\boldsymbol{x})$ 和 $\\boldsymbol{D}^{-1}_O \\boldsymbol{W}$ 是稀疏的，可以很容易看出式4可以通过 $O(K)$ 的递归稀疏-稠密矩阵乘法，每次时间复杂度为 $O(\\vert \\varepsilon \\vert)$ 得到。然后，式2和式4的时间复杂度都为 $O(K\\vert \\varepsilon \\vert)$。对于稠密图，我们可以使用 spectral sparsification(Cheng et al., 2015) 使其稀疏。 C Relation With Spectral Graph ConvolutionProof. 谱图卷积利用归一化的拉普拉斯矩阵 $\\boldsymbol{L = D^{-\\frac{1}{2}}(D - W)D^{\\frac{1}{2}}} = \\mathbf{\\Phi \\Lambda \\Phi^T}$。ChebNet 使 $f_\\theta$ 参数化为一个 $\\Lambda$ 的 $K$ 阶多项式，使用稳定的切比雪夫多项式基计算这个值。$$\\tag{5}\\boldsymbol{X}_{:,p} \\star_\\mathcal{G} f_\\theta = \\mathbf{\\Phi} (\\sum^{K-1}_{k=0} \\theta_k \\mathbf{\\Lambda}^k) \\mathbf{\\Phi^T X}_{:,p} = \\sum^{K-1}_{k=0} \\theta_k \\boldsymbol{L}^k \\boldsymbol{X}_{:,p} = \\sum^{K-1}_{k=0} \\tilde{\\theta}_k T_k(\\tilde{\\boldsymbol{L}})\\boldsymbol{X}_{:,p}$$其中 $T_0(x)=1, T_1(x)=x, T_k(x) = xT_{k-1}(x) - T_{k-2}(x)$ 是切比雪夫多项式的基。令 $\\lambda_{\\mathrm{max}}$ 表示 $\\boldsymbol{L}$ 最大的特征值，$\\tilde{\\boldsymbol{L}} = \\frac{2}{\\lambda_{\\text{max}}} \\boldsymbol{L - I}$ 表示将拉普拉斯矩阵的缩放，将特征值从 $[0, \\lambda_{\\text{max}}]$ 映射到 $[-1, 1]$，因为切比雪夫多项式生成了一个在 $[-1, 1]$ 内正交的基。式5可以看成一个关于 $\\tilde{\\boldsymbol{L}}$ 的多项式，我们一会儿可以看到，ChebNet 卷积的输出和扩散卷积到常数缩放因子的输出相似。假设 $\\lambda_{\\text{max}} = 2$，无向图 $\\boldsymbol{D}_I = \\boldsymbol{D}_O = \\boldsymbol{D}$。$$\\tag{6}\\tilde{\\boldsymbol{L}} = \\boldsymbol{D}^{-\\frac{1}{2}}(\\boldsymbol{D} - \\boldsymbol{W}) \\boldsymbol{D}^{-\\frac{1}{2}} - \\boldsymbol{I} = - \\boldsymbol{D}^{-\\frac{1}{2}} \\boldsymbol{W} \\boldsymbol{D}^{-\\frac{1}{2}} \\sim - \\boldsymbol{D}^{-1} \\boldsymbol{W}$$$\\tilde{\\boldsymbol{L}}$ 和负的随机游走转移矩阵相似，因此式5的输出也和式2直到常数缩放因子的输出相似。","link":"/blog/2018/07/31/diffusion-convolutional-recurrent-neural-network-data-driven-traffic-forecasting/"},{"title":"GCN论文汇总","text":"对看过的图神经网络做个总结，目前主要是GCN。 Semi-Supervised Classification With Graph Convolutional Networks. Kipf &amp; Welling 2017ICLR 2017。使用切比雪夫多项式的1阶近似完成了高效的图卷积架构。 优点 缺点 1阶近似，比k阶近似高效 卷积需使用整个图的拉普拉斯矩阵，图不能扩展 Convolution on Graph: A High-Order and Adaptive Approach.NIPS 2016，重新定义了卷积的定义，利用k阶邻接矩阵，定义考虑k阶邻居的卷积，利用邻接矩阵和特征矩阵构建能同时考虑顶点特征和图结构信息的卷积核。在预测顶点、预测图、生成图三个任务上验证了模型的效果。 优点 缺点 Graph Convolutional Neural Networks for Web-Scale Recommender Systems.KDD 2018。使用图卷积对顶点进行表示，学习顶点的embedding，通过卷积将该顶点的邻居信息融入到向量中。 优点 缺点 超大规模的图 Diffusion-Convolutional Neural Networks.NIPS 2016。在卷积操作中融入了h-hop转移概率矩阵，通过对每个顶点计算该顶点到其他所有顶点的转移概率与特征矩阵的乘积，构造顶点新的特征表示，即diffusion-convolutional representation，表征顶点信息的扩散，然后乘以权重矩阵W，加激活函数，得到卷积的定义。在顶点分类和图分类上做了测试。 优点 缺点 没有增加模型的复杂度 空间复杂度高 使用转移概率矩阵 模型不能捕获尺度较大的空间依赖关系 不同的分类任务（顶点、图）有不同的卷积表达式 Graph Attention Networks.ICLR 2018。图注意力网络，使用self-attention来构建graph attentional layer，attention会考虑当前顶点所有的邻居对它的重要度，基于谱理论的模型不能应用到其他不同结构的图上，而这个基于attention的方法能有效的解决这个问题。 Inductive Representation Learning on Large Graphs.NIPS 2017。提出的方法叫GraphSAGE，针对的问题是之前的NRL是transductive，作者提出的GraphSAGE是inductive。主要考虑了如何聚合顶点的邻居信息，对顶点或图进行分类。 应用： Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic.IJCAI 2018，大体思路：使用Kipf &amp; Welling 2017的近似谱图卷积得到的图卷积作为空间上的卷积操作，时间上使用一维卷积对所有顶点进行卷积，两者交替进行，组成了时空卷积块，在加州PeMS和北京市的两个数据集上做了验证。 Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition.AAAI 2018，以人体关节为图的顶点，构建空间上的图，然后通过时间上的关系，连接连续帧上相同的关节，构成一个三维的时空图。针对每个顶点，对其邻居进行子集划分，每个子集乘以对应的权重向量，得到时空图上的卷积定义。实现时使用Kipf &amp; Welling 2017的方法实现。 优点 缺点 将空间和时间一体化 实现上仍是Kipf &amp; Welling的方法","link":"/blog/2018/07/23/gcn论文汇总/"},{"title":"Gaussian Naive Bayes","text":"假设连续型随机变量服从高斯分布的朴素贝叶斯。发现自己实现的版本比sklearn的精度低了20%左右……研究了一下差在了哪里。 朴素贝叶斯朴素贝叶斯是基于贝叶斯定理与特征条件独立假设的分类器。 原理朴素贝叶斯通过给定训练集 $$T = \\lbrace (x_1, y_1), (x_2, y_2), ···, (x_N, y_N)\\rbrace $$ 训练学习到联合概率分布$P(X, Y)$，通过先验概率分布 $$P(Y = c_k), k = 1,2,…,K$$ 和条件概率分布 $$P(X = x \\mid Y = c_k) = P(X^{(1)} = x^{(1)}, ···, X^{(n)} = x^{(n)} \\mid Y = c_k), k=1,2,…,K$$ 学习到联合概率分布$P(X, Y)$ 由特征相互独立假设，可得 $$P(X = x \\mid Y = c_k) = \\prod^n_{j=1}P(X^{(j)}=x^{(j)} \\mid Y = c_k)$$ 分类时，对给定的输入$x$，模型计算$P(Y = c_k \\mid X = x)$，将后验概率最大的类作为$x$的类输出，后验概率计算如下： $$\\begin{aligned} P(Y = c_k \\mid X = x) &amp;= \\frac{P(X = x \\mid Y = c_k)P(Y = c_k)}{\\sum_kP(X = x \\mid Y = c_k)P(Y = c_k)} \\\\ &amp; = \\frac{P(Y = c_k) \\prod_j P(X^{(j)} = x^{(j)} \\mid Y = c_k)}{\\sum_k P(Y = c_k) \\prod_j P(X^{(j)} = x^{(j)} \\mid Y = c_k)}\\end{aligned}$$ 由于分母对任意的$c_k$都相同，故朴素贝叶斯分类器可以表示为： $$y = \\mathop{\\arg\\max}_{c_k} P(Y = c_k) \\prod_j P(X^{(j)} = x^{(j)} \\mid Y = c_k)$$ 参数估计 如果特征是离散型随机变量，可以使用频率用来估计概率。 $$P(Y = c_k) = \\frac{\\sum^N_{i=1}I(y_i = c_k)}{N}, k=1,2,…,K$$ 设第$j$个特征的取值的集合为${a_{j1}, a_{j2}, …, a_{js_j}}$，则 $$ \\begin{gathered}P(X^{(j)} = a_{jl} \\mid Y = c_k) = \\frac{\\sum^N_{i=1}I(x^{(j)}_i = a_{jl}, y_i = c_k)}{\\sum^N_{i=1}I(y_i = c_k)}\\\\ j=1,2,…,n; \\ l=1,2,…,S_j; \\ k=1,2,…,K \\end{gathered} $$ 如果特征是连续型随机变量，可以假设正态分布来估计条件概率。 $$P(X^{(j)} = a_{jl} \\mid Y = c_k) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2_{c_k,j}}}\\exp{(- \\frac{(a_{jl} - \\mu_{c_k,j})^2}{2 \\sigma^2_{c_k,j}})}$$ 这里$\\mu_{c_k,j}$和$\\sigma^2_{c_k,j}$分别为$Y = c_k$时，第$j$个特征的均值和方差。 代码因为二值分类和$n$值分类是一样的，故以下代码只实现了$n$值分类的朴素贝叶斯分类器。仓库:https://github.com/Davidham3/naive_bayes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205# -*- coding:utf-8 -*-import numpy as npfrom collections import defaultdictdef readDataSet(filename, frequency = 0, training_set_ratio = 0.7, shuffle = True): ''' read the dataset file, and shuffle, remove all punctuations Parameters ---------- filename: str, the filename of the data frequency: int, you will select the words that appeared more than the frequency you specified for example, if you set frequency equals 1, the program will return all words that they have appeared more than once. training_set_ratio: float, the ratio of training data account for in all data shuffle: bool, whether to shuffle the data Returns ---------- train_text: list, each element contains a tuple of words that in each sentence train_labels: list, each element is the label of the corresponding sentence test_text: list test_labels: list ''' with open(filename, 'r', encoding='utf-8') as f: text = f.read().strip().split('\\n') if shuffle: np.random.shuffle(text) import re # split all words by space and add them with their labels to the list \"dataset\" dataset = [] for index, i in enumerate(text): t = i.split('\\t') label = t[0] t1 = re.sub(\"[\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？?、~@#￥%……&amp;*（）]+\", \"\", t[1]) dataset.append((label, re.split(re.compile('\\s+'), t1))) print(\"dataset's size is\", len(dataset)) # split labels and words labels, text = zip(*dataset) split_line = int(len(text) * training_set_ratio) train_text = text[:split_line] train_labels = labels[:split_line] test_text = text[split_line:] test_labels = labels[split_line:] return train_text, train_labels, test_text, test_labelsdef preprocessing_training_data(text, labels): ''' use bag of words to build features for training data Parameters ---------- text: lists, each element contains a list of words in a sentence labels: lists, each element is the label of the sample corresponding to the element in text Returns ---------- trainX: ndarray, training data, the shape of it is (number of samples, number of features) trainY: ndarray, labels of training data, the shape of it is (number of samples, ) words_table: dict, key is words, value is the index in bag of words labels_table: dict, key is the label, value is the index that represents the corresponding label ''' bag_of_words = tuple(set(word for words in text for word in words)) words_table = {i: index for index, i in enumerate(bag_of_words)} trainX = np.empty((len(text), len(bag_of_words))) for index, words in enumerate(text): for word in words: trainX[index, words_table[word]] += 1 labels_table = {i: index for index, i in enumerate(set(labels))} trainY = np.array([labels_table[i] for i in labels]) return trainX, trainY, words_table, labels_tabledef preprocessing_testing_data(text, labels, words_table, labels_table): ''' use bag of words to build features for testing data Parameters ---------- text: lists, each element contains a list of words in a sentence labels: lists, each element is the label of the sample corresponding to the element in text words_table: dict, key is words, value is the index in bag of words labels_table: dict, key is the label, value is the index that represents the corresponding label Returns ---------- testX: ndarray, testing data, the shape of it is (number of samples, number of features) testY: ndarray, labels of testing data, the shape of it is (number of samples, ) ''' testX = np.empty((len(text), len(words_table))) for index, words in enumerate(text): for word in words: col = words_table.get(word) if col is not None: testX[index, words_table[word]] += 1 testY = [] for i in labels: l = labels_table.get(i) if l is not None: testY.append(l) else: labels_table[i] = len(labels_table) testY.append(labels_table[i]) testY = np.array(testY) return testX, testYclass GaussianNB: ''' Gaussian naive bayes for continous features ''' def __init__(self): self.probability_of_y = {} self.mean = {} self.var = {} def fit(self, trainX, trainY): ''' use trainX and trainY to compute the prior probability for each class and then compute the mean and variance for each features for each class Parameters ---------- trainX: ndarray, training data, the shape of it is (number of samples, number of features) trainY: ndarray, labels of training data, the shape of it is (number of samples, ) ''' labels = set(trainY.tolist()) for y in labels: x = trainX[trainY == y, :] self.probability_of_y[y] = x.shape[0] / trainX.shape[0] self.mean[y] = x.mean(axis = 0) var = x.var(axis = 0) var[var == 0] += 1e-9 * var.max() self.var[y] = var def predict(self, testX): ''' predict the labels of testX Parameters ---------- testX: ndarray, testing data, the shape of it is (number of samples, number of features) Returns ---------- ndarray: each element is a str variable, which represent the label of corresponding testing data ''' results = np.empty((testX.shape[0], len(self.probability_of_y))) labels = [] for index, (label, py) in enumerate(self.probability_of_y.items()): t = np.exp(- ((testX - self.mean[label]) ** 2) / (2 * self.var[label])) / np.sqrt(2 * np.pi * self.var[label]) t[t == 0] = np.finfo(np.longdouble).eps a = np.log(t) results[:, index] = np.exp(np.sum(a, axis = 1)) * py labels.append(label) return np.array(labels)[np.argmax(results, axis = 1)]def accuracy(prediction, testY): ''' compute accuracy for prediction Parameters ---------- prediction: ndarray, the prediction generated by the classifier testY: ndarray, true labels Returns ---------- float, accuracy ''' return np.sum((prediction - testY) == 0) / testY.shape[0]def main(): datadir = 'SMSSpamCollection' train_text, train_labels, test_text, test_labels = readDataSet(datadir) trainX, trainY, words_table, labels_table = preprocessing_training_data(train_text, train_labels) print('training data shape:', trainX.shape, trainY.shape) testX, testY = preprocessing_testing_data(test_text, test_labels, words_table, labels_table) print('testing data shape:', testX.shape, testY.shape) a = GaussianNB() a.fit(trainX, trainY) r = a.predict(testX) print('accuracy:', accuracy(r, testY))if __name__ == '__main__': main() 按照上面的代码实现完后，和scikit-learn的Gaussian Naive Bayes做对比，发现精度查了20%左右…… 然后就看了scikit-learn的实现，发现sklearn的实现将公式化简后实现的。 首先，我们的目标是： $$y = \\mathop{\\arg\\max}_{c_k} P(Y = c_k) \\prod_j P(X^{(j)} = x^{(j)} \\mid Y = c_k)$$ 因为求这个的时候可能会出现下溢的情况，也就是很多项都很小的时候，这个连乘就会出问题。 为了解决这个问题，我原来以为是先取对数，再取指数，这样值就不变了，但是值用取对数就行，因为取对数不改变单调性。 那目标就变成了： $$\\begin{aligned}y &amp;= \\mathop{\\arg\\max}_{c_k} \\log^{P(Y = c_k) \\prod_j P(X^{(j)} = x^{(j)} \\mid Y = c_k)}\\\\&amp;= \\mathop{\\arg\\max}_{c_k} [ \\log^{P(y = c_k)} + \\sum_j \\log^{P(X^{(j)} = x^{(j)} \\mid Y = c_k)} ]\\end{aligned}$$ 在求条件概率的时候，也进行变换： $$\\begin{aligned}\\log^{P(X^{(j)} = x^{(j)} \\mid Y = c_k)} &amp;= \\log^{ \\ \\bigg[\\frac{1}{\\sqrt{2 \\pi \\sigma^2_{c_k,j}}} \\exp{\\bigg(- \\frac{(a_{jl} - \\mu_{c_k,j})^2}{2 \\sigma^2_{c_k,j}}\\bigg)}\\bigg]}\\\\&amp;= \\log^{ \\frac{1}{\\sqrt{2 \\pi \\sigma^2_{c_k,j}}} } + \\log^{ \\exp{\\bigg(- \\frac{(a_{jl} - \\mu_{c_k,j})^2}{2 \\sigma^2_{c_k,j}}\\bigg)} }\\\\&amp;= - \\frac{1}{2} \\log^{2 \\pi \\sigma^2_{c_k,j}} - \\frac{1}{2} \\frac{(a_{jl} - \\mu_{c_k,j})^2}{\\sigma^2_{c_k,j}}\\end{aligned}$$ scikit-learn是实现的这个公式，我想了一下，这个公式比之前的那个公式要简洁很多，之前的公式中，如果求$e$的指数很小，就会出现0，如果手动补一个eps，又会影响结果。而且我们要对方差加eps，之前的公式中如果在方差上加了eps，求完指数运算后可能还要加eps，误差会逐渐的递增。 改进后的模型： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class myGaussianNB: ''' Gaussian naive bayes for continous features ''' def __init__(self): self.label_mapping = dict() self.probability_of_y = {} self.mean = {} self.var = {} def _clear(self): self.label_mapping.clear() self.probability_of_y.clear() self.mean.clear() self.var.clear() def fit(self, trainX, trainY): ''' use trainX and trainY to compute the prior probability for each class and then compute the mean and variance for each features for each class Parameters ---------- trainX: ndarray, training data, the shape of it is (number of samples, number of features) trainY: ndarray, labels of training data, the shape of it is (number of samples, ) ''' self._clear() labels = np.unique(trainY) self.label_mapping = {label: index for index, label in enumerate(labels)} for label in labels: x = trainX[trainY == label, :] self.probability_of_y[label] = x.shape[0] / trainX.shape[0] self.mean[label] = x.mean(axis = 0, keepdims = True) self.var[label] = x.var(axis = 0, keepdims = True) + 1e-9 * np.var(trainX, axis = 0).max() def predict(self, testX): ''' predict the labels of testX Parameters ---------- testX: ndarray, testing data, the shape of it is (number of samples, number of features) Returns ---------- ndarray: each element is a str variable, which represent the label of corresponding testing data ''' results = np.empty((testX.shape[0], len(self.probability_of_y))) labels = [0] * len(self.probability_of_y) for label, index in self.label_mapping.items(): py = self.probability_of_y[label] sum_of_conditional_probability = - 0.5 * np.sum(((testX - self.mean[label]) ** 2) / self.var[label], 1) sum_of_conditional_probability += - 0.5 * np.sum(np.log(2 * np.pi * self.var[label])) results[:, index] = sum_of_conditional_probability + np.log(py) labels[index] = label return np.array(labels)[np.argmax(results, axis = 1)]","link":"/blog/2018/06/14/gaussian-naive-bayes/"},{"title":"FHWA数据","text":"FHWA数据数据地址：https://www.fhwa.dot.gov/policyinformation/tables/tmasdata/数据包含了2011到2016年的全年数据。数据说明：https://www.fhwa.dot.gov/policyinformation/tmguide/说明共有三版，但是数据对应的其实是最老的一个版本，2001年修订的那个文档。 本文只对文档中的一部分进行说明，详细的还是得看官方文档。 数据概况数据是美国高速公路局收集的交通流量数据，每月一组。 数据的格式是FHWA定的，数据会被输入到FHWA维护的两个数据库中，Traffic Volume Trends（TVT）和Vehicle Travel Information System（VTRIS）。TVT系统处理连续的流量数据，按月生成交通流量趋势报告。VIRIS系统处理车辆分类和卡车称重数据，作为年度卡车重量研究。这两个数据管理系统处理、验证、汇总、维护交通数据。TVT和VIRIS向任何人提供，通过这个地址http://www.fhwa.dot.gov/ohim/tvtw/tvtwpage.htm获得。数据收集计划由the Office of Management and Budget批准，OMB # 2125-0587，期限是2004年4月30日。（因为现在看的文档是2001年编写的，所以已经过时了，但是数据格式没有过时，新版的文档反而和当前数据(2016年)的格式对不上） 数据记录分为四类，站点描述数据、流量数据、车辆分类数据、卡车重量数据。每类数据都有自己的格式。接下来会分章节讨论四类数据的格式规范。 注意：一些字段被标记为”critical”，意思是必不可少的字段。这里描述的所有数据都上ASCII flat文件。对于缺失的字段或数据，会用空格代替。数字是右对其的，如果没有说明的话左侧补空格或0。数值型字段缺失或不可用的数据，输入空格或右对齐的-1。 四类数据记录中的部分数据项是相同的。举个例子，所有的记录包含一个6字符长度的站点标识符。这使得每个州都需要使用一个共同的标识系统。 站点描述中的一些字段被替换成了需要与GIS连接起来的交通数据。这会使得数据与NHPN(National Highway Planning Network)或相似的系统重叠。 站点数据描述站点数据描述对所有的流量、车辆分类、称重站点都适用。一个站点描述文件包含了对每个交通监测站的记录（每年）。所有的字段都是字符型。命名规则是”ssyy.STA”（现在已经不是了）。 2016年数据的文件名：AK_2016 (TMAS).STA，前两个字符表示这个州的缩写。TMAS是指Travel Monitoring Analysis System(National)。 下标 含义 样例 critical/optional 1 Record Type: 记录类型(S表示站点) S c 2-3 FIPS State Codes: 州编号 02 c 4-9 Station Identification: 站标识符 000101 c 10 Direction of Travel Code: 路的走向 1 c 11 Lane of Travel: 哪条路 1 c 12-13 Year of Data: 年 16 c 14-15 Functional Classification Code: 功能类型 1R o 16 Number of Lanes in Direction Indicated: 这个方向几条路 1 o 17 Sample Type for Traffic Volume: 是否用于监测流量 T o 18 Number of Lanes Monitored for Traffic Volume: 几条路监测流量 1 o 19 Method of Traffic Volume Counting: 流量监测方法 3 o 20 Sample Type for Vehicle Classification: 车辆分类方法 o 21 Number of Lanes Monitored for Vehicle Classification: 几条路对车辆分类 0 o 22 Method of Vehicle Classification: 车辆分类方法 0 o 23 Algorithm for Vehicle Classification: 车辆分类算法 o 24-25 Classification System for Vehicle Classification: 车辆分类系统 13 o 26 Sample Type for Truck Weight: 称重类型 o 27 Number of Lanes Monitored for Truck Weight: 几条路称重 0 o 28 Method of Truck Weighing: 称重方法 0 o 29 Calibration of Weighing System: 称重系统精度 o 30 Method of Data Retrieval: 数据获得的类型 2 o 31 Type of Sensor: 检测器类型 P o 32 Second Type of Sensor: 检测器的第二种类型 L o 33 Primary Purpose: 安装检测器的意图 P o 34-45 LRS Identification 001700000000 o 46-51 LRS Location Point 81967 o 52-59 Latitude: 纬度 62351650 o 60-68 Longitude: 经度 150252360 o 69-72 SHRP Site Identification o 73-78 Previous Station ID o 79-80 Year Station Established: 哪年建的站点 91 o 81-82 Year Station Discontinued 00 o 83-85 FIPS County Code 170 o 86 HPMS Sample Type Y o 87-98 HPMS Sample Identifier 170000008007 o 99 National Highway System Y o 100 Posted Route Signing 3 o 101-108 Posted Signed Route Number 00000003 o 109 Concurrent Route Signing 0 o 110-117 Concurrent Signed Route Number o 118-167 Station Location PARKS HIGHWAY AT CHULITNA - NB o 数据样例：S0200010111161R1T13 00 13 00 2PLP001700000000 8196762351650150252360 9100170Y170000008007Y3000000030 PARKS HIGHWAY AT CHULITNA - NB (后面有很多空格) 流量数据格式2016年数据的文件名：AK_JAN_2016 (TMAS).VOL12个字段 下标 描述 样例 critical/optional 1 Record Type: 记录类型 3 c 2-3 FIPS State Code 02 c 4-5 Functional Classification Code 1R c 6-11 Station Identification 000101 c 12 Direction of Travel Code 1 c 13 Lane of Travel 1 c 14-15 Year of Data 16 c 16-17 Month of Data 01 c 18-19 Day of Data 01 c 20 Day of Week(1表示周日) 6 o 21-25,…136-140 Traffic Volume Counted Fields(从00:00到24:00，24个小时的流量) 00005 00004 00002 … 00003 o 141 Restrictions(1表示施工或活动影响流量，2表示检测器出现问题，0表示正常) 0 o 数据样例：3021R0001011116010160000500004000020000000001000010000200001000150003100026000430003200052000340002800024000140001400007000120000800007000030不知道为什么这行数据在浏览器里面直接出去了。。。 关于车辆分类和称重数据就不描述了，可以直接看官方文档。","link":"/blog/2018/07/12/fhwa数据/"},{"title":"Dynamic Bike Reposition: A Spatio-Temporal Reinforcement Learning Approach","text":"KDD 2018.强化学习处理共享单车调度问题。 ABSTRACT共享单车系统在很多城市都开始使用了，尽管拥挤和空的站点都会导致客户的流失。当前，运营者试图在系统运行中不断地在站点间调度车辆。然而，如何在一个长时间的范围内有效地调度自行车使得乘客的流失最少还没有得到解决。我们提出了一个基于时空强化学习方法的自行车调度模型解决这个问题。首先，一个互相依赖且内部平衡的聚类算法用来对站点聚类。类簇有两点属性，每个类簇类内平衡而且类间独立。因为在很大的系统中有很多三轮车调整自行车，聚类对于减少问题的复杂度来说很重要。其次，我们给每个类簇分配了多辆三轮车，对类内自行车进行调度。我们设计了一个时空强化学习模型对每个类簇进行策略的学习，目标是在长时间范围内降低用户的流失。为了学习每个模型，我们设计了一个深度神经网络来估计它的最优长期价值函数，最优策略可以从这个函数中轻松地推导出来。除了将模型定义成多智能体的方式，我们还通过两个时空剪枝规则减少了训练的复杂度。其三，我们基于两个预测其设计了一个系统模拟器来预测训练和评估调度模型。在Citi Bike的真实数据集上的实验验证了我们的模型的有效性。 1 INTRODUCTION共享单车系统给公民提供了便利的出行方式。用户可以在一个随机的站点租或返还自行车，通过刷卡，生成一条单车使用记录。然而，因为在一个城市内的单车使用是非常不平衡的，系统中经常出现没有车的空站点以及缺少可用停车位的拥挤站点，导致乘客的流失。当前，系统运营者使用 dynamic bike reposition 来处理这个问题，也就是，在系统运行中使用三轮车不断的在站点间调整车辆。然而，如何在长时间范围内调整车辆使得顾客流失最少还是一个问题。实时监测并不是一个好的方案，因为在观测到不平衡后对车辆重新分配太晚了。仅仅基于单车使用预测来调度只会导致一个贪心且短视的策略，在长时间来看不会是最优的。我们总结了解决这个问题的三个挑战。 A bike-sharing system is complex and dynamic. 系统中经常有几十辆三轮车在几百个站点间调度车辆。在这么一个大的系统内协同调度很复杂，更不用说系统是在运行中保持动态的情况了。难以预测系统的动态性有三点原因：1) 图1. A 展示出了一个月每个小时的租车需求。可以看到，每天的租用模式变化得很剧烈，受多个复杂的因素影响，比如天气、事件以及站点间的相关性。2) 很多移动看起来是随机的。图1. B的第一张图表示出了历史通勤，也就是2016年4月到10月每个工作日的早上这段期间，平均至少发生一次的移动，只占了18%。我们用图1. C的例子解释了这种现象：从A到B，站点间的移动有12种可能，用户一般会根据哪个站点有可用的车辆或车位，选择随机的一条路线，使12条路线中的一条变成可能的频繁移动路线。3) 影响车辆使用的外部因素非常不平衡，比如，晴天时长要比雨天时长多很多。因此，在每个条件下训练一个学习器不能保证在次要条件下的准确性。 A single bike reposition has long-term effect. 一个简单的调度是好是坏不是那么简单就能判断的。我们将用图2的两个例子详细描述一下，红圈表示没有可用车位的站点，绿圈表示一个空的站点；带有一个数字和一个时间标记的实线箭头表示在那个时段会有多少辆车从起点租用并返还到目的地；虚线箭头表示了一个三轮车的调度是如何进行的；$t_0 &lt; t_1 &lt; t_2$。首先，一个简单的调度会影响系统内的车辆使用很长的一段时间。如图2. A) 所示，如果一个三轮车到了空站点 $s_1$，在 $t_0$ 时段放了5辆车，在 $s_1$ 的可用车辆就变成了$5$，在 $t_1$ 时段 $s_1$ 可以服务 $5$ 位即将到来的租车者；这5个租车者骑车到 $s_2$ 在 $t_1 &lt; t_2$ 还车，4位到来的租车者在 $t_2$ 时段 $s_2$ 车站想租车的也可以使用那些还回来的车。因此，一个调度可以服务的额外用户的数量很难估计。其次，当前的调度会影响以下情况。如图2. B) 所示，如果一个三轮车到站点 $s_1$ 在 $t_0$ 时段带走了 9 辆自行车，那里的可用车位就变成了 9，因此9个用户就可以在 $t_1$ 时段把他们的自行车还到 $s_1$。然而，因为 $s_1$ 离 $s_3$ 太远，在完成picking up之后，这辆三轮车不能在 $t_2$ 时段之前将5辆自行车运送到空站点 $s_3$，来服务5位即将到来的租客。 Uncertainties in partical reposition. 在实际的调度过程中有很多不确定因素。尽管我们可以预测系统的动态，我们不能保证预测与实际完全一样，因为模型会有错误以及随机噪声。此外，完成一次调度的时间也是不同的，比如，从 $s_1$ 运送车辆到 $s_2$ 今天可能需要10分钟，明天可能就要15分钟，尽管方法可能一样。这可能是由于变化的外部因素导致，比如，恶劣的天气状况以及交通的拥挤程度，或是随机噪声。因为动态调度是在系统运行的时候工作的，时间很重要，这也可以从上面两个例子看出来。这些不确定因素，还有长期影响，使得优化模型非常复杂，甚至是无法工作。 我们提出了一个基于时空强化学习的动态调度模型来解决这三个挑战。我们的贡献可以归纳为4点： 我们提出了一个两步聚类算法，称为 Inter-Independent Inner-Balance算法，简称 IIIB。这个算法首先在系统中迭代地将独立的站点聚类生成小的功能区域，保证每个区域更稳定的租车需求以及车辆移动模式。其次，这个算法根据区域间的移动，将这些区域聚类成组，保证每个类簇是类内平衡且类间独立的。将整个系统分为各个类簇，极大地减少了问题的复杂程度。 我们基于两个预测器生成了一个系统模拟器。一个是O-Model，通过一个基于相似度的KNN模型预测每个区域的租车需求，考虑复杂的影响因素，解决了不平衡样本的问题。另一个模型是I-Model，通过一个基于车辆移动的推断方法，预测每个区域的还车需求。 我们提出了一个时空强化学习模型，STRL，为每个类簇学习一个最优的类内调度策略。STRL的状态通过捕获系统动态性以及实时的不确定性仔细地设计出来。因为状态以及行动空间很大，我们设计了一个深度神经网络为每个STRL估计最优的长期价值函数，通过这个可以推断出它的最优调度策略。除了将模型定义成多智能体的方式，我们还通过两个时空剪枝规则减少了训练的复杂度。 我们在Citi Biki 2016年4月到10月的数据集上证明了我们的模型比baselines更有效。 2 OVERVIEW这部分定义了符号以及相关术语 2.1 PreliminaryDefinition 1 Transition. 一个移动 $f_{ij} = (s_i, s_j, \\tau_i, \\tau_j)$ 是一辆自行车的使用记录，描述了一辆自行车从地点 $S_i$ 在时刻 $\\tau_i$ 被租用以及在地点 $s_j$ 时刻 $\\tau_j$ 返还。 Definition 2 Demand. 时段 $t$ 的地点 $s_i$ 的租用需求 $o_{i,t}$ 是想在 $s_i$ 时段 $t$ 内租用自行车的顾客的个数，包括成功以及失败的。时段 $t$ 内地点 $s_i$ 的返还需求 $r_{i,t}$的定义类似。 Definition 3 Episode. 一个时段 $E$ 是一天的一个长时段，在这个时段中，我们想最小化总的客户流失。我们在3.1.2中详细地定义了我们问题中的 Episodes，保证了一些约束，而不是随机选取的。 2.2 Framework如图3所示，我们的模型包括了一个离线学习过程以及一个在线调度过程。这个学习过程有三个部分，分别是IIIB聚类算法，系统模拟器生成过程以及每个类簇的STRL模型。 IIIB Clustering Algorithm. 为了解决第一个问题，也就是一个系统很大且很复杂，我们提出了一个两步IIIB聚类算法。首先对那些相近且有相似移动模式的站点聚类，在系统中生成小的功能区域。然后基于他们区域间的迁移规律，将区域聚类成组。给每个类簇分配多辆三轮车完成类簇内区域间的调度，而不是类间的自行车调度。 Simulator Generation. 为了训练和评估调度模型，我们基于两个预测器生成了一个系统模拟器，也就是分别预测每个区域的租车需求和还车需求的O-Model和I-Model。对于一个指定的时段，比如周六早上7点到7点半，我们先根据历史的天气统计结果生成一个可能的天气状况，比如晴天，然后O-Model预测每个区域周六早上7点到7点半晴天的租车需求。基于这些预测，每个区域的租车需求由泊松过程模拟。每次一辆自行车被租用，I-Model就会估计它的目的地区域以及到达时间，并且追踪它。每个区域的还车事件通过连续地检查自行车是否到达那里来生成。 STRL Model. 我们提出了一个STRL模型，对每个类簇学习一个最优的类内调度方案。我们这个基于强化学习的模型是多智能体形式。每次一个三轮车完成它最后的调度，它会不等待其他调度的完成，紧接着开始一个由方案生成的新的调度。新的调度基于当前状态生成，这个调度会很仔细地定义，来捕获系统动态性和实时的不确定性。一个状态包含多个因素，如，每个区域当前自行车和车库的可用性；实时预测的租车和还车需求；三轮车的状态，包含它自身的以及其他的；当前的时间，等等。我们设计了一个深度神经网络为每个STRL估计最优的长期价值函数，通过这个我们能推导出最优的调度策略。网络在系统模拟器迭代地训练出来，在图3中用灰色高亮了出来。 Online Reposition. 在学习过程之后，我们在每个类簇上会获得一个神经网络。在在线过程中，当一个三轮车需要一个新的调度时，我们先确定它在哪个类簇中，通过O模型和I模型生成它的当前状态。然后，对应的网络给当前状态中每个可能的调度估计最优的长期价值。选择最大价值的调度并返回。 3 METHODOLOGY3.1 IIIB Clustering Algorithm3.1.1 Region Generation如图1. C)所示的例子，站点间的随机迁移使得车站间的单车调度不是那么有意义，因为我们只需要保证在 $s_1$ 或 $s_2$ 或 $s_3$ 有可用的自行车，在 $s_4$ 或 $s_5$ 或 $s_6$ 或 $s_7$ 有可用的车位。考虑每个站点的车辆和车位的可用性，从 $A$ 到 $B$ 的用户可以选择在哪里租车并且还车。受到这个现象的启发，我们对这个区域周围的几个车站聚类，对目标区域周围的车站进行聚类，生成两个小的区域，也就是 $s_1$，$s_2$ 和 $s_3$ 形成了一个区域，$s_4$，$s_5$，$s_6$ 和 $s_7$ 形成了另一个区域。然后，我们只需要保证每个区域车辆和车位的可用性即可。我们认为一个区域的租车需求比一个单独的车站更稳定，更规律；而且，两个区域间的迁移比两个站点间的迁移更频繁。 为了正式地阐述这个想法，我们基于两个约束在一个系统中生成了一些区域。1) 一个区域的站点应该和其他的站点相近，保证这个区域内顾客的方便。2) 一个区域内的站点应该有相似的OD区域，使得区域间的迁移更专一且更频繁。生成这些的区域的方法是一个迭代的方法，称为二部聚类算法[2]，这个算法会基于车站的位置和迁移模式进行聚类。 基于获得的这些区域，我们分析了 Citi Bike 中历史的车辆使用数据，证实了上述的两个优势。如图1. A)底部所示，一个区域的租车需求更稳定，更规律，因此更容易地精确预测。随机迁移的问题也可用得到解决。图1. B)右图展示了2016年4月到10月工作日的早上区域间的通勤，占了56%。可以看到，得到的区域间迁移模式更简单，使得还车需求预测更简单且更精确。这里获得的区域可用看作是城市中小的功能区，比如居民区周围的车站很可能组成一个区域，而在工作区周围的可能形成另一个。我们对这些区域聚类成组，而不是在整个系统中的这些屈居间直接调度，而且因为两个原因，我们只在类簇内开展调度。1) 聚类可用进一步减少问题的复杂度。2) 司机一般只对城市内的一个区域比较熟悉。 3.1.2 IIIB Clustering Insight获得到的类簇应该有两个性质，每个类簇的内部平衡以及类簇间的相互依赖。 Inner-Balance.","link":"/blog/2018/10/08/dynamic-bike-reposition-a-spatio-temporal-reinforcement-learning-approach/"},{"title":"Geometric deep learning on graphs and manifolds using mixture model CNNs","text":"CVPR 2017. 这篇论文有点难，没看下去。。。原文链接：Geometric deep learning on graphs and manifolds using mixture model CNNs Abstract大部分深度学习处理的是 1D，2D，3D 欧式结构数据，音频信号、图像、视频。最近大家开始研究在非欧氏空间上的数据，如复杂网络、计算社会科学、计算机图形学。我们提出了一个统一的框架，让 CNN 可以泛化到非欧氏空间上，学习局部的、平稳的、针对任务可分解的特征。我们发现之前提出的一些方法都可以放到我们的框架中。我们发现我们的方法效果比前人的方法都要好。 1. Introduction2. Deep learning on graphs无向带权图 $\\mathcal{G} = (\\lbrace 1, \\dots, n\\rbrace, \\mathcal{E}, \\mathbf{W})$，邻接矩阵 $\\mathbf{W} = (w_{ij})$，其中 $w_{ij} = w_{ji}$，如果 $(i, j) \\notin \\mathcal{E}$，则 $w_{ij} = 0$，否则 $w_{ij} &gt; 0$。未归一化的拉普拉斯矩阵是个 $n \\times n$ 的 实对称半正定矩阵 $\\Delta = \\bf D - W$，其中 $\\mathbf{D} = \\text{diag}(\\sum_{j = \\not i} w_{ij})$ 是度矩阵。 拉普拉斯矩阵有特征值分解 $\\bf \\Delta = \\Phi \\Lambda \\Phi^T$，其中 $\\Phi = (\\phi_1, \\dots, \\phi_n)$ 是相互正交的特征向量，$\\Lambda = \\text{diag}(\\lambda_1, …, \\lambda_n)$ 特征值组成的对角矩阵。在传统的谐波分析中，特征向量是拉普拉斯算子，特征值可以看作是频率。给定图上的一个信号 $\\mathbf{f} = (f_1, \\dots, f_n)^T$，它的图傅里叶变换是 $\\hat{\\mathbf{f}} = \\Phi^T \\mathbf{f}$。给定两个信号 $\\bf f, g$，他们的谱卷积定义为傅里叶变换的 element-wise product： $$\\tag{1}\\mathbf{f} \\star \\mathbf{g} = \\Phi (\\Phi^T \\mathbf{f}) \\odot (\\Phi^T g) = \\Phi \\ \\text{diag}(\\hat{g}_1, \\dots, \\hat{g}_n) \\hat{f},$$ 对应了欧氏空间卷积理论。 其实这里我没理解啊，我记得卷积的定义不是傅里叶变换的乘积的逆变换吗，所以感觉说的有点不对，但公式倒是对了。。。 Spectral CNN. Bruna et al. 使用卷积在谱上的定义将 CNN 泛化到图上，得到一个谱卷积层的定义： $$\\tag{2}\\mathbf{f}^{out}_l = \\xi (\\sum^p_{l’=1} \\Phi_k \\hat{G}_{l,l’} \\Phi^T_k \\mathbf{f}^{in}_{l’})$$ 这里维数为 $n \\times p$ 和 $n \\times q$ 的矩阵 $\\mathbf{F}^{in} = (\\mathbf{f}^{in}_1, \\dots, \\mathbf{f}^{in}_p)$，$\\mathbf{F}^{out} = (\\mathbf{f}^{out}_1, \\dots, \\mathbf{f}^{out}_q)$ 分别表示 $p$ 维和 $q$ 维的图上的输入和输出信号，$\\Phi = (\\phi_1, \\dots, \\phi_k)$ 是前几个特征向量组成的 $n \\times k$ 的矩阵，$\\hat{\\mathbf{G}_{l,l’}} = \\text{diag}(\\hat{g}_{l,l’,1}, \\dots, \\hat{g}_{l,l’,k})$ 是一个 $k \\times k$ 的对角矩阵，表示频域内一个可学习的滤波器，$\\xi$ 是一个非线性激活单元（e.g. ReLU）。这个框架的池化操作在图上的模拟是一个图的缩减操作，给定一个 $n$ 个结点的图，生成一个 $n’ &lt; n$ 个结点的图，将信号从原来的图上变换到缩减后的图上。 这个框架有几个缺点。首先，谱滤波器的系数是 basis dependent，而且，在一个图上学习到的基于谱的 CNN 模型不能应用在其他的图上。其次，图傅里叶变换的计算因为 $\\bf \\Phi$ 和 $\\bf \\Phi^T$ 的乘法，会达到 $\\mathcal{O}(n^2)$，因为这里没有像 FFT 一样的算法。第三，不能保证在谱域内的滤波器在顶点域上是局部化的；假设使用 $k = O(n)$ 个归一化的拉普拉斯矩阵的特征向量，一个谱卷积层需要 $pqk = O(n)$ 个参数。 Smooth Spectral CNN. 之后，Henaff et al. 认为 smooth 谱滤波器系数可以使得卷积核在空间上局部化，使用了这个形式： $$\\tag{3}\\hat{g}_i = \\sum^r_{j=1} \\alpha_i \\beta_j (\\Lambda_i)$$ 其中 $\\beta_1(\\lambda), \\dots, \\beta_r(\\lambda)$ 是一些固定的插值核，$\\mathbb{\\alpha} = (\\alpha_1, \\dots, \\alpha_r)$ 是插值系数。矩阵形式中，滤波器写为 $\\text{diag}(\\hat{G}) = \\mathbf{B\\alpha}$，其中 $\\bf{B} = (b_{ij}) = (\\beta_j (\\lambda_i))$ 是一个 $k \\times r$ 的矩阵。这样一个参数化可以使参数保持在 $n$ 个。 Chebyshev Spectral CNN (ChebNet). 为了减轻计算图傅里叶变换的代价，Defferrard et al 使用了切比雪夫多项式来表示卷积核： $$\\tag{4}g_\\alpha(\\Delta) = \\sum^{r-1}_{j=0} \\alpha_j T_j(\\tilde{\\Delta}) = \\sum^{r-1}_{j=0} \\alpha_j \\Phi T_j (\\tilde{\\Lambda}) \\Phi^T,$$ 其中 $\\tilde{\\Delta} = 2 \\lambda^{-1}_n \\Delta - \\bf I$ 是 rescaled 拉普拉斯矩阵，它的特征值 $\\tilde{\\Lambda} = 2 \\lambda^{-1}_n \\Lambda - \\bf I$ 在区间 $[-1, 1]$ 内，$\\alpha$ 是 $r$ 维的滤波器中的多项式系数， $$\\tag{5}T_j(\\lambda) = 2 \\lambda T_{j-1}(\\lambda) - T_{j-2} (\\lambda),$$ 表示 $j$ 阶切比雪夫多项式，$T_1(\\lambda) = \\lambda$，$T_0(\\lambda) = 1$。 这样的方法有几个优点。首先，它不需要计算拉普拉斯矩阵的特征向量。由于切比雪夫多项式的递归定义，计算滤波器 $g_\\alpha(\\Lambda) \\bf f$ 要使用拉普拉斯矩阵 $r$ 次，会导致一个 $\\mathcal{O}(rn)$ 的操作。其次，因为拉普拉斯矩阵是一个局部操作，只影响顶点的一阶邻居，它的 $(r-1)$次幂影响 $r$阶邻居，得到的滤波器是局部化的。","link":"/blog/2018/12/18/geometric-deep-learning-on-graphs-and-manifolds-using-mixture-model-cnns/"},{"title":"github大文件上传","text":"刚才开源了我们组在AAAI 2020上一篇论文的代码和数据，上传数据的时候超了GitHub的100M大小限制，GitHub说让我用lfs解决，研究了一下怎么传，记录一下，以后说不定还会用到。 1234567891011121314151617git lfs installgit lfs track &quot;data.tar.gz&quot;git add .gitattributesgit commit -m &quot;Updated attributes&quot;git pushgit add data.tar.gzgit lfs ls-filesgit commit -m &quot;Add file&quot;git push 原理还不太懂，这几天太忙了，过几天看看。","link":"/blog/2019/12/20/github大文件上传/"},{"title":"Graph Attention Networks","text":"ICLR 2018。图注意力网络，使用 self-attention 来构建 graph attentional layer，attention 会考虑当前顶点所有的邻居对它的重要性，基于谱图理论的模型不能应用到其他不同结构的图上，而这个基于attention的方法能有效的解决这个问题。原文链接：Graph Attention Networks 摘要我们提出了图注意力网络(GAT)，新型图神经网络，利用自注意力层解决基于图卷积及其相似方法的缺点。通过堆叠这种层（层中的顶点会注意邻居的特征），我们可以给邻居中的顶点指定不同的权重，不需要任何一种耗时的矩阵操作（比如求逆）或依赖图结构的先验知识。我们同时解决了基于谱的图神经网络的几个关键挑战，并且使我们的模型很轻松的应用在 inductive 或 transductive 问题上。我们的 GAT 模型在4个 transductive 和 inductive 图数据集上达到且匹敌当前最先进的算法：Cora, Citeseer, Pubmed citation networks，protein-protin interaction。 1 INTRODUCTIONCNN 结构可以有效的重复使用卷积核，在网格型的数据上应用。然而很多问题都是基于图结构的。 早期的工作使用RNN来处理图结构中数据的表示。2005 年和 2009 年提出了 GNN（Graph Neural Networks）的概念，作为 RNN 的泛化，可以直接处理更一般的图结构，比如带环图、有向、无向图。GCN 包含了迭代的过程，迭代时顶点状态向前传播，后面使用一个神经网络来产生输出。Li et al., 2016使用了Cho et al., 2014提出的门控循环单元进行改进。 进一步的研究分为谱方法和非谱方法。 一方面，谱方法使用图的谱表示，成功应用到了顶点分类的问题上。Bruna et al., 2014在傅里叶域中定义了卷积操作，通过计算拉普拉斯矩阵的特征值分解，由于潜在的大量的计算，引出了后续的非谱方法。Henaff et al., 2015引入了带有 smooth coefficients 的谱滤波器，可以使他们在空间局部化。后来 Defferrard et al., 2016 提出了通过拉普拉斯矩阵的切比雪夫多项式展开的近似表达。最后，Kipf &amp; Welling 2017 通过限制滤波器只考虑 1 阶邻居从而简化了之前的方法。然而，前面提到的所有的谱方法，学习到的卷积核参数都依赖于拉普拉斯特征值分解后的特征向量，也就是说依赖于图结构。因此，训练在一个指定图结构的模型不能应用到不同结构的图上。 另一方面，还有一些非谱方法 (Duvenaud et al., 2015; Atwood &amp; Towsley, 2016; Hamilton et al., 2017)，这些方法直接在图上定义卷积操作，直接在空间上相近的邻居上应用卷积操作。这些方法的一个挑战是需要定义一个能处理不同数量邻居的卷积操作，并且保证 CNN 权重共享的性质。在某些情况下，这需要学习为每个 node degree 学习一个权重矩阵 (Duvenaud et al., 2015)，在对每个 input channel 和 neighborhood degree 训练权重时，使用转移矩阵的幂定义邻居 (Atwood &amp; Towsley, 2016)，或是对有着固定数量顶点的邻居进行提取和归一化(Niepert et al., 2016)。Monti et al., 2016提出了混合模型 CNN，(MoNet)，这个空间方法提供了一个 CNN 在图上泛化的统一的模型。最近，Hamilton et al., 2017提出了 GraphSAGE，对每个顶点采样采出一个固定数量的邻居，然后使用一个指定的聚合操作聚集他们（比如取所有采样邻居的均值，或是将他们放进RNN后产生的结果）。这个方法在几个大规模的 inductive 数据集上获得了很惊人的效果。 注意力机制在很多基于序列的任务中已经成为了一个标准 (Bahdanau et al., 2015; Gehring et al., 2016)。注意力机制的一个好处是可以处理变长输入，专注于输入中最相关的部分来做决策。使用注意力机制计算一个序列的表示时，一般提到的是 self-attention 或 intra-attention。与 RNN 或卷积一起使用时，self-attention 在机器阅读(Cheng et al., 2016)和学习句子表示(Lin et al., 2017)这些任务上很有用。然而，Vaswani et al., 2017的研究表明，self-attention 不仅可以提升 RNN 和卷积的模型，在机器翻译任务上也是可以构建出性能最强的模型的。 受最近工作的启发，我们提出了基于 attention 的架构对图结构的顶点进行分类。思路是通过对顶点邻居的注意，计算图中每个顶点的表示，然后使用一个 self-attention 机制。注意力架构有几个有趣的性质：(1) 操作高效，因为它可以“顶点-邻居”对上并行计算；(2) 通过指定对邻居任意的权重，它可以在有着不同度的顶点上使用；(3) 模型可以直接应用在 inductive learning 任务上，包括模型必须要生成完整的未见过的图等任务。我们在4个 benchmark 上验证了我们的方法：Cora，Citeseer，Pubmed citation networks，protein-protein interaction，获得了比肩 state-of-the-art 的结果，展现了基于 attention 的模型在处理任意结构的图的可能性。 值得注意的是，如 Kipf &amp; Welling 2017和Atwood &amp; Towsley 2016，我们的工作可以重写为 MoNet(Monti et al., 2016)的一种特殊形式。除此以外，我们的分享神经网络跨边计算时是对关系网络公式的联想(Santoro et al., 2017)和VAIN(Hoshen, 2017)，在这两篇文章中，object 和 agent 间的关系被聚合成对，通过使用一种共享机制。相似地，我们的注意力模型可以与 Duan et al., 2017 和 Denil et al., 2017 的工作相连，他们使用一个邻居注意力操作来计算环境中不同 object 的注意力系数。其他相关的方法包括局部线性嵌入(LLE)(Roweis &amp; Saul, 2000)和记忆网络(Weston et al., 2014)。LLE 在每个数据点选择了固定数量的邻居，为每个邻居学习了权重系数，以此将每个数据点重构为邻居的加权之和。之后的优化提取了顶点嵌入的特征。记忆网络与我们的工作也有关系，如果我们将一个顶点的邻居解释为记忆，通过注意它的值可以计算顶点特征，之后通过在同样的位置存储新特征进行更新。 2 GAT ARCHITECTURE我们会在这部分描述如何创建 block layer 来构造任意的 graph attention networks，并且指明理论和实际上的优点以及相比于之前在神经图处理上的工作的缺点。 2.1 Graph Attentional Layer首先描述单个 graph attentional layer，因为这种层会在整个 GAT 架构中使用。我们使用的 attention 和 Bahdanau et al., 2015 的工作相似。 输入是一组顶点特征，${\\mathbf{h}} = \\lbrace \\vec{h}_1, \\vec{h}_2, …, \\vec{h}_N \\rbrace , \\vec{h}_i \\in \\mathbb{R}^F$，其中 $N$ 是顶点数，$F$ 是每个顶点的特征数。这个层会生成一组新的顶点特征，${\\mathbf{h}’} = \\lbrace \\vec{h}’_1, \\vec{h}’_2, …, \\vec{h}’_N\\rbrace , \\vec{h}’_i \\in \\mathbb{R}^{F’}$，作为输出。 为了在将输入特征变换到高维特征时获得充足的表现力，至少需要一个可学习的线性变换。为了到达这个目的，每个顶点都会使用一个共享参数的线性变换，参数为 ${\\mathbf{W}} \\in \\mathbb{R}^{F’ \\times F}$。然后在每个顶点上做一个 self-attention ——一个共享的attention机制 $a : \\mathbb{R}^{F’} \\times \\mathbb{R}^{F’} \\rightarrow \\mathbb{R}$ 来计算注意力分数 attention coefficients： $$\\tag{1}e_{ij} = a(\\mathbf{W} \\vec{h}_i, \\mathbf{W} \\vec{h}_j)$$ 表示顶点 $j$ 的特征对顶点 $i$ 的重要性(importance)。在一般的公式中，模型可以使每个顶点都注意其他的每个顶点，扔掉所有的结构信息。我们使用 mask attention 使得图结构可以注入到注意力机制中——我们只对顶点 $j \\in \\mathcal{N_i}$ 计算$e_{ij}$，其中$\\mathcal{N_i}$ 是顶点 $i$ 在图中的一些邻居。在我们所有的实验中，这些是 $i$ 的一阶邻居（包括 $i$ ）。为了让系数在不同的顶点都可比，我们对所有的 $j$ 使用 softmax 进行了归一化： $$\\tag{2}\\alpha_{ij} = \\mathrm{softmax}_j (e_{ij}) = \\frac{\\exp{e_{ij}}}{\\sum_{k \\in \\mathcal{N}_i} \\exp{e_{ik}}}$$ 在我们的实验中，注意力机制 $a$ 是一个单层的前向传播网络，参数为权重向量 $\\vec{\\text{a}} \\in \\mathbb{R}^{2F’}$，使用LeakyReLU作为非线性层（斜率$\\alpha = 0.2$）。整个合并起来，注意力机制计算出的分数（如图1左侧所示）表示为： $$\\tag{3}\\alpha_{ij} = \\frac{ \\exp{ ( \\mathrm{LeakyReLU} ( \\vec{\\text{a}}^T [\\mathbf{W} \\vec{h}_i \\Vert \\mathbf{W} \\vec{h}_j ] ))}}{\\sum_{k \\in \\mathcal{N_i}} \\exp{(\\mathrm{LeakyReLU}(\\vec{\\text{a}}^T [\\mathbf{W} \\vec{h}_i \\Vert \\mathbf{W} \\vec{h}_k]))}}$$ 其中 $·^T$ 表示转置，$\\Vert$ 表示concatenation操作。 得到归一化的分数后，使用归一化的分数计算对应特征的线性组合，作为每个顶点最后的输出特征（最后可以加一个非线性层，$\\sigma$）： $$\\tag{4}\\vec{h}’_i = \\sigma(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} \\mathbf{W} \\vec{h}_j)$$ 为了稳定 self-attention 的学习过程，我们发现使用 multi-head attention 来扩展我们的注意力机制是很有效的，就像 Vaswani et al., 2017。特别地，$K$ 个独立的 attention 机制执行 式4 这样的变换，然后他们的特征连(concatednated)在一起，就可以得到如下的输出： $$\\tag{5}\\vec{h}’_i = \\Vert^{K}_{k=1} \\sigma(\\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\mathbf{W}^k \\vec{h}_j)$$ 其中 $\\Vert$ 表示concatenation，$\\alpha^k_{ij}$ 是通过第 $k$ 个注意力机制 $(a^k)$ 计算出的归一化的注意力分数，$\\mathbf{W}^k$ 是对应的输入线性变换的权重矩阵。注意，在这里，最后的返回输出 $\\mathbf{h}’$，每个顶点都会有 $KF’$ 个特征（不是 $F’$ ）。特别地，如果我们在网络的最后一层使用 multi-head attention，concatenation 就不再可行了，我们会使用 averaging，并且延迟使用最后的非线性层（分类问题通常是 softmax 或 sigmoid ）： $$\\vec{h}’_i = \\sigma(\\frac{1}{K} \\sum^K_{k=1} \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\mathbf{W}^k \\vec{h}_j)$$ multi-head 图注意力层的聚合过程如图1右侧所示。 2.2 Comparisons to related work2.1节描述的图注意力层直接解决了之前在图结构上使用神经网络建模的方法的几个问题：· 计算高效：self-attention层的操作可以在所有的边上并行，输出特征的计算可以在所有顶点上并行。没有耗时的特征值分解。单个的GAT计算$F’$个特征的时间复杂度可以压缩至$O(\\vert V \\vert F F’ + \\vert E \\vert F’)$，$F$是输入的特征数，$\\vert V \\vert$和$\\vert E \\vert$是图中顶点数和边数。复杂度与Kipf &amp; Welling, 2017的GCN差不多。尽管使用multi-head attention可以并行计算，但也使得参数和空间复杂度变成了$K$倍。· 对比GCN，我们的模型允许对顶点的同一个邻居分配不同的重要度，使得模型能力上有一个飞跃。不仅如此，对学习到的attentional权重进行分析可以得到更好的解释性，就像机器翻译领域一样（比如Bahdanau et al., 2015的定性分析）。· 注意力机制以一种共享的策略应用在图的所有的边上，因此它并不需要在之前就需要得到整个图结构或是所有的顶点的特征（很多之前的方法的缺陷）。因此这个方法有几个影响： 图不需要是无向的（如果边$j \\rightarrow i$没有出现，我们可以直接抛弃掉$\\alpha_{ij}$的计算） 这个方法可以直接应用到inductive learning——包括在训练过程中在完全未见过的图上评估模型的任务上。最近发表的Hamilton et al., 2017的inductive方法为了保持计算过程的一致性，对每个顶点采样采了一个固定数量的邻居；这就使得这个方法在推断的时候不能考虑所有的邻居。此外，在使用一个基于LSTM的邻居聚合方式时（Hochreiter &amp; Schmidhuber, 1997），这个方法在某些时候能达到最好的效果。这意味着存在邻居间存在一个一致的顶点序列顺序，作者通过将随机顺序的序列输入至LSTM来验证它的一致性。我们的方法不会受到这些问题中任意一个的影响——它在所有的邻居上运算（虽说会有计算上的开销，但扔能和GCN这样的速度差不多），并且假设任意顺序都可以。· 如第一节提到的，GAT可以重写成MoNet(Monti et al., 2016)的一种特殊形式。更具体的来说，设pseudo-coordinate function为$u(x, y) = f(x) \\Vert f(y)$，$f(x)$表示$x$的特征（可能是MLP变换后的结果），$\\Vert$表示concatenation；权重函数为$w_j(u) = \\mathrm{softmax}(\\mathrm{MLP}(u))$（softmax在一个顶点所有的邻居上计算）会使MoNet的patch operator和我们的很相似。尽管如此，需要注意到的是，对比之前MoNet的实例，我们的模型使用顶点特征计算相似性，而不是顶点的结构性质（这需要之前就已经直到图结构）。 我们可以做出一种使用稀疏矩阵操作的GAT层，将空间复杂度降低到顶点和边数的线性级别，使得GAT模型可以在更大的图数据集上运行。然而，我们使用的tensor操作框架只支持二阶tensor的稀疏矩阵乘法，限制了当前实现的版本的模型能力（尤其在有多个图的数据集上）。解决这个问题是未来的一个重要研究方向。在这些使用稀疏矩阵的场景下，在某些图结构下GPU的运算并不能比CPU快多少。另一个需要注意的地方是我们的模型的感受野的大小的上届取决于网络的深度（与GCN和其他模型相似）。像skip connections(He et al., 2016)这样的技术可以来近似的扩展模型的深度。最后，在所有边上的并行计算，尤其是分布式的计算可以设计很多冗余的计算，因为图中的邻居往往高度重叠。 3 Evaluation我们与很多强力的模型进行了对比，在四个基于图的数据集上，达到了state-of-the-art的效果。这部分将总结一下我们的实验过程与结果，并对GAT提取特征表示做一个定性分析。 3.1 DatasetsTransductive learning 我们使用了三个标准的citation network benchmark数据集——Cora, Citeseer和Pubmed(Sen et al., 2008)——并按Yang et al., 2016做的transductive实验。这些数据集中，顶点表示文章，边（无向）表示引用。顶点特征表示文章的BOW特征。每个顶点有一个类标签。我们使用每类20个顶点用来训练，训练算法使用所有的顶点特征。模型的预测性能是在1000个测试顶点上进行评估的，我们使用了500个额外的顶点来验证意图（像Kipf &amp; Welling 2017）。Cora数据集包含了2708个顶点，5429条边，7个类别，每个顶点1433个特征。Citeseer包含3327个顶点，4732条边，6类，每个顶点3703个特征。Pubmed数据集包含19717个顶点，44338条边，3类，每个顶点500个特征。 Inductive learning 我们充分利用protein-protein interaction(PPI)数据集，这个数据集包含了不同的人体组织（Zitnik &amp; Leskovec, 2017）。数据集包含了20个图来训练，2个验证，2个测试。关键的是，测试的图包含了训练时完全未见过的图。为了构建图，我们使用Hamilton et al., 2017预处理后的数据。平均每个图的顶点数为2372个。每个顶点有50个特征，组成了positional gene sets，motif gene sets and immunological signatures。从基因本体获得的每个顶点集有121个标签，由Molecular Signatures Database(Subramanian et al., 2005)收集，一个顶点可以同时拥有多个标签。 这些数据集的概貌在表1中给出。 3.2 State-of-the-art methodsTransductive learning 对于transductive learning任务，我们对比了Kipf &amp; Welling 2017的工作，以及其他的baseline。包括了label propagation(LP)(Zhu et al., 2003)，半监督嵌入(SemiEmb)(Weston et al., 2012)，manifold regulariization(ManiReg)(Belkin et al., 2006)，skip-gram based graph embeddings(DeepWalk)(Perozzi et al., 2014)，the iterative classification algorithm(ICA)(Lu &amp; Getoor, 2003)和Planetoid(Yang et al., 2016)。我们也直接对比了GCN(Kipf &amp; Welling 2017)，还有利用了高阶切比雪夫的图卷积模型(Defferrard et al., 2016)，还有Monti et al., 2016提出的MoNet。 Inductive learning 对于inductive learning任务，我们对比了Hamilton et al., 2017提出的四个不同的监督的GraphSAGE 这些方法提供了大量的聚合特征：GraphSAGE-GCN（对图卷积操作扩展inductive setting），GraphSAGE-mean（对特征向量的值取element-wise均值），GraphSAGE-LSTM（通过将邻居特征输入到LSTM进行聚合），GraphSAGE-pool（用一个共享的多层感知机对特征向量进行变换，然后使用element-wise取最大值）。其他的transductive方法要么在inductive中完全不合适，要么就认为顶点是逐渐加入到一个图中，使得他们不能在完全未见过的图上使用（如PPI数据集）。 此外，对于两种任务，我们提供了每个顶点共享的MLP分类器（完全没有整合图结构信息）的performance。 3.3 Experimental SetupTransductive learning 我们使用一个两层的GAT模型。超参数在Cora上优化过后在Citeseer上复用。第一层包含$K = 8$个attention head，计算得到$F’ = 8$个特征（总共64个特征），之后接一个指数线性单元（ELU）（Clevert et al., 2016）作为非线性单元。第二层用作分类：一个单个的attention head计算$C$个特征（其中$C$是类别的数量），之后用softmax激活。处理小训练集时，在模型上加正则化。在训练时，我们使用$L_2$正则化，$\\lambda = 0.0005$。除此以外，两个层的输入都使用了$p = 0.6$的dropout(Srivastava et al., 2014)，在normalized attention coefficients上也使用了（也就是在每轮训练时，每个顶点都被随机采样邻居）。如Monti et al., 2016观察到的一样，我们发现Pubmed的训练集大小(60个样本)需要微调：我们使用$K = 8$个attention head，加强了$L_2$正则，$\\lambda = 0.001$。除此以外，我们的结构都和Cora和Citeseer的一样。 Inductive learning我们使用一个三层的GAT模型。前两层$K = 4$，计算$F’ = 256$个特征（总共1024个特征），然后使用ELU。最后一层用于多类别分类：$K = 6$，每个计算121个特征，取平均后使用logistic sigmoid激活。训练集充分大所以不需要使用$L_2$正则或dropout——但是我们使用了skip connections(He et al., 2016)在attentional layer间。训练时batch size设置为2个图。为了严格的衡量出使用注意力机制的效果（与GCN相比），我们也提供了constant attention mechanism，$a(x, y) = 1$，使用同样的架构——也就是每个邻居上都有相同的权重。 两个模型都使用了Glorot初始化(Glorot &amp; Bengio, 2010)，使用Adam SGD(Kingma &amp; Ba, 2014)优化cross-entropy，Pubmed上初始学习率是0.01，其他数据集是0.005。我们在cross-entropy loss和accuracy(transductive)或micro-F1(inductive)上都使用了early stopping策略，迭代次数为100轮。代码：https://github.com/PetarV-/GAT 3.4 Results对于transductive任务，我们提交了我们的方法100次的平均分类精度（还有标准差），也用了Kipf &amp; Welling., 2017和Monti et al., 2016的metrics。特别地，对于基于切比雪夫方法(Defferrard et al., 2016)，我们提供了二阶和三阶最好的结果。为了公平的评估注意力机制的性能，我们还评估了一个计算出64个隐含特征的GCN模型，并且尝试了ReLU和ELU激活，记录了100轮后更好的那个结果（GCN-64）（结果显示ReLU更好）。对于inductive任务，我们计算了micro-averaged F1 score在两个从未见过的测试图上，平均了10次结果，也使用了Hamilton et al., 2017的metrics。特别地，因为我们的方法是监督的，我们对比了GraphSAGE。为了评估聚合所有的邻居的优点，我们还提供了我们通过修改架构（三层GraphSAGE-LSTM分别计算[512, 512, 726]个特征，128个特征用来聚合邻居）所能达到的GraphSAGE最好的结果（GraphSAGE）。最后，为了公平的评估注意力机制对比GCN这样的聚合方法，我们记录了我们的constant attention GAT模型的10轮结果（Const-GAT）。结果展示出我们的方法在四个数据集上都很好，和预期一致，如2.2节讨论的那样。具体来说，在Cora和Citeseer上我们的模型上升了1.5%和1.6%，推测应该是给邻居分配不同的权重起到了效果。值得注意的是在PPI数据集上：我们的GAT模型对于最好的GraphSAGE结果提升了20.5%，这意味着我们的模型可以应用到inductive上，通过观测所有的邻居，模型会有更强的预测能力。此外，针对Const-GAT也提升3.9%，再一次展现出给不同的邻居分配不同的权重的巨大提升。 学习到的特征表示的有效性可以定性分析——我们提供了t-SNE(Maaten &amp; Hinton, 2008)的可视化——我们对在Cora上面预训练的GAT模型中第一层的输出做了变换（图2）。representation在二维空间中展示出了可辩别的簇。注意，这些簇对应了数据集的七个类别，验证了模型在Cora上对七类的判别能力。此外，我们可视化了归一化的attention系数（对所有的8个attention head取平均）的相对强度。像Bahdanau et al., 2015那样适当的解释这些系数需要更多的领域知识，我们会在未来的工作中研究。 4 Conclusions我们展示了图注意力网络(GAT)，新的卷积风格神经网络，利用masked self-attentional层。图注意力网络计算高效（不需要耗时的矩阵操作，在图中的顶点上并行计算），处理不同数量的邻居时对邻居中的不同顶点赋予不同的重要度，不需要依赖整个图的结构信息——因此解决了之前提出的基于谱的方法的问题。我们的这个利用attention的模型在4个数据集针对transductive和inductive（特别是对完全未见过的图），对顶点分类成功地达到了state-of-the-art的performance。 未来在图注意力网络上有几点可能的改进与扩展，比如解决2.2节描述的处理大批数据时的实际问题。还有一个有趣的研究方向是利用attention机制对我们的模型进行一个深入的解释。此外，扩展我们的模型从顶点分类到图分类也是一个更具应用性的方向。最后，扩展我们的模型到整合边的信息（可能制视了顶点关系）可以处理更多的问题。","link":"/blog/2018/07/13/graph-attention-networks/"},{"title":"Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning","text":"TKDE 2019，网格流量预测，用一个模型同时预测每个网格的流入/流出流量和网格之间的转移流量，分别称为顶点流量和边流量，同时预测这两类流量是本文所解决的多任务预测问题。本文提出的是个框架，所以里面用什么组件应该都是可以的，文章中使用了 FCN。使用两个子模型分别处理顶点流量和边流量预测问题，使用两个子模型的输出作为隐藏状态表示，通过拼接或加和的方式融合，融合后的新表示再分别输出顶点流量和边流量。这篇文章和之前郑宇的文章一样，考虑了三种时序性质、融合了外部因素。损失函数从顶点流量预测值和真值之间的差、边流量预测值和真值之间的差、顶点流量预测值之和与边流量的预测值之差三个方面考虑。数据集是北京和纽约的出租车数据集。 Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning Abstract——预测流量（如车流、人流、自行车流）包括结点的流入、流出流量以及不同的结点间的转移，在交通运输系统中的时空网络里扮演着重要的角色。然而，这个问题受多方面复杂因素影响，比如不同地点的空间关系、不同时段的时间关系、还有像活动和天气这样的外部因素，所以这是个有挑战性的问题。此外，一个结点的流量（结点流量）和结点之间的转移（边流量）互相影响。为了解决这个问题，我们提出了一个多任务的深度学习框架可以同时预测一个时空网络上的结点流量和边流量。基于全卷积网络，我们的方法设计了两个复杂的模型分别处理结点流量预测和边流量预测。这两个模型通过组合中间层的隐藏表示连接，而且共同训练。外部因素通过一个门控融合机制引入模型。在边流量预测模型上，我们使用了一个嵌入组件来处理顶点间的系数转移问题。我们在北京和纽约的出租车数据集上做了实验。实验结果显示比11种方法都好。 1 Introduction时空网络（ST-networks），如运输网络和传感器网络，在世界上到处都是，每个点有个空间坐标，每个边具有动态属性。时空网络中的流量有两种表示，如图 1，顶点流量（一个结点的流入和流出流量）和边流量（结点间的转移流量）。在运输系统中，这两类流量可通过4种方式测量，1. 近邻道路的车辆数，2. 公交车的旅客数，3. 行人数，4. 以上三点。图1b 是一个示意图。取顶点 $r_1$ 为例，我们可以根据手机信令和车辆 GPS 轨迹分别计算得到流入流量是 3，流出流量是 3。$r_3$ 到 $r_1$ 的转移是 3，$r_1$ 到 $r_2$ 和 $r_4$ 的转移是 2 和 1。因此，如图1c所示，我们能拿到两种类型的流量，四个结点的流入和流出分别是 $(3,3,0,5)$ 和 $(3,2,5,1)$。所有的边转移都看作是在有向图上发生的。 预测这类的流量对公共安全，交通管理，网络优化很重要。取人口流动做一个例子，2015 年跨年夜的上海，踩踏事故导致 36 人死亡。如果能预测每个区域之间的人流转移，这样的悲剧就可以通过应急预案避免或减轻。 然而，同时预测所有结点和边的转移是很难的： Scale and complexity: 一个地方的流入和流出依赖于它的邻居，有近邻的也有遥远的，因为人们会在这些区域之间转移，尤其是有活动的时候。给定一个城市，有 $N$ 个地点，$N$ 很大，那么就有 $N^2$ 种转移方式，尽管这些转移可能不会同时发生。因此，预测地点的流量，要么是流入、流出或是转移流量，我们需要考虑地点之间的依赖关系。而且，预测也考虑过去时段的流量。此外，我们不能单独地预测每个地点的流量，因为城市内的地点间是相连的，相关的，互相影响的。复杂度和尺度都是传统机器学习模型，如概率图模型在解决这个问题时面临的巨大挑战。 Model multiple correlations and external factors: 我们需要对三种关系建模来处理预测问题。第一个是不同地点流量的空间相关性，包含近邻和遥远的。第二个是一个地点不同时段的流量间的时间关系，包括时间近邻、周期和趋势性。第三，流入流出流量和转移流量高度相关，互相影响。一个区域的转入流量之和是这个区域的流入流量。精确地预测一个区域的流出流量可以让预测其他区域的转移流量更精确，反之亦然。此外，这些流量受外部因素影响，如活动、天气、事故等。如何整合这些信息还是个难题。 Dynamics and sparsity: 由于 $N^2$ 种情况，区域间随时间改变的转移流量比流入流出流量要大得多。一个地点和其他地点间的转移会在接下来的时段发生，可能是 $N^2$ 中的很小一部分（稀疏）。预测这样的稀疏转移也是个难题。 为了解决上述挑战，我们提出了多任务深度学习框架MDL（图4）来同时预测顶点流量和边流量。我们的贡献有三点： MDL 设计了一个深度神经网络来预测顶点流量（命名为 NODENET），另一个深度神经网络预测边流量（命名为 EDGENET）。通过将他们的隐藏状态拼接来连接这两个深度神经网络，并一同训练。此外，这两类流量的相关性通过损失函数中的正则项来建模。基于深度学习的模型可以处理复杂性和尺度等问题，同时多任务框架增强了每类流量的预测性能。 NODENET 和 EDGENET 都是 three-stream 全卷积网络（3S-FCNs），closeness-stream, period-stream, trend-stream 捕获三种不同的时间相关性。每个 FCN 也同时捕获近邻和遥远的空间关系。一个门控组件用来融合时空相关性和外部因素。为了解决转移稀疏的问题，EDGENET 中我们设计了一个嵌入组件，用一个隐藏低维表示编码了稀疏高维的输入。 我们在北京和纽约的 taxicab data 上评估了方法。结果显示我们的 MDL 超越了其他 11 种方法。 表 1 列出了这篇文章中出现的数学符号。 2 Problem FormulationDefinition 1(Node). 一个空间地图基于经纬度被分成 $I \\times J$ 个网格，表示为 $V = \\lbrace r_1, r_2, …, r_{I\\times J} \\rbrace$，每个元素表示一个空间节点，如图2(a)。 令 $(\\tau, x, y)$ 为时空坐标，$\\tau$ 表示时间戳，$(x, y)$ 表示空间点。一个物体的移动可以记为一个按时间顺序的空间轨迹，起点和终点表示为 $s = (\\tau_s, x_s, y_s)$ 和 $e = (\\tau_e, x_e, y_e)$，表示出发地和目的地。$\\mathbb{P}$ 表示所有的起止对。 Definition 2(In/out flows). 给定一组起止对 $\\mathbb{P}$。$\\mathcal{T} = \\lbrace t_1, \\dots t_T\\rbrace$ 表示一个时段序列。对于地图上第 $i$ 行第 $j$ 列的顶点 $r_{ij}$，时段 $t$ 流出和流入的流量分别定义为： $$\\tag{1}\\mathcal{X}_t(0, i, j) = \\vert \\lbrace (s,e) \\in \\mathbb{P} : (x_s, y_s) \\in r_{ij} \\wedge \\tau_s \\in t \\rbrace \\vert$$ $$\\tag{2}\\mathcal{X}_t(1, i, j) = \\vert \\lbrace (s,e) \\in \\mathbb{P} : (x_e, y_e) \\in r_{ij} \\wedge \\tau_e \\in t \\rbrace \\vert$$ 其中 $\\mathcal{X}_t(0, :, :)$ 和 $\\mathcal{X}_t(1, :, :)$ 表示流出和流入矩阵。$(x, y) \\in r_{ij}$ 表示点 $(x, y)$ 在顶点 $r_{ij}$ 上，$\\tau_e \\in t$ 表示时间戳 $\\tau_e$ 在时段 $t$ 内。流入和流出矩阵在特定时间的矩阵如图2。 考虑两类流量（流入和流出），一个随时间变化的空间地图一般表示一个时间有序的张量序列，每个张量对应地图在特定时间的一个快照。详细来说，每个张量包含两个矩阵：流入矩阵和流出矩阵，如图 2 所示。 让 $V$ 表示时空网络中的顶点集，$N \\triangleq \\vert V \\vert = I \\times J$ 是顶点数。一个时间图包含 $T$ 个离散的不重叠的时段，表示为有向图 $G_{t_1}, \\dots G_{t_T}$ 的时间有序序列。图 $G_t = (V, E_t)$ 捕获了时段 $t$ 时空系统上的拓扑状态。对于每个图 $G_t$ (其中 $t = t_1, \\dots, t_T$) 存在一个对应的权重矩阵 $\\mathbf{S}_t \\in \\mathbb{R}^{N \\times N}$，表示时段 $t$ 的带权有向边。在我们的研究中，时段 $t$ 顶点 $r_s$ 到顶点 $r_e$ 的边的权重，是一个非负标量，表示 $r_s$ 到 $r_e$ 的 transition，时段 $t$ 上两个顶点间没有连接的话，对应的元素在 $\\mathbf{S}_t$ 中为 0。 Definition 3 (Transition). 给定一组起止点对 $\\mathbb{P}$。$\\mathcal{T} = \\lbrace t_1, \\dots, t_T \\rbrace$ 是一组时段的序列。$\\mathbf{S}_t$ 是时段 $t$ 的转移矩阵，$r_s$ 到 $r_e$ 之间的转移表示为 $\\mathbf{S}_t(r_s, r_e)$，定义为： $$\\tag{3}\\mathbf{S}_t(r_s, r_e) = \\vert \\lbrace (s,e) \\in \\mathbb{P} : (x_s, y_s) \\in r_s \\wedge (x_e, y_e) \\in r_e \\wedge \\tau_s \\in t \\wedge \\tau_e \\in t \\rbrace \\vert$$ 其中 $r_s, r_e \\in V$ 是起始顶点和终止顶点。$(x, y) \\in r$ 表示点 $(x, y)$ 在网格 $r$ 上。$\\tau_s \\in t$ 和 $\\tau_e \\in t$ 表示时间戳 $\\tau_s$ 和 $\\tau_e$ 都在时段 $t$ 内。我们考虑转移至发生在一个特定的时段内。因此，对于实际应用来说，我们可以预测起始和结束都发生在未来的转移。 2.1 Converting time-varying graphs into tensors我们将每个时间上的图转为张量。给定时间 $t$ 有向图 $G_t = (V, E_t)$，我们先做展开，然后计算有向带权矩阵（转移矩阵 $\\mathbf{S}_t$），最后给定一个张量 $\\mathcal{M}_t \\in \\mathbf{R}^{2N \\times I \\times J}$。图 3 是示意图。(a)给定时间 $t$ 4 个顶点 6 条边的图。(b)首先展开成有向图。(c)对每个顶点，有一个流入的转移，还有个流出的转移，由一个向量表示（维度是8）。取 $r_1$ 为例，它的流出和流入转移向量分别为 $[0, 2, 0, 1]$ 和 $[0, 0, 3, 0]$，拼接后得到一个向量 $[0, 2, 0, 1, 0, 0, 3, 0]$，包含流出和流入的信息。(d)最后，我们将矩阵 reshape 成一个张量，每个顶点根据原来地图有一个固定的空间位置，保护了空间相关性。 2.2 FLow Prediction Problem流量预测，简单来说，是时间序列问题，目标是给定历史 $T$ 个时段的观测值，预测 $T+1$ 时段每个区域的流量。但是我们的文章中流量有两个层次，流入和流出以及区域间的转移流量。我们的目标是同时预测这些流量。此外，我们还融入了外部因素如房价信息，天气状况，温度等等。这些外部因素可以收集并提供一些额外有用的信息。相关的符号在表 1 之中。 Problem 1. 给定历史观测值 $\\lbrace \\mathcal{X}_t, \\mathcal{M}_t \\mid t = t_1, \\dots, t_T \\rbrace$，外部特征 $\\mathcal{E}_T$，我们提出一个模型共同预测 $\\mathcal{X}_{t_{T+1}}$ 和 $\\mathcal{M}_{t_{T+1}}$。 3 Multitask Deep Learning图 4 展示了我们的 MDL 框架，包含 3 个组成部分，分别用于数据转换，顶点流量建模，边流量建模。我们首先将轨迹（或订单）数据转换成两类流量，i) 顶点流量表示成有时间顺序的张量序列 $\\lbrace\\mathcal{X}_t \\mid t = t_1, \\dots, t_T \\rbrace$ (1a); ii) 边流量是一个有时间顺序的图序列（转移矩阵）$\\lbrace\\mathbf{S}_t \\mid t = t_1, \\dots, t_T \\rbrace$ (2a)，之后再根据 2.1 节的方法转换为张量的序列 $\\lbrace\\mathcal{M}_t \\mid t = t_1, \\dots, t_T \\rbrace$ (2b)。这两类像视频一样的数据之后放到 NODENET 和 EDGENET 中。以 NODENET 为例，它选了三个不同类型的片段，放入 3S-FCN 中，对时间相关性建模。在这个模型中，每部分的 FCN 可以通过多重卷积捕获空间相关性。NODENET 和 EDGENET 中间的隐藏表示通过一个 BRIDGE 组件连接，使两个模型可以共同训练。我们使用一个嵌入层来处理转移稀疏的问题。一个门控融合组件用来整合外部信息。顶点流量和边流量用一个正则化来建模。 3.1 EDGENET根据上述的转换方法，每个时段的转移图可以转换成一个张量 $\\mathcal{M}_t \\in \\mathbb{R}^{2N \\times I \\times J}$。对于每个顶点 $r_{ij}$，它最多有 $2N$ 个转移概率，包含 $N$ 个流入和 $N$ 个流出。然而，对于一个确定的时段，顶点间的转移是稀疏的。受 NLP 的嵌入方法启发，我们提出了使用空间嵌入方法，解决这样的稀疏和高维问题。详细来说，空间嵌入倾向于学习一个将 $2N$ 维映射到 $k$ 维的函数： $$\\tag{4}\\mathcal{Z}_t(:, i, j) = \\mathbf{W}_m \\mathcal{M}_t (:, i, j) + \\mathbf{b}_m, 1 \\leq i \\leq I, 1 \\leq j \\leq J$$ 其中 $\\mathbf{W}_m \\in \\mathbb{R}^{k \\times 2N}$ 和 $\\mathbf{b}_m \\in \\mathbb{R}^k$ 是参数。所有的结点共享参数。$\\mathcal{M}_t(:, i, j) \\in \\mathbb{R}^{2N}$ 表示 $(i, j)$ 的向量。 流量，比如城市中的交通流，总是受时空依赖关系影响。为了捕获不同的时间依赖（近邻、周期、趋势），Zhang et al. 提出了深度时空残差网络，沿时间轴选择不同的关键帧。受这点的启发，我们选择近邻、较近、远期关键帧来预测时段 $t$，分别表示为 $M^{dep}_t = \\lbrace M^{close}_t, M^{period}_t, M^{trend}_t \\rbrace$，如下： Closeness dependents:$$M^{close}_t = \\lbrace \\mathcal{Z}_{t-l_c}, \\dots, \\mathcal{Z}_{t-1} \\rbrace$$ Period dependents:$$M^{period}_t = \\lbrace \\mathcal{Z}_{t-l_p}, \\mathcal{Z}_{t-(l_p - 1) \\cdot p}, \\dots, \\mathcal{Z}_{t-p} \\rbrace$$ Trend dependents:$$M^{trend}_t = \\lbrace \\mathcal{Z}_{t-l_q \\cdot q}, \\mathcal{Z}_{t-(l_q - 1)\\cdot q}, \\dots, \\mathcal{Z}_{t-q} \\rbrace$$ 其中 $p$ 和 $q$ 是周期和趋势范围。$l_c$, $l_p$ 和 $l_q$ 是三个序列的长度。 输出（即下个时段的预测）和输入有相同的分辨率。这样的人物和图像分割问题很像，可以通过全卷积网络 (FCN) [22] 处理。 受到这个启发，我们提出了三组件的 FCN，如图 4，来捕获时间近邻、周期和趋势依赖。每个组件都是个 FCN，包含了很多卷积（图 5）。根据卷积的性质，一个卷积层可以捕获空间近邻关系。随着卷积层数的增加，FCN 可以捕获更远的依赖，甚至是城市范围大小的空间依赖。然而，这样的深层卷积网络很难训练。因此我们使用残差连接来帮助训练。类似残差网络中的残差连接，我们使用一个包含 BN，ReLU，卷积的块。令三个近邻、周期、趋势三组件的输出分别为 $\\mathcal{M}_c$, $\\mathcal{M}_p$, $\\mathcal{M}_q$。不同的顶点在近邻、周期、趋势上可能有不同的性质。为了解决这个问题，我们提出使用一个基于参数矩阵的融合方式（图 4 中的 PM 融合）： $$\\tag{5}\\mathcal{M}_{fcn} = \\mathbf{W}_c \\odot \\mathcal{M}_c + \\mathbf{W}_p \\odot \\mathcal{M}_p + \\mathbf{W}_q \\odot \\mathcal{M}_q$$ 其中 $\\odot$ 是哈达玛积，$\\mathbf{W}$ 是参数，调整三种时间依赖关系的影响。 3.2 NODENET and BRIDGE类似 EDGENET，NODENET 也是一个 3S-GCN，我们选择近邻、较近、遥远的关键帧作为近邻、周期、趋势依赖。区别是 NODENET 没有嵌入层因为输入的通道数只有 2。这三种不同的依赖放入三个不同的 FCN 中，输出通过 PM 融合组件融合（图 4）。然后，得到 3S-FCN 的输出，表示为 $\\mathcal{X}_{fcn} \\in \\mathbb{R}^{C_x \\times I \\times J}$。 考虑顶点流量与边流量的相关性，所以从 NODENET 和 EDGENET 学习到的表示应该被连起来。为了连接 NODENET 和 EDGENET，假设 NODENET 和 EDGENET 的隐藏表示分别为 $\\mathcal{X}_{fcn}$ 和 $\\mathcal{M}_{fcn}$。我们提出两种融合方法： SUM Fusion: 加和融合方法直接将两种表示相加： $$\\tag{6}\\mathcal{H}(c, :, :) = \\mathcal{X}_{fcn}(c, :, :) + \\mathcal{M}_{fcn}(c, :, :), c = 0, \\dots, C - 1$$ 其中 $C$ 是 $\\mathcal{X}_{fcn}$ 和 $\\mathcal{M}_{fcn}$ 的通道数，$\\mathcal{H} \\in \\mathbb{R}^{C \\times I \\times J}$。显然这种融合方法受限于两种表示必须有相同的维度。 CONCAT Fusion: 为了从上述的限制中解脱，我们提出了另一种融合方法。顺着通道拼接两个隐藏表示： $$\\tag{7}\\mathcal{H}(c, :, :) = \\mathcal{X}_{fcn}(c, :, :), c=0, \\dots, C_x - 1$$ $$\\tag{8}\\mathcal{H}(C_x + c, :, :) = \\mathcal{M}_{fcn}(c, :, :), c=0, \\dots, C_m - 1$$ $C_x$ 和 $C_m$ 分别是两个隐藏表示的通道数。$\\mathcal{H} \\in \\mathbb{R}^{(C_x + C_m) \\times I \\times J}$。拼接融合实际上可以通过互相强化更好地融合顶点流量和边流量。像 BRIDGE 一样我们也讨论了其他的融合方式（4.3 节）。 在拼接融合中，我们在 NODENET 和 EDGENET 中分别加了一层卷积。卷积用来将合并的隐藏特征 $\\mathcal{H}$ 映射到 不同通道大小的输出上，即 $\\mathcal{X}_{res} \\in \\mathbb{R}^{2 \\times I \\times J}$ 和 $\\mathcal{M}_{res} \\in \\mathbb{R}^{2N \\times I \\times J}$，如图 6。 3.3 Fusing External Factors Using a Gating Mechanism外部因素，活动、天气会影响时空网络不同区域的流量。举个例子，一起事故可能会阻塞一个局部区域的交通，一场暴风雨可能会减少整个城市的流量。这样的外部因素就像一个开关，如果它打开了那流量会产生巨大的变化。基于这个思路，我们开发了一种基于门控机制的融合，如图 6 所示。时间 $t$ 的外部因素表示为 $\\mathcal{E}_t \\in \\mathbb{R}^{l_e \\times I \\times J}$，$\\mathcal{E}_t(:, i, j) \\in \\mathbb{R}^{l_e}$ 表示一个特定顶点的外部信息。我们可以通过下式获得 EDGENET 的门控值： $$\\tag{9}\\mathbf{F}_m(i, j) = \\sigma(\\mathbf{W}_e(:, i, j) \\cdot \\mathcal{E}_t(:, i, j) + \\mathbf{b}_e(i, j)), 1 \\leq i \\leq I, 1 \\leq j \\leq J$$ 其中 $\\mathbf{W}_e \\in \\mathbb{R}^{l_e \\times I \\times J}$ 和 $\\mathbf{b}_e \\in \\mathbb{R}^{I \\times J}$ 是参数。$\\mathbf{F}_m \\in \\mathbb{R}^{I \\times J}$ 是 GATING 的输出，$\\mathbf{F}_m(i, j)$ 是对应时空网络中结点 $r_{ij}$ 的门控值。$\\sigma(\\cdot)$ 是 sigmoid 激活函数，$\\cdot$ 是两向量的内积。 然后我们使用 PRODUCT 融合方式： $$\\tag{10}\\hat{\\mathcal{M}}_t(c, :, :) = \\text{tanh}(\\mathbf{F}_m \\odot \\mathcal{M}_{Res}(c, :, :)), c = 0, \\dots, 2N - 1$$ 类似的，NODENET 最后在时间 $t$ 的预测结果为： $$\\tag{11}\\hat{\\mathcal{X}}_t(c, :, :) = \\text{tanh} (\\mathbf{F}_x \\odot \\mathcal{X}_{Res}(c, :, :)), c = 0, 1$$ 其中 $\\mathbf{F}_x \\in \\mathbb{R}^{I \\times J}$ 是 GATING 的另一个输出。对于顶点流量和边流量使用不同的门控值的一个原因是外部因素对流入/流出流量和不同地点之间的转移流量的影响是不一致的。 3.4 Losses令 $\\phi$ 为 EDGENET 中所有的参数，我们的目标是通过最小化目标函数学习这些参数： $$\\tag{12}\\mathop{\\mathrm{argmin}}\\limits_{\\phi} \\mathcal{J}_{edge} = \\sum_{t \\in \\mathcal{T}}\\sum^{2N-1}_{c=0} \\Vert Q^c_t \\odot (\\hat{\\mathcal{M}}_t(c, :, :) - \\mathcal{M}_t(c, :, :)) \\Vert^2_F$$ 其中 $Q^c_t$ 是指示矩阵，表示 $\\mathcal{M}_t(c, :, :)$ 中所有非零元素。$\\mathcal{T}$ 是可用的时段，$\\Vert \\cdot \\Vert_F$ 是矩阵的 F 范数。 类似的，$\\theta$ 是 NODENET 的参数，目标函数是： $$\\tag{13}\\mathop{\\mathrm{argmin}}\\limits_{\\theta} \\mathcal{J}_{node} = \\sum_{t \\in \\mathcal{T}}\\sum^1_{c=0} \\Vert P^c_t \\odot (\\hat{\\mathcal{X}}_t(c, :, :) - \\mathcal{X}_t(c, :, :)) \\Vert^2_F$$ 其中 $P^c_t$ 是指示矩阵，表示 $\\mathcal{X}_t(c, :, :)$ 中所有非零元素。我们知道对于一个结点来说，它的转入流量之和就是它的流入流量，转出流量之和就是流出流量。定义 2 中定义，$\\hat{\\mathcal{X}}_t(0, :, :)$ 和 $\\hat{\\mathcal{X}}_t(1, :, :)$ 分别是流出和流入矩阵。根据 2.1 节定义的方法构建转移矩阵，可知前 $N$ 个通道表示转出流量，后 $N$ 个通道表示转入流量。因此，有下面的损失函数： $$\\tag{14}\\mathop{\\mathrm{argmin}}\\limits_{\\theta, \\phi} \\sum_{t \\in \\mathcal{T}} \\sum_i \\sum_j (\\Vert \\hat{\\mathcal{X}}_t(0, i, j) - \\sum^{N-1}_{c=0} \\hat{\\mathcal{M}}_t(c,i,j) \\Vert^2 + \\Vert \\hat{\\mathcal{M}}_t(1,i,j) - \\sum^{2N-1}_{c=N} \\hat{\\mathcal{M}}_t(c,i,j) \\Vert^2)$$ 或者等价的可以写成 最后，我们获得融合的损失： $$\\tag{16}\\mathop{\\mathrm{argmin}}\\limits_{\\theta, \\phi} \\lambda_{node} \\mathcal{J}_{node} + \\lambda_{edge} \\mathcal{J}_{edge} + \\lambda_{mdl} \\mathcal{J}_{mdl}$$ 其中，$\\lambda_{node}$, $\\lambda_{edge}$, $\\lambda_{mdl}$ 是可调节的参数。 3.4.1 Optimization Algorithm 算法 1 是 MDL 的训练过程。1-4 行是构建训练样例。7-8 行是用批量样本优化目标函数。 4 Experiments两个数据集 TaxiBJ 和 TaxiNYC，看表 2。我们使用 RMSE 和 MAE 作为评价指标。 4.1 Settings4.1.1 Datasets我们使用表 3 中的两个数据集。每个数据集包含两个子集，轨迹/出行和外部因素，细节如下： TaxiBJ: 北京出租车 GPS 轨迹数据有四个时段：20130101-20131030, 20140301-20140630, 20150501-20150630, 201501101-20160410。我们用最后 4 个星期作为测试集，之前的数据作为训练集。 TaxiNYC: NYC 2011 到 2014 年的出租车订单数据。订单数据包含上车和下车的时间。上车和下车地点。最后四个星期作为测试集。 4.1.2 BaselinesHA, ARIMA, SARIMA, VAR, RNN, LSTM. GRU, ST-ANN, ConvLSTM, ST-ResNet, MRF. 4.1.3 PreprocessingMDL 的输出，我们用 $\\text{tanh}$ 作为最后的激活函数。我们用最大最小归一化。评估的时候，将预测值转换为原来的值。对于外部因素，使用 one-hot，假期和天气放入二值向量中，用最大最小归一化把温度和风速归一化。 4.1.4 Hyperparameters$\\lambda_{node} = 1$ 和 $\\lambda_{edge} = 1$，$\\lambda_{mdl} = 0.0005$，$p$ 和 $q$ 按经验设定为一天和一周。三个依赖序列的长度分别为 $l_c \\in \\lbrace 1, 2, 3\\rbrace$, $l_p \\in \\lbrace 1,2,3 \\rbrace$, $l_q \\in \\lbrace 1,2,3 \\rbrace$。卷积的数量是 5 个。训练集的 90% 用来训练，10% 来验证，用早停选最好的参数。然后使用所有的数据训练模型。网络参数通过随机初始化，Adam 优化。batch size 32。学习率 $\\lbrace 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005 \\rbrace$。 4.1.5 Evaluation MetricsRMSE 和 MAE。 4.2 Results Node Flow Prediction. 我们先比流入和流出流量的预测。表 4 展示了两个数据集上的评价指标结果。MDL 和 MRF 比其他所有的方法多要好。我们的 MDL 在 NYC 的数据集上明显比 MRF 好。BJ 的数据集上，MDL 比 MRF 差不多。原因是 NYC 数据集比 BJ 数据集大了三倍。换句话说，在大的数据集上，我们的方法比 MRF 更好。我们也注意到训练 MRF 很好使，在 BJ 数据集上训练了一个星期。 Results of Edge Flow Prediction. 表6 展示了边流量预测。边流量预测的实验很费时。MDL 比其他的都好。 4.3 Evaluation on Fusing Mechanisms融合 NODENET 和 EDGENET 有 CONCAT 和 SUM 两种方法。融合外部因素有 GATED 和 SIMPLE 融合，或者不使用。因此总共有 6 种方法。如表 7。使用同样的超参数设定。我们发现 CONCAT + GATING 比其他的方法好。 4.4 Evaluation on Model Hyper-parameters4.4.1 Effect of Training Data Size我们选了 NYC 3 个月，6 个月，1 年，3 年数据。$l_c = 3$, $l_p = 1$, $l_q = 1$。图 8 是结果。我们观察到数据越多，效果越好。 4.4.2 Effect of Network Depth图 9 展示了网络深度在 NYC 3 个月数据集上的影响。网络越深，RMSE 会下降，因为网络越深越能捕获更大范围的空间依赖。然而，网络更深 RMSE 就会上升，这是因为网络加深后训练会变得困难。 4.4.3 Effect of multi-task component表 8 和图 10 展示了多任务组件的影响。 我们可以看到转移流量预测任务大多数情况下可以提升，$\\lambda_{node} = \\lambda_{edge} = 1$，$\\lambda_{mdl}=0.1$，我们的模型获得最好的效果，两种任务都获得更好的结果，证明了多任务可以互相提升。 4.5 Flow Predictions图 11 描绘了我们的 MDL 在 NYC 上预测两个节点未来一小时的数据。结点 (10, 1)，总是比 (8, 3) 高。我们的模型在预测曲线上更精确。","link":"/blog/2019/04/19/flow-prediction-in-spatio-temporal-networks-based-on-multitask-deep-learning/"},{"title":"Graph Convolutional Neural Networks for Web-Scale Recommender Systems","text":"KDD 2018。使用图卷积对顶点进行表示，学习顶点的 embedding ，通过卷积将该顶点的邻居信息融入到向量中。原文链接：Graph Convolutional Neural Networks for Web-Scale Recommender Systems。 ABSTRACT最近在图数据上的深度神经网络在推荐系统上表现的很好。然而，把这些算法应用到数十亿的物品和数百亿的用户上仍然是个挑战。 我们提出了一种在 Pinterest 上的大规模深度推荐引擎，开发了一种高效的图卷积算法 PinSage，融合了随机游走和图卷积，来生成顶点（物品）的表示，同时整合了顶点信息和图结构。对比之前的 GCN 方法，我们研究的模型基于高效的随机游走来结构化卷积操作，而且还设计了一个新型的训练策略，这个策略依赖于 harder-and-harder 训练样本，来提高模型的鲁棒性和收敛能力。 我们的 PinSage 在 Pinterest 上面的75亿个样本上进行训练，图上有30亿个顶点表示 pins 和 boards，180亿条边。根据离线指标、用户研究和 A/B 测试，PinSage 生成了相比其他深度学习和基于图的方法更高质量的推荐结果。据我们所知，这是深度图表示目前规模最大的应用，并且为新一代基于图卷积结构的大规模推荐系统奠定了基础。 1 INTRODUCTION深度学习方法在推荐系统中越来越重要，用来学习图像、文本、甚至是用户的有效的低维表示。使用深度学习学习到的表示可以用来补充、或是替换像协同过滤这样传统的推荐算法。这些表示很有用，因为他们可以在各种推荐任务中重复使用。举个例子，使用深度模型学习得到的物品的表示，可以用来做 “物品-物品” 推荐，也可以来按主题推荐（比如，歌单、或是 Feed流的内容）。 近些年可以看到这个领域的很多重要的发展，尤其是新的可以学习图结构的深度学习方法的发展，是一些推荐应用的基础（比如在用户-物品网络上或社交网络上推荐）。 在这些成功的深度学习框架中比较重要的是图卷积网络（GCN）。核心的原理是学习如何迭代地使用神经网络从局部图邻居中聚合特征信息（图1）。这里，一个简单的卷积操作从一步邻居中变换并聚合特征信息，并且通过堆叠多个这样的卷积，信息可以传播到图中很广的地方。不像纯基于内容的深度模型（如 RNN ），GCN 利用内容信息和图结构。基于 GCN 的模型的方法已经在无数推荐系统中形成了新的标准（参见[19]的综述）。然而，这些b enchmark 上面获得的提升，还没有被转换到真实环境的应用中去。 主要挑战是要将训练和基于 GCN 的顶点表示在数十亿的顶点和数百亿的边的图中进行。扩展 GCN 很困难，因为很多在大数据环境中，很多基于这些 GCN 设计的假设都不成立了。比如，所有的基于 GCN 的推荐系统需要在训练时使用图的拉普拉斯矩阵，但是当顶点数很大的时候，这就不现实了，因为算不出来。 3 METHOD在这部分，我们将描述 PinSage 的结构和训练的技术细节，也会讲一下使用训练好的 PinSage 模型来高效地生成 embedding 的MapReduce pipeline。 我们方法的计算关键在于局部图卷积的表示(notion)。我们使用多个卷积模块来聚合一个顶点局部的邻域特征信息（图1），来生成这个顶点的 embedding（比如一个物品）。每个模块学习如何从一个小的图邻域中聚合信息，并且通过堆叠多个这样的模块，我们的方法可以获得局部网络的拓扑结构信息。更重要的是，这些局部卷积模块的参数对所有的顶点来说是共享的，这使得我们的方法的参数的计算复杂度与输入的图的大小无关。 3.1 Problem SetupPinterest 是一个内容挖掘应用，在这里用户与 pins 进行交互，这些 pins 是在线内容的可见标签（比如用户做饭时的食谱，或者他们想买的衣服）。用户用 boards 将 pins 组织起来，boards 里面包含了 pins 组成的集合，这些 pins 在用户看来是主题相关的。Pinterest 组成的图包含了 20 亿的 pins，10 亿的 boards，超过 180 亿的边（也就是 pins 对应 boards 的关系）。 我们的任务是生成可以用于推荐的高质量的 embedding 或 pins 的表示（比如，使用最近邻来查找 pin 的推荐，或是使用下游的再评分系统进行推荐）。为了学习这些 embedding，我们对 Pinterest 环境进行建模，得到一个二部图，顶点分为两个不相交的集合，$\\mathcal{I}$ 表示 pins，$\\mathcal{C}$ 表示 boards。当然，我们的方法是可以泛化到其他方面的，比如 $\\mathcal{I}$ 看作是物品，$\\mathcal{C}$ 看作是用户定义的环境或收藏品集合等。 再来说说图结构，我们假设 pins/items $u \\in \\mathcal{I}$ 与特征 $x_u \\in \\mathbb{R}^d$ 相关。通常来说，这些特征可能是物品的元数据或上下文信息，在 Pinterest 的例子中，pins 是和富文本与图片特征相关的。我们的目标是利用这些输入特征，也利用二部图的图结构性质来生成高质量的 embedding。这些 embedding 可以用于推荐系统，通过最近邻查找来生成推荐，或是作为用评分来推荐的机器学习系统的特征。 为了符号的简洁，我们使用 $\\mathcal{V} = \\mathcal{I} \\cup \\mathcal{C}$ 来表示图中的顶点集，没有特殊需要不区分 pin 和 board 顶点，一律使用 node 来表示顶点。 3.2 Model Architecture我们使用局部卷积模块对顶点生成 embeddings。首先输入顶点的特征，然后学习神经网络，神经网络会变换并聚合整个图上的特征来计算顶点的 embeddings（图1）。 Forward propagation algorithm. 考虑对顶点 $u$ 生成 embedding $z_u$ 的任务，需要依赖顶点的输入特征和这个顶点周围的图结构。 我们的 PinSage 算法是一个局部卷积操作，我们可以通过这个局部卷积操作学到如何从 $u$ 的邻居聚合信息（图1）。这个步骤在算法1 CONVOLVE 中有所描述。从本质上来说，我们通过一个全连接神经网络对 $\\forall{v} \\in \\mathcal{N}(u)$，也就是 $u$ 的邻居的表示 $z_v$ 进行了变换，之后在结果向量集合上用一个聚合/池化函数（例如：一个 element-wise mean 或是加权求和，表示为 $\\gamma$）（Line 1）。这个聚合步骤生成了一个 $u$ 的邻居$\\mathcal{N}(u)$ 的表示 $n_u$。之后我们将这个聚集邻居向量 $n_u$ 和 $u$ 的当前表示向量进行拼接后，输入到一个全连接神经网络做变换（Line 2）。通过实验我们发现使用拼接操作会获得比平均操作[21]好很多的结果。除此以外，第三行的 normalization 使训练更稳定，而且对近似最近邻搜索来说归一化的 embeddings 更高效（Section 3.5）。算法的输出是集成了 $u$ 自身和他的局部邻域信息的表示。 Importance-based neighborhoods. 我们方法中的一个重要创新是如何定义的顶点邻居 $\\mathcal{N}(u)$，也就是我们在算法1中是如何选择卷积的邻居集合。尽管之前的 GCN 方法简单地检验了 k-hop 邻居，在 PinSage 中我们定义了基于重要性的邻域，顶点 $u$ 的邻居定义为 $T$ 个顶点，这 $T$ 个顶点对 $u$ 是最有影响力的。具体来说，我们模拟了从顶点 $u$ 开始的随机游走，并且计算了通过随机游走[14]对顶点的访问次数的 $L_1$ 归一化值。$u$ 的邻居因此定义为针对顶点 $u$ 来说 $T$ 个最高的归一化的访问数量的顶点。 这个基于重要性的邻域定义的优点有两点。第一点是选择一个固定数量的邻居顶点来聚集可以在训练过程中控制内存开销[18]。第二，在算法1中聚集邻居的向量表示时可以考虑邻居的重要性。特别地，我们在算法1中实现的 $\\gamma$ 是一个加权求均值的操作，权重就是 $L_1$ 归一化访问次数。我们将这个新的方法称为重要度池化(importance pooling)。 Stacking convolutions. 每次使用算法1的 CONVOLVE 操作都会得到一个顶点的新的表示，我们可以在每个顶点上堆叠卷积来获得更多表示顶点 $u$ 的局部邻域结构的信息。特别地，我们使用多层卷积，其中对第 $k$ 层卷积的输入依赖于 $k-1$ 层的输出（图1），最初的表示（”layer 0”）等价于顶点的输入特征。需要注意的是，算法1中的模型参数（$Q$, $q$, $W$ 和 $w$）在顶点间是共享的，但层与层之间不共享。 算法2详细描述了如何堆叠卷积操作，针对一个 minibatch 的顶点 $\\mathcal{M}$ 生成 embeddings。首先计算每个顶点的邻居，然后使用 $K$ 个卷积迭代来生成目标顶点的 K 层表示。最后一层卷积层的输出之后会输入到一个全连接神经网络来生成最后的 embedding $z_u$，$\\forall{u} \\in \\mathcal{M}$。 模型需要学习的参数有：每个卷积层的权重和偏置（$Q^{(k)}$，$q^{(k)}$，$W^{(k)}$，$w^{(k)}$，$\\forall{k} \\in \\lbrace 1,…,K\\rbrace $），还有最后的全连接网络中的参数 $G_1$，$G_2$，$g$。算法1的第一行的每层输出的维度（也就是 $Q$ 的列空间的维度）设为 $m$。为了简单起见，我们将所有卷积层（算法1的第三行的输出）的输出都设为同一个数，表示为 $d$。模型最后的输出（算法2第18行之后）也设为 $d$。 3.3 Model Training我们使用 max-margin ranking loss 来训练 PinSage。在这步，假设我们有了一组标记的物品对 $\\mathcal{L}$，$(q,i) \\in \\mathcal{L}$ 认为是相关的，也就是当查询 $q$ 时，物品 $i$ 是一个好的推荐候选项。训练阶段的目标是优化 PinSage 的参数，使得物品对 $(q,i) \\in \\mathcal{L}$ 的 embedding 在标记集合中尽可能的接近。 我们先来看看 margin-based loss function。首先我们来看看我们使用的可以高效地计算并且使 PinSage 快速收敛的一些技术，这些技术可以让我们训练包含数十亿级别的顶点的图，以及数十亿训练样本。最后，我们描述我们的 curriculum-training scheme，这个方法可以全方位的提升我们的推荐质量。 Loss function. 为了训练模型的参数，我们使用了一个基于最大边界的损失函数。基本的思想是我们希望最大化正例之间的内积，也就是说，查询物品的 embedding 和对应的相关物品的 embedding 之间的内积。与此同时我们还想确保负例之间的内积，也就是查询物品的 embedding 和那些不相关物品 embedding 之间的内积要小于通过提前定义好的边界划分出的正例的内积。对于单个顶点对 embeddings $(z_q, z_i):(q, i) \\in \\mathcal{L}$ 的损失函数是： $$J_{\\mathcal{G}}(z_qz_i) = \\mathbb{E}_{n_k \\thicksim p_n(q)}\\max\\lbrace 0, z_q \\cdot z_{n_k}-z_q \\cdot z_i + \\Delta\\rbrace$$ 其中，$P_n(q)$ 表示物品 $q$ 的负样本分布，$\\Delta$ 表示 margin 超参数。一会儿会讲负样本采样。 Multi-GPU training with large minibatches. 为了在训练中充分利用单台机器的多个 GPU，我们以一种 multi-tower 的方法运行前向和反向传播。我们首先将每个 minibatch（图1底部）分成相等大小的部分。每个 GPU 获得 minibatch 的一部分，使用同一组参数来运算。在反向传播之后，所有 GPU 上的针对每个参数的梯度进行汇集，然后使用一个同步的 SGD。由于训练需要极大数量的样本，我们在运行时使用了很大的 batch size，范围从 512 到 4096。 我们使用与 Goyal et al.[16] 提出的相似的技术来确保快速收敛，而且在处理大 batch size 时训练的稳定和泛化精度。我们在第一轮训练的时候根据线性缩放原则使用一个 gradual warmup procedure，使学习率从小增大到一个峰值。之后学习率以指数级减小。 Producer-consumer minibatch construction. 在训练的过程中，由于邻接表和特征矩阵有数十亿的顶点，所以放在了 CPU 内存中。然而，在训练 PinSage 的 CONVOLVE 步骤时，每个 GPU 需要处理邻居和顶点邻居的特征信息。从 GPU 访问 CPU 内存中的数据时会有很大的开销。为了解决这个问题，我们使用了一个 re-indexing 的方法创建包含了顶点和他们的邻居的子图 $G’ = (V’, E’)$，在当前的 minibatch 中会被加入到计算中。只包含当前 minibatch 计算的顶点特征信息的小的特征矩阵会被抽取出来，顺序与 $G’$ 中顶点的 index 一致。$G’$ 的邻接表和小的特征矩阵会在每个 minibatch 迭代时输入到 GPU 中，这样就没有了 GPU 和 CPU 间的通信开销了，极大的提高了 GPU 的利用率。 训练过程改变了 CPU 和 GPU 的使用方式。模型计算是在 GPU，特征抽取、re-indexing、负样本采样是在 CPU 上运算的。使用 multi-tower 训练的 GPU 并行和 CPU 计算使用了 OpenMP[25]，我们设计了一个生产者消费者模式在当前迭代中使用 GPU 计算，在下一轮使用 CPU 计算，两者并行进行。差不多减少了一半的时间。 Sampling negative items. 负样本采样在我们的损失函数中作为 edge likelihood[23] 的归一化系数的近似值。为了提升 batch size 较大时的训练效率，我们采样了 500 个负样本作为一组，每个 minibatch 的训练样本共同使用这一组。相比于对每个顶点在训练时都进行负样本采样，这极大地减少了每次训练时需要计算的 embeddings 的数量。从实验上来看，我们发现这两种方法在表现上没什么特别大的差异。 在最简单的情况中，我们从整个样本集中使用均匀分布的抽样方式。然而，确保正例($(q, i)$)的内积大于 $q$ 和 500 个负样本中每个样本的内积是非常简单的，而且这样做不能提供给系统足够学习的分辨率。我们的推荐算法应该能从 200 亿个商品中找到对于物品 $q$ 来说最相关的 1000 个物品。换句话说，我们的模型应该能从超过 2 千万的物品中区分/辨别出 1 件物品。但是通过随机采样的 500 件物品，模型的分辨率只是 $\\frac{1}{500}$。因此，如果我们从 200 亿物品中随机抽取 500 个物品，这些物品中的任意一个于当前这件查询的物品相关的几率都很小。因此，模型通过训练不能获得好的参数，同时也不能对相关的物品进行区分的概率很大。为了解决上述问题，对于每个正训练样本（物品对$(q, i)$），我们加入了”hard”负例，也就是那些与查询物品 $q$ 有某种关联的物品，但是又不与物品 $i$ 有关联。我们称这些样本为”hard negative items”。通过在图中根据他们对查询物品 $q$ 的个性化 PageRank 分数来生成[14]。排名在 2000-5000 的物品会被随机采样为 hard negative items。如图2所示，hard negative examples 相比于随机采样的负样本更相似于查询物品，因此对模型来说挑战是排名，迫使模型学会在一个好的粒度上分辨物品。 使用 hard negative items 会让能使模型收敛的训练轮数翻倍。为了帮助模型收敛，我们使用了 curriculum training scheme[4]。在训练的第一轮，不适用 hard negative items，这样算法可以快速地找到 loss 相对较小的参数空间。之后我们在后续的训练中加入了 hard negative items，专注于让模型学习如何从弱关系中区分高度关联的pins。在第 $n$ 轮，我们对每个物品的负样本集中加入了 $n-1$ 个 hard negative items。 3.4 Node Embeddings via MapReduce在模型训练结束后，对于所有的物品（包括那些在训练中未见过的物品）直接用训练好的模型生成 embeddings 还是有挑战的。使用算法2对顶点直接计算 embedding 的方法会导致重复计算，这是由顶点的K-hop 邻居导致的。如图1所示，很多顶点在针对不同的目标顶点生成 embedding 的时候被很多层重复计算多次。为了确保推算的有效性，我们使用了一种 MapReduce 架构来避免在使用模型进行推算的时候的重复计算问题。 我们发现顶点的 embedding 在推算的时候会很好的将其自身带入到 MR 计算模型中。图3详细地表述了 pin-to-board Pinterest 二部图上的数据流，我们假设输入（”layer-0”）顶点是 pins/items（layer-1 顶点是 boards/contexts）。MR pipeline 有两个关键的组成部分： 一个 MapReduce 任务将所有的 pins 投影到一个低维隐空间中，在这个空间中会进行聚合操作（算法1，第一行） 另一个 MR 任务是将结果的 pins 表示和他们出现在的 boards 的 id 进行连接，然后通过 board 的邻居特征的池化来计算 board 的 embedding。 注意，我们的方法避免了冗余的计算，对于每个顶点的隐向量只计算一次。在获得 boards 的 embedding 之后，我们使用两个以上的 MR 任务，用同样的方法计算第二层 pins 的 embedding，这个步骤也是可以迭代的（直到 K 个卷积层）。 3.5 Efficient nearest-neighbor lookups由 PinSage 生成的 embeddings 可以用在很多下游推荐任务上，在很多场景中我们可以直接使用这些 embeddings 来做推荐，通过在学习到的嵌入空间中使用最近邻查找。也就是，给定一个查询物品 $q$，我们使用 K-近邻的方式来查找查询物品 embedding 的 K 个邻居的嵌入。通过局部敏感哈希[2]的近似 K 近邻算法很高效。在哈希函数计算出后，查找物品可以通过一个基于 Weak AND 操作[5]的两阶段查询实现。PinSage 模型是离线计算的并且所有节点的表示通过 MR 计算后存放到数据库中，高效的最近邻查找方法可以使系统在线提供推荐结果。","link":"/blog/2018/06/17/graph-convolutional-neural-networks-for-web-scale-recommender-systems/"},{"title":"hexo markdown mathjax 冲突问题","text":"写完公式渲染不出来，比如\\vec{i}_j就会出错markdown中的下划线_表示斜体，在latex中，是下标。\\\\在latex中是换行，在markdown中会转义成\\。所以导致如果写公式\\vec{i}_j，本来应该是向量i的j下标，就会渲染不出来。解决方案：修改hexo的渲染源码打开nodes_modules/marked/lib/marked.js将1escape: /^\\\\([\\\\`*{}\\[\\]()# +\\-.!_&gt;])/, 改为:1escape: /^\\\\([`*{}\\[\\]()# +\\-.!_&gt;])/, 这样就会去掉\\\\的转义了。将1em: /^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, 改为:1em:/^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,","link":"/blog/2018/07/14/hexo-markdown-mathjax-冲突问题/"},{"title":"Hadoop HA安装三：zookeeper的安装","text":"zookeeper通常以“复制模式”运行于一个计算机集群上，这个计算机集群被称为一个“集合体”。zookeeper通过复制来实现高可用性，只要集合体中半数以上的机器处于可用状态，它就可以提供服务。出于这个原因，一个集合体通常包含奇数台机器。 zookeeper的安装zookeeper通常以“复制模式”运行于一个计算机集群上，这个计算机集群被称为一个“集合体”。zookeeper通过复制来实现高可用性，只要集合体中半数以上的机器处于可用状态，它就可以提供服务。出于这个原因，一个集合体通常包含奇数台机器。 安装本文选择了在cluster2，cluster3和cluster4三台机器上安装将zookeeper解压到/usr/local目录下，并配置环境变量# vi /etc/profile在最下面加上2行12export ZOOKEEPER_HOME=/usr/local/zookeeper-3.4.6export PATH=$ZOOKEEPER_HOME/bin:$PATH 然后在conf中新建zoo.cfg文件，输入以下内容：12345678910111213141516# 客户端心跳时间(毫秒)tickTime=2000# 允许心跳间隔的最大时间initLimit=10# 同步时限syncLimit=5# 数据存储目录dataDir=/home/hadoop_files/hadoop_data/zookeeper# 数据日志存储目录dataLogDir=/home/hadoop_files/hadoop_logs/zookeeper/dataLog# 端口号clientPort=2181# 集群节点和服务端口配置server.1=hadoop-cluster2:2888:3888server.2=hadoop-cluster3:2888:3888server.3=hadoop-cluster4:2888:3888 创建zookeeper的数据存储目录和日志存储目录123# mkdir -p /home/hadoop_files/hadoop_data/zookeeper# mkdir -p /home/hadoop_files/hadoop_logs/zookeeper/dataLog# mkdir -p /home/hadoop_files/hadoop_logs/zookeeper/logs 修改文件夹的权限12# chown -R hadoop:hadoop /home/hadoop_files# chown -R hadoop:hadoop /usr/local/zookeeper-3.4.6 在cluster2号服务器的data目录中创建一个文件myid，输入内容为1，myid应与zoo.cfg中的集群节点相匹配1# echo &quot;1&quot; &gt;&gt; /home/hadoop_files/hadoop_data/zookeeper/myid 修改zookeeper的日志输出路径# vi bin/zkEnv.sh12345678if [ &quot;x${ZOO_LOG_DIR}&quot; = &quot;x&quot; ]then ZOO_LOG_DIR=&quot;/home/hadoop_files/hadoop_logs/zookeeper/logs&quot;fiif [ &quot;x${ZOO_LOG4J_PROP}&quot; = &quot;x&quot; ]then ZOO_LOG4J_PROP=&quot;INFO,ROLLINGFILE&quot;fi 修改zookeeper的日志配置文件# vi conf/log4j.properties12zookeeper.root.logger=INFO,ROLLINGFILElog4j.appender.ROLLINGFILE=org.apache.log4j.DailyRollingFileAppender 将这个zookeeper-3.4.6的目录复制到其他的两个节点上12# scp -r /usr/local/zookeeper-3.4.6 cluster3:/usr/local/# scp -r /usr/local/zookeeper-3.4.6 cluster4:/usr/local/ 复制后在那两台机器上使用root用户修改目录所有者为hadoop用户，并修改他们的myid为2和3。 退回hadoop用户1# exit 然后使用hadoop用户，使用zkServer.sh start分别启动三个zookeeper，顺序无所谓。三个都启动后，使用jps命令查看，若有QuorumPeerMain则说明服务正常启动，没有的话，使用zkServer.sh start-foreground查看一下哪里出了问题。 安装中遇到的问题 zookeeper启动不了使用zkServer.sh start-foreground运行zookeeper，显示line 131:exec java: not found解决办法： 12# cd /usr/local# chown –R hadoop:hadoop zookeeper-3.4.6 改一下用户权限即可 打开logs文件夹里面的zookeeper.log显示connection refused错误原因：一般来说这是配置的问题，我出现这个问题的主要原因是，我在zoo.cfg中写了三个server，但是只在server1上启动zkServer.sh所以会出现connection refused。事实上只在一个机器上启动zookeeper时，使用zkServer.sh status查看状态时，会显示zk可能没有运行，但是这并不是说明你的zookeeper有问题，只是那两个还没启动好而已，当3台机器的zookeeper都启动后，3台机器会自动进行投票，选出一个leader两个follower，此时再用zkServer.sh status查看状态的时候就可以看到这台机器是leader还是follower了。","link":"/blog/2017/08/21/ha安装三：zookeeper的安装/"},{"title":"hexo yilia 主题","text":"发现了一个特别好看的主题，yilia，但是已经不维护了，坑还挺多的，记录一下。主题地址：https://github.com/litten/hexo-theme-yilia 首先git clone到themes目录中，记得要改名，目录名九教yilia，把前面的hexo-theme去掉 修改完hexo根目录下_config.yml里面的theme为yilia后，运行服务器，发现出现 123456&lt;%- partial(&apos;_partial/head&apos;) %&gt;&lt;%- partial(&apos;_partial/header&apos;) %&gt;&lt;%- body %&gt;&lt;% if (theme.sidebar &amp;&amp; theme.sidebar !== &apos;bottom&apos;){ %&gt; &lt;%- partial(&apos;_partial/sidebar&apos;) %&gt; &lt;% } %&gt;&lt;%- partial(&apos;_partial/footer&apos;) %&gt;&lt;%- partial(&apos;_partial/mobile-nav&apos;) %&gt; &lt;%- partial(&apos;_partial/after-footer&apos;) %&gt; 说明少装了插件，执行以下命令安装插件： 123npm install hexo-renderer-ejs --savenpm install hexo-renderer-stylus --savenpm install hexo-renderer-marked --save 进入文章后，头像就显示不出来了，这是源码中的bug。 修改themes\\yilia\\layout_partial\\left-col.ejs的第六行，改为 1&lt;img src=&quot;&lt;%=theme.root%&gt;&lt;%=theme.avatar%&gt;&quot; class=&quot;js-avatar&quot;&gt; 头像就设置为”img/avatar.jpg”即可。 同时，还要修改themes\\yilia\\layout_partial\\mobile-nav.ejs 里面的第10行，修改为 1&lt;img src=&quot;&lt;%=theme.root%&gt;&lt;%=theme.avatar%&gt;&quot; class=&quot;js-avatar&quot;&gt; 打赏的二维码也有这个问题 修改themes\\yilia\\layout_partial\\article.ejs 找到&lt;img class=”reward-img”这个标签，改后面src的值 支付宝的改成这个 1&lt;%=theme.root%&gt;&lt;%= theme.alipay%&gt; 微信的改成这个 1&lt;%=theme.root%&gt;&lt;%= theme.weixin%&gt; “所有文章”按钮的安装 首先使用命令 1node -v 检查版本是不是大于6.2 然后在hexo的配置文件_config.yml最下面加上 123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: false raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 重启服务器即可 分享功能有问题，我发现share_jia的微信分享不好使，就使用了Mob分享 先去官网注册账号http://mob.com/，然后申请shareSDK，会得到一个App Key 在yilia主题里面的_config.yml中的最后，加上 12sharesdk: true #是否开启分享shareSDKappkey: 你的App Key 然后在layout中的_partial中新建目录share， 创建文件：yilia/layout/_partial/share/share.ejs 123456789101112131415161718&lt;!--MOB SHARE BEGIN--&gt;&lt;div class=\"-mob-share-ui-button -mob-share-open\"&gt;分享&lt;/div&gt;&lt;div class=\"-mob-share-ui\" style=\"display: none\"&gt; &lt;ul class=\"-mob-share-list\"&gt; &lt;li class=\"-mob-share-weibo\"&gt;&lt;p&gt;新浪微博&lt;/p&gt;&lt;/li&gt; &lt;li class=\"-mob-share-tencentweibo\"&gt;&lt;p&gt;腾讯微博&lt;/p&gt;&lt;/li&gt; &lt;li class=\"-mob-share-qzone\"&gt;&lt;p&gt;QQ空间&lt;/p&gt;&lt;/li&gt; &lt;li class=\"-mob-share-qq\"&gt;&lt;p&gt;QQ好友&lt;/p&gt;&lt;/li&gt; &lt;li class=\"-mob-share-weixin\"&gt;&lt;p&gt;微信&lt;/p&gt;&lt;/li&gt; &lt;li class=\"-mob-share-twitter\"&gt;&lt;p&gt;Twitter&lt;/p&gt;&lt;/li&gt; &lt;li class=\"-mob-share-youdao\"&gt;&lt;p&gt;有道云笔记&lt;/p&gt;&lt;/li&gt; &lt;li class=\"-mob-share-mingdao\"&gt;&lt;p&gt;明道&lt;/p&gt;&lt;/li&gt; &lt;li class=\"-mob-share-linkedin\"&gt;&lt;p&gt;LinkedIn&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt;&lt;div class=\"-mob-share-close\"&gt;取消&lt;/div&gt;&lt;/div&gt;&lt;div class=\"-mob-share-ui-bg\"&gt;&lt;/div&gt;&lt;script id=\"-mob-share\" src=\"http://f1.webshare.mob.com/code/mob-share.js?appkey={{ theme.shareSDKappkey }}\"&gt;&lt;/script&gt;&lt;!--MOB SHARE END--&gt; 然后编辑layout/_partial/article.ejs 找个合适的位置加入以下内容 123&lt;% if (!index &amp;&amp; theme.sharesdk){ %&gt; &lt;%- partial(&apos;_partial/share/share.ejs&apos;) %&gt;&lt;% } %&gt; 左边昵称的字体有点丑，在themes\\yilia\\source\\main.0cf68a.css里面修改，找到header-author，修改里面的font-family，我改成了 1font-family:\"Times New Roman\",Georgia,Serif 之前的那个share_jia我修复了，之前微信分享之所以不成功，好像是因为百度网盘取消了生成二维码的功能，导致之前的链接不可用了。解决方法是修改themes\\yilia\\layout_partial\\post\\share.ejs 把第49行中的//pan.baidu.com/share/qrcode?url=修改为 1//api.qrserver.com/v1/create-qr-code/?size=150x150&amp;data= 如何在左侧显示总文章数？ 修改themes\\yilia\\layout_partial\\left-col.ejs 在 1234567&lt;nav class=&quot;header-menu&quot;&gt; &lt;ul&gt; &lt;% for (var i in theme.menu){ %&gt; &lt;li&gt;&lt;a href=&quot;&lt;%- url_for(theme.menu[i]) %&gt;&quot;&gt;&lt;%= i %&gt;&lt;/a&gt;&lt;/li&gt; &lt;%}%&gt; &lt;/ul&gt;&lt;/nav&gt; 后面加入 123&lt;nav&gt; 总文章数 &lt;%=site.posts.length%&gt;&lt;/nav&gt; 添加评论系统yilia默认带了几个系统，但我是从next这个主题转过来的，之前用的是来必力(livere)，不想换了，就得手动在yilia里面加。首先是去注册livere，然后获取到自己的id新建themes\\yilia\\layout_partial\\comment\\livere.ejs内容如下： 123456789101112131415161718&lt;!-- 来必力City版安装代码 --&gt;&lt;div id=&quot;lv-container&quot; data-id=&quot;city&quot; data-uid=&quot;&lt;%=theme.livere_uid%&gt;&quot;&gt;&lt;script type=&quot;text/javascript&quot;&gt; (function(d, s) { var j, e = d.getElementsByTagName(s)[0]; if (typeof LivereTower === &apos;function&apos;) { return; } j = d.createElement(s); j.src = &apos;https://cdn-city.livere.com/js/embed.dist.js&apos;; j.async = true; e.parentNode.insertBefore(j, e); })(document, &apos;script&apos;);&lt;/script&gt;&lt;noscript&gt;为正常使用来必力评论功能请激活JavaScript&lt;/noscript&gt;&lt;/div&gt;&lt;!-- City版安装代码已完成 --&gt; 然后编辑themes\\yilia\\layout_partial\\article.ejs找到这句话：&lt;% if (!index &amp;&amp; post.comments){ %&gt;在下面直接加入： 1234567&lt;% if (theme.livere){ %&gt;&lt;%- partial(&apos;comment/livere&apos;, {key: post.slug,title: post.title,url: config.url+url_for(post.path)}) %&gt;&lt;% } %&gt; 意思就是如果主题配置文件中有livere这个变量，且不为false，那就在下面加入comment/livere.ejs里面的内容所以接下来需要在主题配置文件(themes\\yilia_config.yml)中添加以下内容： 12livere: truelivere_uid: 你的id 添加字数统计首先需要安装hexo-wordcount使用如下命令安装 1npm i --save hexo-wordcount Node 版本7.6.0之前,请安装 2.x 版本 (Node.js v7.6.0 and previous)1npm install hexo-wordcount@2 --save 然后在themes\\yilia\\layout_partial\\left-col.ejs中添加 1总字数 &lt;span class=&quot;post-count&quot;&gt;&lt;%= totalcount(site, &apos;0,0.0a&apos;) %&gt;&lt;/span&gt; 编辑themes\\yilia\\layout_partial\\article.ejs在header下面加入 123&lt;div align=&quot;center&quot; class=&quot;post-count&quot;&gt; 字数：&lt;%= wordcount(post.content) %&gt;字 | 预计阅读时长：&lt;%= min2read(post.content) %&gt;分钟&lt;/div&gt; 即可显示单篇字数和预计阅读时长。 关于访问litten.me:9005的问题，这个主题的作者之前为了更好地完善这个主题，有时候会收集用户的客户端信息，详情请见https://github.com/litten/hexo-theme-yilia/issues/528，如果不想被统计，就将themes\\yilia\\source-src\\js\\report.js里面的内容清空，不过这个请求是异步的，不会影响博客加载速度，而且作者已经不维护了，所以关不关都行。","link":"/blog/2018/07/13/hexo-yilia-主题/"},{"title":"GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction","text":"IJCAI 2018，看了一部分，还没看完。原文链接：GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction Abstract大量的监测器被部署在各个地方，连续协同地监测周围的环境，如空气质量。这些检测器生成很多时空序列数据，之间有着空间相关性。预测这些时空数据很有挑战，因为预测受很多因素影响，比如动态的时空关联和其他因素。我们在这篇论文中使用多级基于注意力机制的 RNN 模型，结合空间、气象和检测器数据来预测未来的监测数值。我们的模型由两部分组成： 多级注意力机制对时空依赖关系建模 一个通用的融合模块对多领域的外部信息进行融合 实验用了两个真实的数据集，空气质量数据和水质监测数据，结果显示我们的模型比9个baselines都要好。 1 Introduction现实世界中有大量的检测器，如空气监测站。每个监测站都有一个地理位置，不断地生成时间序列数据。一组检测器不断的监测一个区域的环境，数据间有空间依赖关系。我们成这样的监测数据为 geosensory time series。此外，一个检测器生成多种 geo-sensory 时间序列是很常见的，因为这个检测器同时监测不同的目标。举个例子，图1a，路上的循环检测器实时记录车辆通行情况，也记录他们的速度。图1b 展示了检测器每五分钟生成的关于水质的三个不同的气候指标。除了监测，对于 geo-sensory 时间序列预测还有一个重要的需求就是交通预测。 然而，预测 geo-sensory 时间序列很有挑战性，主要受两个因素影响： 动态时空关系·检测器间复杂的关系。图1c展示了不同检测器的时间序列间的空间关系是高度动态的，随着时间不断改变。除此以外，geo-sensory时间序列根据地区有非线性的变化。当对动态关系建模时，传统方法（如概率图模型）的计算量会很大，因为他们有很多参数。·检测器内部的动态关系。首先，一个geo-sensory时间序列通常由一种周期模式（如，图1c中的$S_1$），这种模式一直变化，并且地理上也有改变。其次，检测器记录经常有很大的振动，很快地减少前一个数值的影响。因此，如何选择一个时间间隔来预测也是一个问题。 外部因素。检测器数据也被周围的环境影响，比如气象（例如强风），几点（比如早晚高峰）还有土地使用情况等。 为了解决这些挑战，我们提出了一个多级注意力网络（GeoMAN）来预测一个检测器未来几个小时的数值。我们的贡献有三点： 多级注意力机制。我们研发了一个多级注意力机制对动态时空关系建模。具体来说，第一级，我们提出了一个新的注意力机制，由局部空间注意力和全局空间注意力组成，用来捕获不同检测器时间序列间的复杂空间关系。第二级，时间注意力对一个时间序列中的动态时间关系进行建模。 外部因素融合模型。我们设计了一个通用融合模型融合不同领域的外部因素。学习到的隐含表示会输入至多级注意力网络中来提升这些外部因素的重要性。 真实的评价。我们基于两个真实的数据集评估我们的方法。大量的实验证明了我们的方法相比于baseline的优越性。 2 Preliminary2.1 Notations假设，有 $N_g$ 个检测器，每个都生成 $N_l$ 种时间序列。我们指定其中的一个为 target series 来预测，其它序列作为特征。时间窗为 $T$，我们使用 $\\mathbf{Y} = (\\mathbf{y}^1, \\mathbf{y}^2, …, \\mathbf{y}^{N_g}) \\in \\mathbb{R}^{N_g \\times T}$ 来表示所有目标序列在过去 $T$ 个小时的监测值，其中 $\\mathbf{y}^i \\in \\mathbb{R}^T$ 属于监测器 $i$。我们使用 $\\mathbf{X}^i = (\\mathbf{x}^{i, 1}, \\mathbf{x}^{i, 2}, …, \\mathbf{x}^{i, N_l})^{\\rm T} = (\\mathbf{x}^i_1, \\mathbf{x}^i_2, …, \\mathbf{x}^i_T) \\in \\mathbb{R}^{N_l \\times T}$ 作为检测器 $i$ 的局部特征，其中 $\\mathbf{x}^{i,k} \\in \\mathbb{R}^T$ 是这个检测器的第 $k$ 个时间序列，$\\mathbf{x}^i_t = (x^{i,1}_t, x^{i,2}_t, …, x^{i,N_l}_t)^{\\rm T} \\in \\mathbb{R}^{N_l}$ 表示检测器 $i$ 在时间 $t$ 的所有时间序列的值。除了检测器 $i$ 的局部特征，由于不同检测器间的空间关系，其他的检测器会共享大量对于预测有用的信息。为了这个目的，我们将每个检测器的局部特征融合到集合 $\\mathcal{X}^i = \\lbrace \\mathbf{X}^1, \\mathbf{X}^2, …, \\mathbf{X}^{N_g}\\rbrace$ 中作为检测器 $i$ 的全局特征。 2.2 Problem Statement给定每个检测器之前的值和外部因素，预测检测器 $i$ 在未来 $\\tau$ 个小时的值，表示为 $\\hat{\\mathbf{y}}^i = (\\hat{y}^i_{T+1}, \\hat{y}^i_{T+2}, …, \\hat{y}^i_{T+\\tau})^{\\rm T} \\in \\mathbb{R}^{\\tau}$. 3 Multi-level Attention Networks 图 2 展示了我们方法的框架。基于编码解码架构[Cho et al., 2014b]，我们用两个分开的 LSTM，一个对输入序列编码，也就是对历史的 geo-sensory 时间序列，另一个来预测输出的序列 $\\hat{y}^i$。更具体的讲，我们的模型 GeoMAN 有两个主要部分： 多级注意力机制。包含一个带有两类空间注意力机制的编码器和一个带有时间注意力的解码器。在编码器，我们开发了两种不同的注意力机制，局部空间注意力和全局空间注意力，如图 2 所示，这两种注意力机制通过前几步编码器的隐藏状态、前几步检测器的值和空间信息（检测器网络），可以在每个时间步上捕获检测器间的复杂关系。在解码器，我们使用了一个时间注意力机制来自适应地选择之前的时间段来做预测。 外部因素融合。这个模块用来处理外部因素的影响，输出会作为解码器的一部分输入。这里，我们使用 $h_t \\in \\mathbb{R}^m$ 和 $s_t \\in \\mathbb{R}^m$ 来表示编码器在时间 $t$ 的隐藏状态和细胞状态。$d_t \\in \\mathbb{R}^n$ 和 $s’ \\in \\mathbb{R}^n$ 表示解码器的隐藏状态和细胞状态。 3.1 Spatial AttentionLocal Spatial Attention我们先介绍空间局部注意力机制。对应一个监测器，在它的局部时间序列上有复杂的关联性。举个例子，一个空气质量检测站会记录不同的时间序列如 PM2.5，NO 和 SO2。事实上，PM2.5 的浓度通常被其他时间序列影响，包括其他的空气污染物和局部空气质量 [Wang et al., 2005]。为了解决这个问题，给定第 $i$ 个检测器第 $k$ 个局部特征向量 $\\mathbf{x}^{i,k}$，我们使用注意力机制自适应地捕获目标序列和每个局部特征间的动态关系： $$\\tag{1}e^k_t = \\mathbf{v}^T_l \\text{tanh} (\\mathbf{W}_l [\\mathbf{h}_{t-1};\\mathbf{s}_{t-1}] + \\mathbf{U}_l \\mathbf{x}^{i,k} + \\mathbf{b}_l)$$ $$\\tag{2}\\alpha^k_t = \\frac{\\text{exp}(e^k_t)}{\\sum^{N_l}_{j=1}\\text{exp}(e^j_t)}.$$ 其中 $[\\cdot;\\cdot]$ 是拼接操作（论文这里写的是 concentration，我怎么觉得是concatenation呢。。。）。$\\mathbf{v}_l, \\mathbf{b}_l \\in \\mathbb{R}^T, \\mathbf{W}_l \\in \\mathbb{R}^{T \\times 2m}, \\mathbf{U}_l \\in \\mathbb{R}^{T \\times T}$ 是参数。局部特征的注意力权重通过编码器中输入的局部特征和历史状态共同决定。这个注意力分数语义上表示每个局部特征的重要性，局部空间注意力在时间步 $t$ 的输出向量通过下式计算： $$\\tag{3}\\tilde{\\mathbf{x}}^{local}_t = (\\alpha^1_t x^{i,1}_t, \\alpha^2_t x^{i,2}_t, \\dots, \\alpha^{N_l}_t x^{i,N_l}_t)^{\\rm T}.$$ Global Spatial Attention对于一个监测器记录的目标时间序列，其他监测器是时间序列对其有直接影响。然而，影响权重是高度动态地，随时间变化。因为可能有很多不相关的序列，直接使用各种时间序列作为编码器的输入来捕获不同监测器之间的关系会导致很高的计算开销并且降低模型的能力。而且这样的影响权重受其他监测器的局部条件影响。举个例子，当强风从一个遥远地地方吹过来，这个区域的空气质量回比之前受影响的多。受这个现象的启发，我们开发了一个新的注意力机制捕获不同监测器间的动态关系。给定第 $i$ 个监测器作为我们的预测目标，另一个监测器 $l$，我们计算他们之间的注意力分数如下： $$g^l_t = \\mathbf{v}^{\\rm T}_g \\text{tanh} (\\mathbf{W}_g [\\mathbf{h}_{t-1}; \\mathbf{s}_{t-1}] + \\mathbf{U}_g \\mathbf{y}^l + \\mathbf{W}’_g \\mathbf{X}^l \\mathbf{u}_g + \\mathbf{b}_g),$$ 其中 $\\mathbf{v}_g, \\mathbf{u}_g, \\mathbf{b}_g \\in \\mathbb{R}^T, \\mathbf{W}_g \\in \\mathbb{R}^{T \\times 2m}, \\mathbf{U}_g \\in \\mathbb{R}^{T \\times T}, \\mathbf{W}’_g \\in \\mathbb{R}^{T \\times N_l}$ 是参数。通过考虑目标序列和其他检测器的局部特征，这个注意力机制可以自适应地选择相关的监测器来做预测。同时，通过考虑编码器内前一隐藏状态和细胞状态，历史信息会跨时间流动。 注意，空间因素也会对不同监测器之间的关系做出贡献。一般来说，geo-sensors 通过一个明确的或隐含的网络连接起来。这里，我们使用一个矩阵 $\\mathbf{P} \\in \\mathbb{R}^{N_g \\times N_g}$ 来衡量地理空间相似度（如地理距离的倒数），$P_{i,j}$ 表示监测器 $i$ 和 $j$ 之间的相似度。不同于注意力权重，地理相似度可以看作是先验知识。特别的说，如果 $N_g$ 很大，一个方法是使用最近邻或最相近的一组而不是所有的监测器。之后，我们使用一个 softmax 函数，确定所有的注意力权重之和为1，两个方法共同考虑地理相似度如下： $$\\tag{4}\\beta^l_t = \\frac{\\text{exp}((1-\\lambda)g^l_t + \\lambda P_{i,l})}{\\sum^{N_g}_{j=1} \\text{exp}((1-\\lambda)g^j_t + \\lambda P_{i,j})},$$ 其中，$\\lambda$ 是一个可调的超惨。如果 $\\lambda$ 大，这项会强制注意力权重等于地理相似度。全局注意力的输出向量计算如下： $$\\tag{5}\\tilde{\\mathbf{x}}^{global}_t = (\\beta^1_t y^1_t, \\beta^2_t y^2_t, \\dots, \\beta^{N_g}_t y^{N_g}_t)^{\\rm T}.$$ 3.2 Temporal Attention因为编码解码结构会随着长度增长会很快的降低性能，一个重要的扩展是增加时间注意力机制，可以自适应地选择编码器相关的隐藏状态来生成输出序列，即模型对目标序列中不同时间间隔的动态时间关系建模。特别来说，为了计算每个输出时间 $t’$ 对编码器每个隐藏状态的的注意力向量，我们定义： $$\\tag{6}u^o_{t’} = \\mathbf{v}^{\\rm T}_d \\text{tanh} (\\mathbf{W}’_d [\\mathbf{d}_{t’-1}; \\mathbf{s}’_{t’-1}] + \\mathbf{W}_d \\mathbf{h}_o + \\mathbf{b}_d),$$ $$\\tag{7}\\gamma^o_{t’} = \\frac{\\text{exp} (u^o_{t’})}{\\sum^T_{o=1} \\gamma^o_{t’} \\mathbf{h}_o},$$ $$\\tag{8}\\mathbf{c}_{t’} = \\sum^T_{o=1} \\gamma^o_{t’} \\mathbf{h}_o,$$ 3.3 External Factor FusionGeo-sensory 时间序列和空间因素有强烈的相关性，如 POI 和监测器网络。这些因素一起表示一个区域的功能。而且还有很多时间因素影响监测器的数值，如气象或时间。受之前工作的启发 [Liang et al., 2017; Wang et al., 2018] 专注时空应用中的外部因素的影响，我们设计了一个简单有效的组件来处理这些因素。 如图 2 所示，我们先将包含时间、气象特征的时间因素和表示目标监测器的监测器ID融合。因为未来的天气条件未知，我们使用天气预报来提升性能。这些因素的大部分都是离散特征，不能直接放到神经网络里面，我们通过将离散特征分开放入不同的嵌入层，将离散特征转换为低维向量。根据空间因素，我们使用不同 POI 类型的密度作为特征。因为监测器的属性依赖实际情况，我们只使用网络的结构特征，如邻居数和交集等。最后，我们将获得的嵌入向量和空间特征拼接作为这个模块的输出，表示为 $\\mathbf{ex}_{t’} \\in \\mathbb{R}^{N_e}$，其中 $t’$ 是解码器中未来的时间步。 3.4 Encoder-decoder &amp; Model Training编码器中，我们简单地从局部空间注意力和全局空间注意力聚合输出： $$\\tag{9}\\tilde{\\mathbf{x}}_t = [\\tilde{\\mathbf{x}}^{local}_t; \\tilde{\\mathbf{x}}^{global}_t],$$ 其中 $\\tilde{\\mathbf{x}}_t \\in \\mathbb{R}^{N_l + N_g}$。我们将拼接 $\\tilde{\\mathbf{x}}_t$ 作为编码器的新输入，使用 $\\mathbf{h}_t = f_e(\\mathbf{h}_{t-1}, \\tilde{\\mathbf{x}}_t)$ 更新时间 $t$ 的隐藏状态，$f_e$ 是一个 LSTM 单元。 解码器中，一旦我们获得了时间 $t’$ 的上下文向量 $\\mathbf{c}_{t’}$ 的带权和，我们将他与外部因素融合模块的输出 $\\mathbf{ex}_{t’}$ 还有解码器的最后一个输出 $\\hat{y}^i_{t’-1}$ 融合，用 $\\mathbf{d}_{t’} = f_d (\\mathbf{d}_{t’-1}, [\\hat{y}^i_{t’-1}; \\mathbf{ex}_{t’}; \\mathbf{c}_{t’}])$ 更新解码器的隐藏状态，$f_d$ 是解码器中使用的 LSTM 单元。然后，我们讲上下文向量 $\\mathbf{c}_{t’}$ 和隐藏状态 $\\mathbf{d}_{t’}$ 拼接，得到新的隐藏状态，然后做最后的预测： $$\\tag{10}\\hat{y}^i_{t’} = \\mathbf{v}^{\\rm T}_y (\\mathbf{W}_m [\\mathbf{c}_{t’}; \\mathbf{b}_{t’}] + \\mathbf{b}_m) + b_y,$$ 其中，$\\mathbf{W}_m \\in \\mathbb{R}^{n \\times (m + n))}$ 和 $\\mathbf{b}_m \\in \\mathbb{R}^n$ 将 $[\\mathbf{c}_{t’}; \\mathbf{d}_{t’}] \\in \\mathbb{R}^{m + n}$ 映射到解码器隐藏状态的空间。最后，我们用一个线性变换生成最终结果。 因为我们的方法是平滑且可微的，可以通过反向传播训练。在训练时，我们使用 Adam，最小化 MSE。 $$\\tag{11}\\mathcal{L}(\\theta) = \\Vert \\hat{\\mathbf{y}}^i - \\mathbf{y}^i \\Vert^2_2,$$ 4 Experiments4.1 SettingsDatasets我们在两个数据集中开展了实验，每个数据集包含三个子集：气象数据、POI、监测器网络数据。 水质数据集：中国东南的一个城市的供水系统中的监测器提供了长达三年的每5分钟一个记录的数据，包含了残余氯(RC)、浑浊度和PH值等。我们将 RC 作为目标时间序列，因为它在环境科学中作为常用的水质指标。一共有 14 个监测器，监测 10 个指标，它们之间通过管道网络相连。我们使用 Liu et al., 2016a 提出的指标作为这个数据集的相似度矩阵。 空气质量：从一个公开数据集抓取的，这个数据集包含不同污染物的浓度，还有气象数据，北京地区一共 35 个监测器。主要污染物是 PM2.5，因此我们将它作为目标。我们只使用空间距离的倒数表示两个监测器之间的相似度。 对于水质数据集，我们将数据分成了不重叠的训练集、验证集和测试集，去年的前一半作为验证机，去年的后半段作为测试集。可惜的是，我们在第二个数据集上没能获得很多的数据，因此我们使用了8：1：1的比例划分。 Evaluation Metrics我们使用多个标准评价模型，RMSE 和 MAE。 Hyperparameters我们令 $\\tau = 6$，做短期预测。在训练阶段，batch size 256，学习率 0.001。外部因素融合模块，我使用 $\\mathbb{R}^6$ 嵌入监测器 ID，时间特征 $\\mathbb{R}^10$。我们的模型一共 4 个超参数，$\\lambda$ 根据经验设置，从 0.1 到 0.5。对于窗口长度 $T$，我们设为 $T \\in \\lbrace 6, 12, 24, 36, 48 \\rbrace$。为了简单，我们将编码器和解码器采用同样的隐藏维数，网格搜索 $\\lbrace 32, 64, 128, 256\\rbrace$。我们堆叠 LSTM 来提高性能，层数记为 $q$。验证集上得到的最好参数是 $q = 2, m = n = 64, \\lambda = 2$。 4.2 BaselinesARIMA, VAR, GBRT, FFA, stMTMVL, stDNN, LSTM, Seq2seq, DA-RNN。 对于 ARIMA，我们用前六个小时的数据作为输入。stMTMVL 和 FFA，我们使用和作者一样的设置。和 GeoMAN 类似，我们使用前 $T \\in \\lbrace 6, 12, 24, 36, 48\\rbrace$ 个小时的数据作为其他模型的输入。最后，我们测试了不同的超参数，得到了每个模型的最好效果。 4.3 Model Comparsion我们在两个数据集上比较了模型和 baselines。为了公平，我们在表 2 展示了每个方法的最好性能。 我们的方法在水质预测上得到了最好的性能。比 state-of-the-art 的 DA-RNN 在两个指标上分别提升了 14.2% 和 13.5%。因为 RC 浓度有一个确定的周期模式，stDNN 和 基于 RNN 的模型比 stMTMVL 和 FFA 获得了更好的效果，因为他们能捕获更长的时间依赖。对比 LSTM 智能预测一个未来的时间步，GeoMAN 和 Seq2seq 由于解码器的存在有很大的提升。GBRT 比大部分方法也要好，体现了集成学习的优势。 对比数据相对稳定的水质数据集，PM2.5 的浓度有些时候震荡得很厉害，使得很难预测。表 2 展示了北京的空气质量数据集上一个全面的对比。可以看到我们的模型有最好的效果。我们主要讨论下 MAE。我们的方法比这些方法的 MAE 相对低 7.2% 和 63.5%，展示出了比其他方法更好的泛化效果。另一个有趣的现象是 stMTMVL 在水质预测上表现很好，在空气质量上","link":"/blog/2018/07/09/geoman-multi-level-attention-networks-for-geo-sensory-time-series-prediction/"},{"title":"Image Style Transfer Using Convolutional Neural Networks","text":"CVPR 2016，大体原理：选择两张图片，一张作为风格图片，一张作为内容图片，任务是将风格图片中的风格，迁移到内容图片上。方法也比较简单，利用在ImageNet上训练好的VGG19，因为这种深层次的卷积神经网络的卷积核可以有效的捕捉一些特征，越靠近输入的卷积层捕捉到的信息层次越低，而越靠近输出的卷积层捕捉到的信息层次越高，因此可以用高层次的卷积层捕捉到的信息作为对风格图片风格的捕捉。而低层次的卷积层用来捕捉内容图片中的内容。所以实际的操作就是，将内容图片扔到训练好的VGG19中，取出低层次的卷积层的输出，保存起来，然后再把风格图片放到VGG19中，取出高层次的卷积层的输出，保存起来。然后随机生成一张图片，扔到VGG19中，将刚才保存下来的卷积层的输出的那些卷积层的结果拿出来，和那些保存的结果做个loss，然后对输入的随机生成的图片进行优化即可。原文链接：Image Style Transfer Using Convolutional Neural Networks Image Style Transfer Using Convolutional Neural Networks大体原理选择两张图片，一张作为风格图片，一张作为内容图片，任务是将风格图片中的风格，迁移到内容图片上。方法也比较简单，利用在ImageNet上训练好的VGG19，因为这种深层次的卷积神经网络的卷积核可以有效的捕捉一些特征，越靠近输入的卷积层捕捉到的信息层次越低，而越靠近输出的卷积层捕捉到的信息层次越高，因此可以用高层次的卷积层捕捉到的信息作为对风格图片风格的捕捉。而低层次的卷积层用来捕捉内容图片中的内容。所以实际的操作就是，将内容图片扔到训练好的VGG19中，取出低层次的卷积层的输出，保存起来，然后再把风格图片放到VGG19中，取出高层次的卷积层的输出，保存起来。然后随机生成一张图片，扔到VGG19中，将刚才保存下来的卷积层的输出的那些卷积层的结果拿出来，和那些保存的结果做个loss，然后对输入的随机生成的图片进行优化即可。(Fig2) Figure 2. Style transfer algorithm. First content and style features are extracted and stored. The style image $\\vec{a}$ is passed through the network and its style representation $A^l$ on all layers included are computed and stored(left). The content image $\\vec{p}$ is passed through the network and the content representation $P^l$ in one layer is stored(right). Then a random white noise image $\\vec{x}$ is passed through the network and its style features $G^l$ and content features $F^l$ are computed. On each layer included in the style representation, the element-wise mean squared difference between $G^l$ and $A^l$ is computed to give the style loss $\\mathcal{L}_{style}$(left). Also the mean squared difference between $F^l$ and $P^l$ is computed to give the content loss $\\mathcal{L}_{content}(right)$. The total loss $\\mathcal{L}_{total}$ is then a linear combination between the content and the style loss. Its derivative with respect to the pixel values can be computed using error back-propagation(middle). This gradient is used to iteratively update the image $\\vec{x}$ until it simultaneously matches the style features of the style image $\\vec{a}$ and the content features of the content image $\\vec{p}$(middle, bottom). Deep image representationsWe used the feature space provided by a normalized version of the 16 convolutional and 5 pooling layers of the 19-layer VGG network. We normalized the network by scaling the weights such that the mean activation of each convolutional filter over images and positions is equal to one. Such re-scaling can be done for the VGG network without changing its output, because it contains only rectifying linear activation functions and no normalization or pooling over feature maps.其实这里我不是很明白为什么不会影响输出。 content representationA layer with $N_l$ distinct filters has $N_l$ feature maps each of size $M_l$, where $M_l$ is the height times the width of the feature map. So the responses in a layer $l$ can be stored in a matrix $F^l \\in \\mathcal{R}^{N_l \\times M_l}$ where $F^l_{ij}$ is the activation of the $i^{th}$ filter at position $j$ in layer $l$.Let $\\vec{p}$ and $\\vec{x}$ be the original image and the image that is generated, and $P^l$ and $F^l$ their respective feature representation in layer $l$.We then define the squared-error loss between the two feature representations$$\\mathcal{L}_{content}(\\vec{p}, \\vec{x}, l) = \\frac{1}{2}\\sum_{i, j}(F^l_{ij}-P^l_{ij})^2$$The derivative of this loss with respect to the activations in layer $l$ equals \\begin{equation}\\frac{\\partial{\\mathcal{L}_{content}}}{\\partial{F^l_{ij}}}=\\left{\\begin{aligned}&amp; (F^l - P^l)_{ij} &amp; if \\ F^l_{ij} &gt; 0 \\\\&amp; 0 &amp; if \\ F^l_{ij} &lt; 0\\end{aligned}\\right.\\end{equation} from which the gradient with respect to the image $\\vec{x}$ can be computed using standard error back-propagation. When Convolutional Neural Networks are trained on object recongnition, they develop a representation of the image that makes object information increasingly explicit along the processing hierarchy. Higher layers in the network capture the high-level content in terms of objects and their arrangement in the input image but do not constrain the exact pixel values of the reconstruction very much. We therefore refer to the feature responses in higher layers of the network as the content representation. style representationTo obtain a representation of the style of an input image, we use a feature space designed to capture texture information. This feature space can be built on top of the filter responses in any layer of the network. It consists of the correlations between the different filter responses, where the expecation is taken over the spatial extent of the feature maps. These feature correlations are given by the Gram matrix $G^l \\in \\mathcal{R}^{N_l \\times N_l}$, where $G^l_{ij}$ is the inner product between the vecotrized feature maps $i$ and $j$ in layer $l$:$$G^l_{ij}=\\sum_kF^l_{ik}F^l_{jk}.$$By inducing the feature corelations of multiple layers, we obtain a stationary, multi-scale representation of the input image, which captures its texture information but not the global arrangement. Again, we can visualise the information captured by these style feature spaces built on different layers of the network by constructing an image that matches the style representation of a given input image. This is done by using gradient descent from a white noise image to minimise the mean-squared distance between the entries of the Gram matrices from the original image and the Gram matrices of the image to be generated.Let $\\vec{a}$ and $\\vec{x}$ be the original image and the image that is generated, and $A^l$ and $G^l$ their respective style representation in layer $l$. The contribution of layer $l$ to the toal loss is then$$E_l = \\frac{1}{4N^2_lM^2_l}\\sum_{i,j}(G^l_{ij} - A^l_{ij})^2$$and the total style loss is$$\\mathcal{L}_{style}(\\vec{a}, \\vec{x})=\\sum^L_{l=0}w_lE_l,$$where $w_L$ are weighting factors of the contribution of each layer to the total loss (see below for specific values of $w_l$ in our results). The derivative of $E_l$ with respect to the activations in layer $l$ can be computed analytically: \\begin{equation}\\frac{\\partial{E_l}}{\\partial{F^l_{ij}}}=\\left{\\begin{aligned}&amp; \\frac{1}{N^2_lM^2_l}((F^l)^T(G^l-A^l))_{ji} &amp; if \\ F^l_{ij} &gt; 0 \\\\&amp; 0 &amp; if \\ F^l_{ij} &lt; 0\\end{aligned}\\right.\\end{equation}The gradient of $E_l$ with respect to the pixel values $\\vec{x}$ can be readily computed using standard error back-propagation. style transferTo transfer the style of an artwork $\\vec{a}$ onto a photograph $\\vec{p}$ we synthesise a new image that simultaneously matches the content representation of $\\vec{p}$ and the style representation of $\\vec{a}$. Thus we jointly minimise the distance of the feature representations of a white noise image fron the content representation of the photograph in one layer and the style representation of the painting defined on a numebr of layers of the Convolutional Neural Network. The loss function we minimise is$$\\mathcal{L}_{total}(\\vec{p}, \\vec{a}, \\vec{x})=\\alpha \\mathcal{L}_{content}(\\vec{p}, \\vec{x}) + \\beta \\mathcal{L}_{style}(\\vec{a}, \\vec{x})$$where $\\alpha$ and $\\beta$ are the weighting factors for content and style reconstruction, respectively. The gradient with respect to the pixel values $\\frac{\\partial{\\mathcal{L}_{total}}}{\\partial{\\vec{x}}}$ can be used as input for some numerical optimisation strategy. Here we use L-BFGS, which we found to work best for image synthesis. To extract image information on comparable scales, we always resized the style image to the same size as the content image before computing its feature representations. ResultsTrade-off between content and style matchingSince the loss function we minimise during image synthesis is a linear combination between the loss functions for content and style respectively, we can smoothly regulate the emphasis on either reconstructing the content or the style(Fig4). Figure 4. Relative weighting of matching content and style of the respective source images. The ratio $\\alpha / \\beta$ between matching the content and matching the style increases from top left to bottom right. A high emphasis on the style effectively produces a texturised version of the style image(top left). A high emphasis on the content produces an image with only little stylisation(bottom right). In practice one can smoothly interpolate between the two extremes. Effect of different layers of the Convolutional Neural Network Figure 5. The effect of matching the content representation in different layers of the network. Matching the content on layer ‘conv2_2’ preserves much of the fine structure of the original photograph and the synthesised image looks as if the texture of the painting is simply blended over the photograph(middle). When matching the content on layer ‘conv4_2’ the texture of the painting and the content of the photograph merge together such that the content of photograph is displayed in the style of the painting(bottom). Both images were generated with the same choice of parameters($\\alpha / \\beta = 1 \\times 10^{-3}$). The painting that served as the style image is shown in the bottom left corner and is name Jesuiten Ⅲ by Lyonel Feininger, 1915.Another important factor in the image synthesis process is the choice of layers to match the content and style representation on. As outlined above, the style representation is a multi-scale representation that includes multiple layers of the neural network. The number and position of these layers determines the local scale on which the style is matched, leading to different visual experiences. We find that matching the style representations up to higher layers in the network preserves local images structures an increasingly large scale, leading to a smoother and more continuous visual experience. Thus, the visually most appealing images are usually created by matching the style representation up to high layers in the network, which is why for all images shown we match the style features in layers ‘conv1_1’, ‘conv2_1’, ‘conv3_1’, ‘conv4_1’and ‘conv5_1’ of the network.To analyse the effect of using different layers to match the content features, we present a style transfer result obtained by stylising a photograph with the same artwork and parameter configuration ($\\alpha / \\beta = 1 \\times 10^{-3}$), but in one matching the content features on layer ‘conv2_2’ and in the other on layer ‘conv4_2’(Fig5). When matching the content on a lower layer of the network, the algorithm matches much of the detailed pixel information in the photograph and the generated image appears as if the texture of the artwork is merely blended over the photograph(Fig5, middle). In contrast, when matching the content features on a higher layer of the network, deatiled pixel information of the photograph is not as strongly constraint and the texture of the artwork and the content of the photograph are properly merged. That is, the fine structure of the image, for example the edges and colour map, is altered such that it agrees with the style of the artwork while displaying the content of the photograph(Fig5, bottom). Initialisation of gradient descent Figure 6. Initialisation of the gradient descent. A Initialised from the content image. B Initialised from the style image. C Four samples of images initialised from different white noise images. For all images the ratio $\\alpha / \\beta$ was equal to $1 \\times 10^{-3}$We have initialised all images shown so far with white noise. However, one could also initialise the image synthesis with either the content image or the style image. We explored these two alternatives(Fig6 A, B): although they bias the final image somewhat towards the spatial structure of the initialisation, the different intialisation do not seem to have a strong effect on the outcome of the synthesis procedure. It should be noted that only initialising with noise allows to generate an arbitrary number of new images(Fig6 C). Initialising with a fixed image always deterministically leads to the same outcome (up to stochasticity in the gradient descent procedure). implementation关于实现的部分，我自己用mxnet实现了一下，但是发现和mxnet的example里面给的非常不一样。在他们的实现里面提到了Total variation denoising。而且，论文中的loss function是sum of square，而图2中给出是MSE，取了个平均值。我实现是时候没有取平均，导致loss很大，但是也可以训练。但是自己实现的梯度下降很难收敛，需要对梯度进行归一化，后来使用MXNet的gluon的Trainer训练会比原来好很多。 Total variation denoisingIn signal processing, total variation denoising, also known as total variation regularization, is a process, most often used in digital image processing, that has applications in noise removal. Example of application of the Rudin et al.[1] total variation denoising technique to an image corrupted by Gaussian noise. This example created using demo_tv.m by Guy Gilboa, see external links.It is based on the principle that signals with excessive and possibly spurious detail have high total variation, that is, the integral of the absolute gradient of the signla is high. According to this principle, reducing the total variation of the signal subject to it being a close match to the original signal, removes unwanted detail whilst preserving important details such as edges. The concept was pioneered by Rudin, Osher, and Fatemi in 1992 and so is today known as the ROF model.This noise removal technique has advantages over simple techniques such as linear smoothing or median filtering which reduce noise but at the same time smooth away edges to a greater or lesser degree. By contrast, total variation denoising is remarkably effective at simultaneously preserving edges whilst smoothing away noise in flat regions, even at low signal-to-noise ratios. 1D signal seriesFor a digital signal $y_n$, we can, for example, define the total variation as:$$V(y)=\\sum_n\\vert y_{n+1}-y_n\\vert$$Given an input signal $x_n$, the goal of total variation denoising is to find an approximation, call it $y_n$, that has smaller total variation than $x_n$ but is “close” to $x_n$. One measure of closeness is the sum of square errors:$$E(x, y)=\\frac{1}{2}\\sum_n(x_n - y_n)^2$$So the total variation denoising problem amounts to minimizing the following discrete functional over the signal $y_n$:$$E(x, y) + \\lambda V(y)$$By differentiating this functional with respect to $y_n$, we can derive a corresponding Euler-lagrange equation, that can be numerically integrated with the original signal $x_n$ as initial condition. This was the original approach. Alternatively, since this is a convex functional, techniques from convex optimization can be used to minimize it and find the solution $y_n$. Regularization propertiesThe regularization parameter $\\lambda $ plays a critical role in the denoising process. When $\\lambda = 0$, there is no smoothing and the result is the same as minimizing the sum of squares. As $\\lambda \\to \\infty $, however, the total variation term plays an increasingly strong role, which forces the result to have smaller total variation, at the expanse of being less like the input (noisy) signal. Thus, the choice of regularization parameter is critical to achieving just the right amount of noise removal. 2D signal imagesWe now consider 2D signals $y$, such as images. The total variation norm proposed by the 1992 paper is$$V(y) = \\sum_{i,j}\\sqrt{\\vert y_{i+1,j}-y_{i,j}\\vert ^2 + \\vert y_{i, j+1} - y_{i, j}\\vert ^2}$$and is isotropic and not differentiable. A variation that is sometimes used, since it may sometimes be easier to minimize, is an anisotropic version$$V_{aniso}(y) = \\sum_{i,j}\\sqrt{\\vert y_{i+1,j}-y_{i,j}\\vert ^2} + \\sqrt{\\vert y_{i,j+1} - y_{i,j}\\vert ^2} = \\sum_{i,j}\\vert y_{i+1,j}-y_{i,j}\\vert + \\vert y_{i,j+1}-y_{i,j}\\vert $$The standard total variation denoising problem is still of the form$$\\min_yE(x,y)+\\lambda V(y)$$where $E$ is the 2D L2 norm. In contrast to the 1D case, solving this denoising is non-trivial. A recent algorithm that solves this is known as the primal dual method.Due in part to much research in compressed sensing in the mid-2000s, there are many algorithms, such as the split-Bregman method, that solve variants of this problem. 不过我个人在实现的时候，实现了两个版本，一个是增加了total variation denoising，另一个是没增加total variation denoising的a。代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117import mxnet as mxfrom skimage import iofrom skimage import transformfrom mxnet import ndimport numpy as npimport matplotlib.pyplot as plt%matplotlib inlinefrom mxnet.gluon.model_zoo import vision as modelsfrom mxnet.gluon import nnfrom mxnet import autogradfrom mxnet.gluon import Trainerfrom mxnet.gluon import Parameterimport warningswarnings.filterwarnings(\"ignore\")content_image_path = '../../gluon-tutorial-zh/img/pine-tree.jpg'style_image_path = 'the_starry_night.jpg'rgb_mean = np.array([0.485, 0.456, 0.406])rgb_std = np.array([0.229, 0.224, 0.225])def preprocessing(img, image_shape, ctx = mx.cpu()): newImage = transform.resize(img, image_shape) newImage = newImage.transpose((2, 0, 1)) newImage = (newImage - rgb_mean.reshape(3, 1, 1)) / rgb_std.reshape(3, 1, 1) return nd.array(np.expand_dims(newImage, 0), ctx = ctx)def postprocessing(img): newImage = img[0].asnumpy() * rgb_std.reshape(3, 1, 1) + rgb_mean.reshape(3, 1, 1) return newImage.transpose((1, 2, 0)).clip(0, 1)def get_net(style_layers, content_layers): net = nn.HybridSequential() for i in range(max(style_layers + content_layers) + 1): net.add(pretrained_net.features[i]) net.hybridize() return netdef extract_features(net, img, content_layers, style_layers): x = img.copy() content_results = [] style_results = [] for i in range(len(net)): x = net[i](x) if i in content_layers: content_results.append(x) if i in style_layers: style_results.append(x) return content_results, style_resultsdef content_loss(content_results, content_target): losses = [] for i in range(len(content_results)): losses.append((content_results[i] - content_target[i]).square().sum()) return nd.add_n(*losses) / 2def gram(feature_map): N = feature_map.shape[1] M = np.prod(feature_map.shape[2:]) new_feature_map = feature_map.reshape((N, M)) return nd.dot(new_feature_map, new_feature_map.T)def style_loss(style_results, style_target, weights): losses = [] for i in range(len(style_results)): l = (gram(style_results[i]) - style_target[i]).square().sum() \\ / (4 * np.prod(style_results[i].shape[1:])) losses.append(weights[i] * l) return nd.add_n(*losses)def get_loss(content_loss_result, style_loss_result, ratio): return content_loss_result * ratio + style_loss_resultstyle_layers = [2, 7, 16, 25, 34] # 这里与论文不同，我选的层比论文给出的更深，为了捕捉到更抽象的stylecontent_layers = [21]net = get_net(style_layers, content_layers)content_image = io.imread(content_image_path)style_image = io.imread(style_image_path)pretrained_net = models.vgg19(pretrained=True)ctx = mx.gpu(1)net.collect_params().reset_ctx(ctx)content_img = preprocessing(content_image, (200, 300), ctx = ctx)style_img = preprocessing(style_image, (200, 300), ctx = ctx)output = Parameter('output', shape=content_img.shape)output.initialize(ctx=ctx)# output.set_data(nd.random_normal(shape = content_img.shape).abs())output.set_data(content_img)content_img_result, _ = extract_features(net, content_img, content_layers, style_layers)_, style_img_result = extract_features(net, style_img, content_layers, style_layers)content_results, style_results = extract_features(net, output.data(), content_layers, style_layers)style_target = [gram(i) for i in style_img_result]trainer = Trainer([output], 'adam', {'learning_rate': 0.01, 'beta1': 0.9, 'beta2': 0.99})for epoch in range(3000): with autograd.record(): content_results, style_results = extract_features(net, output.data(), content_layers, style_layers) loss = get_loss(content_loss(content_results, content_img_result), style_loss(style_results, style_target, [0.2] * 5), 1e-4) loss.backward() if epoch % 100 == 0: print(loss.asscalar()) trainer.step(1)plt.imshow(postprocessing(output.data())) 这里在实现的时候，使用了这个2D图像的total variation denoising，也就是，每个像素应尽可能的与左侧和上方的像素相近。所以最后的优化目标是三部分组成，第一部分是content loss，第二部分是style loss，第三部分是total variation loss。研究一下mxnet给出的examplemodel_vgg19.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# Licensed to the Apache Software Foundation (ASF) under one# or more contributor license agreements. See the NOTICE file# distributed with this work for additional information# regarding copyright ownership. The ASF licenses this file# to you under the Apache License, Version 2.0 (the# \"License\"); you may not use this file except in compliance# with the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing,# software distributed under the License is distributed on an# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY# KIND, either express or implied. See the License for the# specific language governing permissions and limitations# under the License.import find_mxnetimport mxnet as mximport os, sysfrom collections import namedtupleConvExecutor = namedtuple('ConvExecutor', ['executor', 'data', 'data_grad', 'style', 'content', 'arg_dict'])def get_symbol(): # declare symbol data = mx.sym.Variable(\"data\") conv1_1 = mx.symbol.Convolution(name='conv1_1', data=data , num_filter=64, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False, workspace=1024) relu1_1 = mx.symbol.Activation(name='relu1_1', data=conv1_1 , act_type='relu') conv1_2 = mx.symbol.Convolution(name='conv1_2', data=relu1_1 , num_filter=64, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False, workspace=1024) relu1_2 = mx.symbol.Activation(name='relu1_2', data=conv1_2 , act_type='relu') pool1 = mx.symbol.Pooling(name='pool1', data=relu1_2 , pad=(0,0), kernel=(2,2), stride=(2,2), pool_type='avg') conv2_1 = mx.symbol.Convolution(name='conv2_1', data=pool1 , num_filter=128, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False, workspace=1024) relu2_1 = mx.symbol.Activation(name='relu2_1', data=conv2_1 , act_type='relu') conv2_2 = mx.symbol.Convolution(name='conv2_2', data=relu2_1 , num_filter=128, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False, workspace=1024) relu2_2 = mx.symbol.Activation(name='relu2_2', data=conv2_2 , act_type='relu') pool2 = mx.symbol.Pooling(name='pool2', data=relu2_2 , pad=(0,0), kernel=(2,2), stride=(2,2), pool_type='avg') conv3_1 = mx.symbol.Convolution(name='conv3_1', data=pool2 , num_filter=256, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False, workspace=1024) relu3_1 = mx.symbol.Activation(name='relu3_1', data=conv3_1 , act_type='relu') conv3_2 = mx.symbol.Convolution(name='conv3_2', data=relu3_1 , num_filter=256, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False, workspace=1024) relu3_2 = mx.symbol.Activation(name='relu3_2', data=conv3_2 , act_type='relu') conv3_3 = mx.symbol.Convolution(name='conv3_3', data=relu3_2 , num_filter=256, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False, workspace=1024) relu3_3 = mx.symbol.Activation(name='relu3_3', data=conv3_3 , act_type='relu') conv3_4 = mx.symbol.Convolution(name='conv3_4', data=relu3_3 , num_filter=256, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False, workspace=1024) relu3_4 = mx.symbol.Activation(name='relu3_4', data=conv3_4 , act_type='relu') pool3 = mx.symbol.Pooling(name='pool3', data=relu3_4 , pad=(0,0), kernel=(2,2), stride=(2,2), pool_type='avg') conv4_1 = mx.symbol.Convolution(name='conv4_1', data=pool3 , num_filter=512, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False, workspace=1024) relu4_1 = mx.symbol.Activation(name='relu4_1', data=conv4_1 , act_type='relu') conv4_2 = mx.symbol.Convolution(name='conv4_2', data=relu4_1 , num_filter=512, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False, workspace=1024) relu4_2 = mx.symbol.Activation(name='relu4_2', data=conv4_2 , act_type='relu') conv4_3 = mx.symbol.Convolution(name='conv4_3', data=relu4_2 , num_filter=512, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False, workspace=1024) relu4_3 = mx.symbol.Activation(name='relu4_3', data=conv4_3 , act_type='relu') conv4_4 = mx.symbol.Convolution(name='conv4_4', data=relu4_3 , num_filter=512, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False, workspace=1024) relu4_4 = mx.symbol.Activation(name='relu4_4', data=conv4_4 , act_type='relu') pool4 = mx.symbol.Pooling(name='pool4', data=relu4_4 , pad=(0,0), kernel=(2,2), stride=(2,2), pool_type='avg') conv5_1 = mx.symbol.Convolution(name='conv5_1', data=pool4 , num_filter=512, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False, workspace=1024) relu5_1 = mx.symbol.Activation(name='relu5_1', data=conv5_1 , act_type='relu') # style and content layers style = mx.sym.Group([relu1_1, relu2_1, relu3_1, relu4_1, relu5_1]) content = mx.sym.Group([relu4_2]) return style, contentdef get_executor(style, content, input_size, ctx): out = mx.sym.Group([style, content]) # make executor arg_shapes, output_shapes, aux_shapes = out.infer_shape(data=(1, 3, input_size[0], input_size[1])) arg_names = out.list_arguments() arg_dict = dict(zip(arg_names, [mx.nd.zeros(shape, ctx=ctx) for shape in arg_shapes])) grad_dict = {\"data\": arg_dict[\"data\"].copyto(ctx)} # init with pretrained weight pretrained = mx.nd.load(\"./model/vgg19.params\") for name in arg_names: if name == \"data\": continue key = \"arg:\" + name if key in pretrained: pretrained[key].copyto(arg_dict[name]) else: print(\"Skip argument %s\" % name) executor = out.bind(ctx=ctx, args=arg_dict, args_grad=grad_dict, grad_req=\"write\") return ConvExecutor(executor=executor, data=arg_dict[\"data\"], data_grad=grad_dict[\"data\"], style=executor.outputs[:-1], content=executor.outputs[-1], arg_dict=arg_dict)def get_model(input_size, ctx): style, content = get_symbol() return get_executor(style, content, input_size, ctx) nstyle.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355# Licensed to the Apache Software Foundation (ASF) under one# or more contributor license agreements. See the NOTICE file# distributed with this work for additional information# regarding copyright ownership. The ASF licenses this file# to you under the Apache License, Version 2.0 (the# \"License\"); you may not use this file except in compliance# with the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing,# software distributed under the License is distributed on an# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY# KIND, either express or implied. See the License for the# specific language governing permissions and limitations# under the License.import find_mxnetimport mxnet as mximport numpy as npimport importlibimport logginglogging.basicConfig(level=logging.DEBUG)import argparsefrom collections import namedtuplefrom skimage import io, transformfrom skimage.restoration import denoise_tv_chambolleCallbackData = namedtuple('CallbackData', field_names=['eps','epoch','img','filename'])def get_args(arglist=None): parser = argparse.ArgumentParser(description='neural style') # 选择模型，默认是VGG19 parser.add_argument('--model', type=str, default='vgg19', choices = ['vgg'], help = 'the pretrained model to use') # 内容图片的路径 parser.add_argument('--content-image', type=str, default='input/IMG_4343.jpg', help='the content image') # 风格图片的路径 parser.add_argument('--style-image', type=str, default='input/starry_night.jpg', help='the style image') # 停止迭代的阈值，若relative change小于这个数就停止迭代 parser.add_argument('--stop-eps', type=float, default=.005, help='stop if the relative chanage is less than eps') # 内容图片在loss上的权重 parser.add_argument('--content-weight', type=float, default=10, help='the weight for the content image') # 风格图片在loss上的权重 parser.add_argument('--style-weight', type=float, default=1, help='the weight for the style image') # total variation在loss上的权重 parser.add_argument('--tv-weight', type=float, default=1e-2, help='the magtitute on TV loss') # 最大迭代次数 parser.add_argument('--max-num-epochs', type=int, default=1000, help='the maximal number of training epochs') # parser.add_argument('--max-long-edge', type=int, default=600, help='resize the content image') # 初始的学习率 parser.add_argument('--lr', type=float, default=.001, help='the initial learning rate') # 使用哪块GPU parser.add_argument('--gpu', type=int, default=0, help='which gpu card to use, -1 means using cpu') # 输出图像的路径 parser.add_argument('--output_dir', type=str, default='output/', help='the output image') # 每多少轮保存一次当前的输出结果 parser.add_argument('--save-epochs', type=int, default=50, help='save the output every n epochs') # parser.add_argument('--remove-noise', type=float, default=.02, help='the magtitute to remove noise') # 每迭代多少轮减小一下学习率 parser.add_argument('--lr-sched-delay', type=int, default=75, help='how many epochs between decreasing learning rate') # 学习率衰减因子 parser.add_argument('--lr-sched-factor', type=int, default=0.9, help='factor to decrease learning rate on schedule') if arglist is None: return parser.parse_args() else: return parser.parse_args(arglist)def PreprocessContentImage(path, long_edge): ''' 内容图片预处理 Parameter: path, str, 图片路径 long_edge, int, float, str(float), 图像被缩放后长边的长度 ''' # 读取图片，使用skimage.io.imread，返回numpy.ndarray img = io.imread(path) # img.shape前两个数分别是多少行和多少列，第三个数是channel数 logging.info(\"load the content image, size = %s\", img.shape[:2]) # resize一下图片，resize后的范围在0到1内 factor = float(long_edge) / max(img.shape[:2]) new_size = (int(img.shape[0] * factor), int(img.shape[1] * factor)) resized_img = transform.resize(img, new_size) # 乘以256恢复到原来的区间 sample = np.asarray(resized_img) * 256 # swap axes to make image from (224, 224, 3) to (3, 224, 224) sample = np.swapaxes(sample, 0, 2) sample = np.swapaxes(sample, 1, 2) # sub mean，这里的均值应该是ImageNet数据集在RGB三通道上的均值 sample[0, :] -= 123.68 sample[1, :] -= 116.779 sample[2, :] -= 103.939 logging.info(\"resize the content image to %s\", new_size) return np.resize(sample, (1, 3, sample.shape[1], sample.shape[2]))def PreprocessStyleImage(path, shape): ''' 对风格图片的预处理 Parameter: path, str, 图像路径 shape, tuple, 长度为4的tuple，第三个元素和第四个元素是content image的size ''' img = io.imread(path) resized_img = transform.resize(img, (shape[2], shape[3])) sample = np.asarray(resized_img) * 256 sample = np.swapaxes(sample, 0, 2) sample = np.swapaxes(sample, 1, 2) sample[0, :] -= 123.68 sample[1, :] -= 116.779 sample[2, :] -= 103.939 return np.resize(sample, (1, 3, sample.shape[1], sample.shape[2]))def PostprocessImage(img): ''' 对图像的后处理 Parameter: img, numpy.ndarray ''' img = np.resize(img, (3, img.shape[2], img.shape[3])) img[0, :] += 123.68 img[1, :] += 116.779 img[2, :] += 103.939 img = np.swapaxes(img, 1, 2) img = np.swapaxes(img, 0, 2) # clip函数是用来砍掉小于下界和大于上届的数的 img = np.clip(img, 0, 255) return img.astype('uint8')def SaveImage(img, filename, remove_noise=0.): ''' 保存图片 Parameter: img, numpy.ndarray filename, str remove_noise, float, default=0., ''' logging.info('save output to %s', filename) out = PostprocessImage(img) if remove_noise != 0.0: out = denoise_tv_chambolle(out, weight=remove_noise, multichannel=True) io.imsave(filename, out)def style_gram_symbol(input_size, style): ''' Parameter: input_size, tuple, length=2, 表示content image的size style, mx.sym.Group，里面是style对应的层 ''' _, output_shapes, _ = style.infer_shape(data=(1, 3, input_size[0], input_size[1])) gram_list = [] grad_scale = [] for i in range(len(style.list_outputs())): shape = output_shapes[i] x = mx.sym.Reshape(style[i], target_shape=(int(shape[1]), int(np.prod(shape[2:])))) # use fully connected to quickly do dot(x, x^T) gram = mx.sym.FullyConnected(x, x, no_bias=True, num_hidden=shape[1]) gram_list.append(gram) # grad_scale c*h*w*c grad_scale.append(np.prod(shape[1:]) * shape[1]) return mx.sym.Group(gram_list), grad_scaledef get_loss(gram, content): gram_loss = [] for i in range(len(gram.list_outputs())): gvar = mx.sym.Variable(\"target_gram_%d\" % i) gram_loss.append(mx.sym.sum(mx.sym.square(gvar - gram[i]))) cvar = mx.sym.Variable(\"target_content\") content_loss = mx.sym.sum(mx.sym.square(cvar - content)) return mx.sym.Group(gram_loss), content_lossdef get_tv_grad_executor(img, ctx, tv_weight): \"\"\"create TV gradient executor with input binded on img \"\"\" if tv_weight &lt;= 0.0: return None nchannel = img.shape[1] simg = mx.sym.Variable(\"img\") skernel = mx.sym.Variable(\"kernel\") channels = mx.sym.SliceChannel(simg, num_outputs=nchannel) out = mx.sym.Concat(*[ mx.sym.Convolution(data=channels[i], weight=skernel, num_filter=1, kernel=(3, 3), pad=(1,1), no_bias=True, stride=(1,1)) for i in range(nchannel)]) kernel = mx.nd.array(np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]]) .reshape((1, 1, 3, 3)), ctx) / 8.0 out = out * tv_weight return out.bind(ctx, args={\"img\": img, \"kernel\": kernel})def train_nstyle(args, callback=None): \"\"\"Train a neural style network. Args are from argparse and control input, output, hyper-parameters. callback allows for display of training progress. \"\"\" # input dev = mx.gpu(args.gpu) if args.gpu &gt;= 0 else mx.cpu() content_np = PreprocessContentImage(args.content_image, args.max_long_edge) style_np = PreprocessStyleImage(args.style_image, shape=content_np.shape) # size是内容图片的尺寸 size = content_np.shape[2:] # model Executor = namedtuple('Executor', ['executor', 'data', 'data_grad']) # 导入'model_vgg19.py' model_module = importlib.import_module('model_' + args.model) # 获取到style和content两个mx.sym.Group，里面装着style和content层 style, content = model_module.get_symbol() # 获取到所有style层的gram矩阵和grad scale gram, gscale = style_gram_symbol(size, style) model_executor = model_module.get_executor(gram, content, size, dev) model_executor.data[:] = style_np model_executor.executor.forward() style_array = [] for i in range(len(model_executor.style)): style_array.append(model_executor.style[i].copyto(mx.cpu())) model_executor.data[:] = content_np model_executor.executor.forward() content_array = model_executor.content.copyto(mx.cpu()) # delete the executor del model_executor style_loss, content_loss = get_loss(gram, content) model_executor = model_module.get_executor( style_loss, content_loss, size, dev) grad_array = [] for i in range(len(style_array)): style_array[i].copyto(model_executor.arg_dict[\"target_gram_%d\" % i]) grad_array.append(mx.nd.ones((1,), dev) * (float(args.style_weight) / gscale[i])) grad_array.append(mx.nd.ones((1,), dev) * (float(args.content_weight))) print([x.asscalar() for x in grad_array]) content_array.copyto(model_executor.arg_dict[\"target_content\"]) # train # initialize img with random noise img = mx.nd.zeros(content_np.shape, ctx=dev) img[:] = mx.rnd.uniform(-0.1, 0.1, img.shape) lr = mx.lr_scheduler.FactorScheduler(step=args.lr_sched_delay, factor=args.lr_sched_factor) optimizer = mx.optimizer.NAG( learning_rate = args.lr, wd = 0.0001, momentum=0.95, lr_scheduler = lr) optim_state = optimizer.create_state(0, img) logging.info('start training arguments %s', args) old_img = img.copyto(dev) clip_norm = 1 * np.prod(img.shape) tv_grad_executor = get_tv_grad_executor(img, dev, args.tv_weight) for e in range(args.max_num_epochs): img.copyto(model_executor.data) model_executor.executor.forward() model_executor.executor.backward(grad_array) gnorm = mx.nd.norm(model_executor.data_grad).asscalar() if gnorm &gt; clip_norm: model_executor.data_grad[:] *= clip_norm / gnorm if tv_grad_executor is not None: tv_grad_executor.forward() optimizer.update(0, img, model_executor.data_grad + tv_grad_executor.outputs[0], optim_state) else: optimizer.update(0, img, model_executor.data_grad, optim_state) new_img = img eps = (mx.nd.norm(old_img - new_img) / mx.nd.norm(new_img)).asscalar() old_img = new_img.copyto(dev) logging.info('epoch %d, relative change %f', e, eps) if eps &lt; args.stop_eps: logging.info('eps &lt; args.stop_eps, training finished') break if callback: cbdata = { 'eps': eps, 'epoch': e+1, } if (e+1) % args.save_epochs == 0: outfn = args.output_dir + 'e_'+str(e+1)+'.jpg' npimg = new_img.asnumpy() SaveImage(npimg, outfn, args.remove_noise) if callback: cbdata['filename'] = outfn cbdata['img'] = npimg if callback: callback(cbdata) final_fn = args.output_dir + '/final.jpg' SaveImage(new_img.asnumpy(), final_fn)if __name__ == \"__main__\": args = get_args() train_nstyle(args)","link":"/blog/2018/02/24/image-style-transfer-using-convolutional-neural-networks/"},{"title":"Inductive Representation Learning on Large Graphs","text":"NIPS 2017。提出的方法叫GraphSAGE，针对的问题是之前的NRL是transductive，作者提出的GraphSAGE是inductive。主要考虑了如何聚合顶点的邻居信息，对顶点或图进行分类。原文链接：Inductive Representation Learning on Large Graphs 摘要现存的方法需要图中所有的顶点在训练embedding的时候都出现；这些前人的方法本质是transductive，不能自然地泛化到未见过的顶点。我们提出了GraphSAGE，一个inductive框架利用顶点特征信息（比如文本属性）来高效地为没有见过的顶点生成embedding。与其为每个顶点训练单独的embedding，我们学习到一个函数，这个函数通过从一个顶点的局部邻居采样并聚合顶点特征。我们的算法在三个inductive顶点分类benchmark上超越了那些很强的baseline：我们在基于citation和Reddit post数据的演化的信息图中对未见过的顶点分类，实验表明我们使用一个PPI多图数据集，算法可以泛化到完全未见过的图上。 1 引言顶点嵌入的基本思想是使用降维技术从高维信息中提炼一个顶点的邻居信息，存到低维向量中。这些顶点嵌入之后会作为后续的机器学习系统的输入，解决像顶点分类、聚类、链接预测这样的问题。 然而，前人的工作专注于从一个固定的图中对顶点进行表示，很多真实的应用需要很快的对未见过的顶点或是全新的图（子图）生成embedding。这个推断的能力对于高吞吐的机器学习系统来说很重要，这些系统都运作在不断演化的图上，而且时刻都会遇到未见过的顶点（比如Reddit上的文章，Youtube上的用户或视频）。一个生成顶点embedding的推断方法也会帮助在拥有同样形式特征的图上进行泛化：举个例子，我们可以从一个有机物得到的PPI图上训练一个embedding生成器，然后很简单的使用这个模型对新的有机物上收集的数据生成他们的顶点嵌入。 推断顶点嵌入问题很困难，对比transductive问题，因为泛化未见过的顶点需要将新观测到的子图对其到算法已经优化好的顶点嵌入上。一个推断模型必须学习识别一个顶点邻居的结构性质，这个性质既反映了顶点在图中的局部角色，也反映了它的全局位置。 很多现存的生成顶点嵌入的方法是继承于transductive。这些方法的主流直接使用矩阵分解目标函数对每个顶点的embedding进行优化，不能自然的泛化到未见过的数据，因为他们在一个固定的单个图上的顶点做预测。这些方法可以被修改然后再inductive问题上运行，但是这些修改往往计算复杂度高，再新的预测之前需要额外的梯度下降。当然也有一些使用在图结构上使用卷积神经网络的方法，在embedding上表现的很好。迄今为止，GCN只在固定的图上的transductive问题上应用过。我们工作是扩展了GCN到无监督推断任务上，同时还提出了一个可以将GCN方法泛化出使用可以训练的聚合函数（并不是只有简单的卷积）。 Present work.我们提出了一个general框架，叫GraphSAGE(SAmple and aggreGatE)，用于inductive node embedding。不像基于矩阵分解的嵌入方法，我们利用了顶点特征（比如文本属性，顶点信息，顶点的度）来学习可以生成未见过的顶点的函数。通过在学习算法中结合顶点信息，我们同时学习了每个顶点邻居的拓扑结构和顶点特征在邻居中的分布。尽管我们专注于富特征的图（如有文本信息的引文网络，有功能/分子组成的生物数据），我们的方法扔能充分利用所有图展现的结构特征（比如顶点的度）。因此我们的算法可以应用在没有顶点特征的图上。 我们没有对每个顶点都训练一个单独的embeddding，我们训练了一组aggregator functions，这些函数学习如何从一个顶点的局部邻居聚合特征信息（图1）。每个聚合函数从一个顶点的不同搜索深度聚合信息。测试的时候，或是说推断的时候，我们使用我们训练的系统来对完全未见过的顶点，通过使用学习到的聚合函数来生成embedding。跟随着前人在生成顶点上的工作，我们设计了无监督的损失函数，使得GraphSAGE可以在没有任务监督的情况下训练。我们也展示了使用完全监督的方法如何训练GraphSAGE。 我们在三个顶点分类benchmark上评估了我们的算法，测试了GraphSAGE在未见过的数据上生成有效的embedding的能力。使用了两个基于citation数据和Reddit post数据的演化网络（分别预测paper和post类别），还有一个基于PPI的多图泛化实验（预测蛋白质功能）。我们的方法效果很好，跨领域，监督的方法在F1值上对比只使用顶点特征的方法平均提高了51%，而且GraphSAGE一直都比transductive baseline强很多，尽管baseline在未见过的顶点上的运行时间要长100倍以上。我们也发现我们提出的新的聚合结构比受图卷积启发的聚合函数更好（平均提升了$7.4\\%$）。最后我们通过实验证明了我们方法的表现能力，GraphSAGE能学习一个顶点在一个图中的结构信息，尽管事实上是它是基于特征的（第5部分）。 2 相关工作Factorization-based embedding approaches. 最近的node embedding方法使用随机游走的统计和矩阵分解的目标函数。这些方法和传统的方法如谱聚类，multi-dimensional scaling，PageRank关系很近。因为这些嵌入方法对每个顶点直接训练embedding，本质上是transductive，而且需要大量的额外训练（如随机梯度下降）使他们能预测新的顶点。此外，对于这些方法中的大部分，目标函数对于embedding的正交变换是不变的，意味着嵌入空间不能自然地在图之间泛化，而且在再次训练的时候会drift。一个值得注意的例外是Yang et al.的Planetoid-I算法，是一个inductive，基于嵌入的半监督学习。然而，Planetoid-I在推断的时候不使用任何图结构信息，而在训练的时候将图结构作为一种正则化的形式。不像前面的这些方法，我们利用特征信息来训练可以对未见过的顶点生成embedding的模型。 Supervised learning over graphs. 除了顶点嵌入方法，还有很多在图结构数据上的监督学习方法。包括很多核方法，图的特征向量从多个图的核得到。最近又很多用于图结构的监督学习的神经网络方法。我们的方法从概念上是受到了这些方法的启发。然而，尽管这些方法试图对整个图（或子图）进行分类，我们的工作关注的是如何对单个顶点生成有效的表示。 Graph convolutional networks. 近些年，一些用于图的卷积神经网络被相继提出。这些方法的大部分都不能扩展到大的图上，或是为了整个图的分类而设计（或是两点都有）。然而，我们的方法与Kipf et al.提出的图卷积很相关，在训练的时候需要整个图的拉普拉斯矩阵。我们算法的一个简单的变体可以看作是GCN框架在inductive setting上的扩展，我们会在3.3说明。 3 GraphSAGE我们的关键思想在于我们的方法是学习如何从一个顶点的局部邻居聚合特征信息（比如度或近邻顶点的文本特征）。3.1描述embedding的生成算法。3.2描述随机梯度下降学习参数。 3.1 Embedding generation (i.e., forward propagation) algorithm假设已经学习到了$K$个聚合函数（表示为$AGGERGATE_k, \\forall k \\in \\lbrace 1,…,K\\rbrace$）的参数，对顶点的信息聚合，还有一组权重矩阵$\\mathbf{W}^k, \\forall k \\in \\lbrace 1,…,K\\rbrace$，用来在模型的不同层或搜索深度间传播信息。下一节描述参数是怎么训练的。 算法1背后的直觉是在每次迭代，或搜索深度，顶点从他们的局部邻居聚合信息，并且随着这个过程的迭代，顶点会从越来越远的地方获得信息。 算法1描述了在各整个图上生成embedding的过程，$\\mathcal{G} = \\left( \\mathcal{V}, \\Large{\\varepsilon} \\right)$，以及所有顶点的特征$X_v, \\forall v \\in \\mathcal{V}$作为输入。在算法1最外层循环的每一步如下，$k$表示外循环（或搜索深度）的当前一步，$\\mathbf{h}^k$表示当前这步的一个顶点的表示：首先，每个顶点$v \\in \\mathcal{V}$聚合了在它在中间邻居的表示，$\\lbrace \\mathbf{h}^{k-1}_u, \\forall u \\in \\mathcal{N}(u) \\rbrace$，聚合到向量$\\mathbf{h}^{k-1}_{\\mathcal{N}(v)}$中。注意，这个聚合步骤依赖于外循环前一次迭代生成的表示（比如$k - 1$），$k = 0$表示输入的顶点特征。聚合邻居特征向量后，GraphSAGE之后拼接了顶点当前的表示，$\\mathbf{h}^{k-1}_v$，核聚合的邻居向量一起，$\\mathbf{h}^{k-1}_{\\mathcal{N}(v)}$，拼接后的向量输入到了激活函数为$\\sigma$的全连接层中，将表示变换为下一步使用的形式（$\\mathbf{h}^k_v, \\forall v \\in \\mathcal{V}$）。为了记号的简单，我们将深度为$K$的输出表示记为$\\mathbf{z} \\equiv \\mathbf{h}^K_v, \\forall v \\in \\mathcal{V}$。邻居表示的聚合可以通过多个聚合架构得到（在算法1中表示为$\\mathrm{AGGERGATE}$），我们会在3.3讨论不同的架构。 为了将算法1扩展到minibatch设定上，给定一组输入顶点，我们先采样采出需要的邻居集合（到深度$K$），然后运行内部循环（算法1的第三行），但不是迭代所有的顶点，我们在每个深度只计算必须满足的表示（后记A包括了完整的minibatch伪代码）。 3.2 Learning the parameters of GraphSAGE为了在半监督设定下学习一个有效的表示，我们使用基于图的损失函数来输出表示$\\mathbf{z}_u, \\forall u \\in \\mathcal{V}$，调整权重矩阵$\\mathbf{W}^k, \\forall k \\in \\lbrace 1,…,K\\rbrace$，聚合函数的参数通过随机梯度下降训练。基于图的损失函数倾向于使得相邻的顶点有相似的表示，尽管这会使相互远离的顶点的表示很不一样：$$\\tag{1}J\\mathcal{G}(\\mathbf{z}_u) = -\\log(\\sigma(\\mathbf{z}^T_u \\mathbf{z}_v)) - Q \\cdot \\mathbb{E}_{v_n \\sim P_n(v)} \\log(\\sigma(-\\mathbf{z}^T_u \\mathbf{z}_{v_n})),$$其中$v$是通过定长随机游走得到的$u$旁边的共现顶点，$\\sigma$是sigmoid函数，$P_n$是负采样分布，$Q$定义了负样本的数目。重要的是，不像之前的那些方法，我输入到损失函数的表示$\\mathbf{z}_u$是从包含一个顶点局部邻居的特征生成出来的，而不是对每个顶点训练一个独一无二的embedding（通过一个embedding查询表）。 这个无监督设定模拟了顶点特征提供给后续机器学习应用的情况。在那些表示只在后续的任务中使用的情况下，无监督损失（式1）可以被替换或改良，通过一个以任务为导向的目标函数（比如cross-entropy）。 3.3 聚合架构不像在$N$维网格（如句子、图像、$3\\rm{D}$）上的机器学习，一个顶点的邻居是无序的；因此，算法1中的聚合函数必须在以一个无序的向量上运行。理想上来说，一个聚合函数需要是对称的（也就是对它输入的全排列来说是不变的），而且还要可训练，且保持表示的能力。聚合函数的对称性之确保了我们的神经网络模型可以被训练且可以应用于任意顺序的顶点邻居特征集合上。我们检验了三种聚合函数：Mean aggregator. 第一个聚合函数是均值聚合，我们简单的取$\\lbrace \\mathbf{h}^{k-1}_u, \\forall v \\in \\mathcal{N}(v) \\rbrace$中的向量的element-wise均值。均值聚合近似等价在transducttive GCN框架[17]中的卷积传播规则。特别地，我们可以通过替换算法1中的4行和5行为以下内容得到GCN的inductive变形：$$\\tag{2}\\mathbf{h}^k_v \\leftarrow \\sigma(\\mathbf{W} \\cdot \\mathrm{MEAN}(\\lbrace \\mathbf{h}^{k-1}_v \\rbrace \\cup \\lbrace \\mathbf{h}^{k-1}_u, \\forall u \\in \\mathcal{N}(v) \\rbrace)).$$我们称这个修改后的基于均值的聚合器是convolutional，因为它是一个粗略的，局部化谱卷积的的线性近似[17]。这个卷积聚合器和我们的其他聚合器的重要不同在于它没有算法1中第5行的拼接操作——卷积聚合器没有将顶点前一层的表示$\\mathbf{h}^{k-1}_v$和聚合的邻居向量$\\mathbf{h}^k_{\\mathcal{N}(v)}$拼接起来。拼接操作可以看作一个是在不同的搜索深度或层之间的简单的skip connection的形式，它使得模型获得了巨大的提升。 LSTM aggregator. 我们也检验了一个基于LSTM的复杂的聚合器。对比均值聚合器，LSTM有更强的表达能力。然而，LSTM不是对称的这个需要注意，因为他们处理他们的输入是以一个序列的方式。我们简单地将LSTM应用在一个顶点邻居的随机序列上。 Pooling aggregator. 我们检验的最后一个聚合器既是对称的，又是可训练的。在这个池化方法种，每个邻居的向量都是相互独立输入到全连接神经网络中的；随着这种变化，一个element-wise最大池化操作应用在邻居集合上来聚合信息：$$\\tag{3}\\mathrm{AGGREGATE}^{pool}_k = \\mathrm{max}(\\lbrace \\sigma (\\mathbf{W}_{pool} \\mathbf{h}^k_{u_i} + \\mathbf{b}), \\forall u_i \\in \\mathcal{N}(v) \\rbrace),$$其中，$\\mathrm{max}$表示element-wise最大值操作，$\\sigma$是非线性激活函数。原则上，在最大池化使用之前，函数可以是任意的深度多层感知机，但是我们关注的是单个的单层结构。方法是搜到了最近的神经网络架构在学习general point sets上的启发[29]。直觉上来说，多层感知机可以看作是一组函数，这组函数为邻居集合中的每个顶点计算表示。通过对每个计算得到的特征使用最大池化操作，模型有效地捕获了邻居集合的不同方面。注意，原则上，任何对称的向量函数都可以替换$\\mathrm{max}$操作器（比如element-wise mean）。我们发现最大池化和均值池化在测试时没有太大的差别，所以使用了最大池化完成了后续的实验。 4 实验我们在三个benchmark上测试了GraphSAGE：1. 使用Web of Science citation dataset对不同的学术文章进行主题分类。2. 对属于不同社区的Reddit posts进行分类，3. 对多个PPI图进行蛋白质功能分类。在所有的实验中，我们在训练时没有见过的顶点上做预测，对PPI上对完全未见过的图做预测。 Experimental set-up. 为了在inductive benchmark上面将经验结果置于上下文中考虑，我们对比了四个baseline：随即分类器，基于特征的逻辑回归，raw features和DeepWalk embedding拼接的embedding。我们也比较了GraphSAGE的四个变体，分别使用不同的聚合函数（3.3部分）。因为，GraphSAGE的卷积变体是一种扩展形式，是Kipf et al. 半监督GCN的inductive version，我们称这个变体为GraphSAGE-GCN。我们测试了根据式1的损失函数训练的GraphSAGE变体，还有在cross-entropy上训练的监督变体。对于所有的GraphSAGE变体我们使用ReLU作为激活，$K = 2$，邻居采样大小$S_1 = 25$，$S_2 = 10$（详情见4.4节）。 对于Reddit和citation数据集，我们使用”online”来训练DeepWalk，如Perozzi et al. 提到的那样，我们在做预测前，跑一轮新的SGD来嵌入新的测试顶点（详情见后记）。在多图设定中，我们不能使用DeepWalk，因为通过DeepWalk在不同不相交的图上运行后生成的嵌入空间对其他来说可以是arbitrarily rotated（后记D）。 所有的模型都是用tf实现的，用Adam优化（除了DeepWalk，使用梯度下降效果更好）。我们设计的实验目标是1. 验证GraphSAGE比其他方法好。 2. 严格对比集中聚合架构，为了严格对比，所有的方法使用相同的实现，如minibatch迭代器，损失函数和邻居采样（如果可以的话）。此外，为了防止对比聚合器时非有意的”hyperparameter hacking”，我们检查了所有GraphSAGE变体的超参数集合（为每个变体根据他们在验证集上的表现选择最好的设定）。可能的超参数集合在早期的验证集上决定，这个验证集是citation和Reddit的子集，后续就丢掉了。后记包含了实现的细节。","link":"/blog/2018/07/19/inductive-representation-learning-on-large-graphs/"},{"title":"Kafka生产者与消费者","text":"Kafka是一个分布式、流式消息平台，是一套发布订阅系统，通俗来说就是Kafka producer发布数据至Kafka brokers，然后由Kafka consumer从brokers拉取数据，进行消费。最近上数据仓库的课，学习了Kafka的使用方式以及Kafka的原理。 Kafka官网：Apache Kafka Kafka是一个分布式、流式消息平台，是一套发布订阅系统，通俗来说就是Kafka producer发布数据至Kafka brokers，然后由Kafka consumer从brokers拉取数据，进行消费。 日志有意思的特性是Kafka内的数据都是以日志的形式存储，即便消费完也不会消失，配置文件中配置了过了多长时间日志会销毁掉。这样设计的好处有很多，consumer是有group的，每个组进行消费的时候，都会有个偏移量offset记录在zookeeper中，通过这个offset就知道下次从哪里开始消费了，不同组的offset不一样，这样每个组都可以按照自己的需要进行消费。 主题Kafka的记录是有主题的，这样producer发送到broker的数据其实就是打上了标签，有了分类，消费的时候可以按主题消费，相当于一开始就用主题对数据进行了区分。 效率Kafka集群同时也作为缓冲区，平衡producer和consumer两边的工作进度，不会因为一方过慢造成阻塞一类的问题。 语言写起来的话，肯定是java和scala最好，因为Kafka就是由这两种语言编写的，当然，也有其他语言的接口，比如python。python的话比较有意思的是有两个Kafka框架，一个是kafka-python，另一个是pykafka。推荐使用后者，前者在创建consumer group的时候不是很方便，group内的每个consumer消费的内容都一样，没有实现去重与平衡，这些都需要自己实现，后者的balanced_consumer就挺好的。 自己写的例子","link":"/blog/2018/07/05/kafka生产者与消费者/"},{"title":"Large-Scale Learnable Graph Convolutional Networks","text":"KDD 2018.将图结构数据变换到网格状数据中，使用传统的一维卷积进行卷积。变换的方式是：针对每个特征的大小，对邻居结点进行排序，取这个特征前k大的数作为它邻居这列特征的k个值。如果邻居不够，那就用0来补。这样就能得到该顶点的邻居信息，组成一个矩阵，然后使用一维卷积。但是作者没说为什么非要取最大的k个数。原文链接：Large-Scale Learnable Graph Convolutional Networks 摘要卷积神经网络在网格数据上取得了很大的成功，但是在学习像图这样的数据的时候就面临着很多的挑战。CNN中，可学习的局部滤波器可以自动地捕获高层次的特征。滤波器的计算需要感受野内有固定数量的单元。然而，在图结构中，邻居单元的数量不固定，而且邻居也不有序，所以阻碍了卷积的操作。我们提出了可学习图卷积层(learnable graph convolutional layer LGCL)来解决这些挑战。基于值的排序，LGCL为每个特征自动地选择固定数量的邻居结点，以此将图结构数据变换到1维的网格结构中，然后就可以在图上使用常规的卷积操作了。为了能让模型在大尺度的图上训练，我们提出了一个子图训练方法来减少过多的内存和计算资源的开销。在顶点分类任务上，不论是transductive 还是 inductive，表现得都更好一些。我们的结果展示出了我们的子图训练方法比前人的方法更高效。 3. methods3.1 Challenges of Applying Convolutional Operations on Graph Data为了让传统的卷积操作可以应用在图上，需要解决两个图结构数据和网格数据的差异。首先，顶点的邻居数量通常会变化。其次，我们不能对邻居顶点进行排序，因为他们没有可供排序的信息。举个例子，社交网络中，每个人都可以看作是一个顶点，边表示人与人之间的关系。显然，每个顶点的邻居顶点数量是不同的，因为人们可以有不同数量的朋友。而且，如果没有额外的信息，很难对他们进行排序。 网格数据可以看作是一种特殊的图结构数据，每个顶点有固定数量的邻居。因为卷积操作是直接应用在图像这样的网格数据上。为了看清楚固定邻居数量以及排序信息的重要性，我们举个例子，有一个$3 \\times 3$的卷积核，扫描一张图像。我们将这张图片考虑成一个特殊的图，每个像素是一个顶点。在扫描的过程中，计算包括了中心结点和周围8个邻居结点的计算。这8个顶点在这个特殊的图中通过边连接到中心结点。与此同时，我们使用他们和中心结点的相对位置对他们排序，这对于卷积操作很重要，因为在扫描的过程中，滤波器的权重和图中的顶点要一一对应。举个例子，在上面的例子中，$3 \\times 3$的卷积核，左上角的权重应该总是对应中心节点左上方的邻居结点。没有这样的排序信息，卷积的输出结果就不再是确定的。从刚才的讨论中可以看到传统卷积在图结构数据上应用的挑战。为了解决这两个挑战，我们提出了一个方法将图结构数据变换到网格数据内。 3.2 learnable Graph Convolutional Layers为了让传统卷积可以在图上可用，我们提出了LGCL。LGCL的layer-wise传播规则写为： $$\\tag{3}\\tilde{X}_l = g(X_l, A, k),\\\\X_{l+1} = c(\\tilde{X}_l)$$ 其中，$A$是邻接矩阵，$g(\\cdot)$使用了$k$-largest Node Selection，将图结构数据映射到网格结构，$c(\\cdot)$表示1维常规的CNN，将顶点信息聚合，为每个顶点输出了一个新的特征向量。我们会在下面分开讨论$g(\\cdot)$和$c(\\cdot)$。 $k$-largest Node Selection. 我们提出了一个新的方法称为$k$-largest Node Selection，将图结构映射到网格数据上，其中$k$是LGCL的超参数。在这个操作之后，每个顶点的邻居信息聚合，表示成一个有$(k+1)$个位置的1维的网格状。变换后的数据会输入到CNN中来生成新的特征向量。 假设有行向量$x^1_l, x^2_l, …, x^N_l$的$X_l \\in \\mathbb{R}^{N \\times C}$，表示$N$个顶点的图，每个顶点有$C$个特征。邻接矩阵$A \\in \\mathbb{N}^{N \\times N}$，$k$为定值。顶点$i$的特征向量是$x^i_l$，它有$n$个邻居。通过在$A$中的一个简单查找，我们可以获得这些邻居结点的下标，$i_1, i_2, …, i_n$。对它们对应的特征向量$x^{i_1}_l, x^{i_2}_l, …, x^{i_n}_l$进行拼接，得到$M^i_l \\in \\mathbb{R}^{n \\times C}$。假设$n \\geq k$，就没有泛化上的损失。如果$n &lt; k$，我们可以使用全为0的列，给$M^i_l$加padding。$k$-largest node selection是在$M^i_l$上做的：也就是，对于每列，我们排出$n$个值，然后选最大的$k$个数。我们就可以得到一个$k \\times C$的输出矩阵。因为$M^i_l$表示特征，这个操作等价于为每个特征选择$k$个最大值。通过在第一行插入$x^i_l$，输出变为$\\tilde{M}^i_l \\in \\mathbb{R}^{(k+1) \\times C}$。如图2左部分。通过对每个顶点重复这个操作，$g(\\cdot)$将$X_l$变为$\\tilde{X}_l \\in \\mathbb{R}^{N \\times (k + 1) \\times C}$。 注意，如果将$N$，$(k+1)$，$C$分别看作是batch size，spatial size，通道数，那么$\\tilde{X}_l$可以看作是1维网格状的结构。因此，$k$个最大顶点选择函数$g(\\cdot)$成功地将图结构变换为网格结构。这个操作充分利用了实数的自然顺序信息，使得每个顶点有固定数量的有序邻居。 1-D Convolutional Neural Networks. 就像3.1节讨论的，传统的卷积操作可以直接应用到网格状的数据上。$\\tilde{X}_l \\in \\mathbb{R}^{N \\times (k + 1) \\times C}$是1维的数据，我们部署一个一维CNN模型$c(\\cdot)$。LGCL基本的功能是聚合邻居信息，为每个顶点更新特征。后续的话，它需要$X_{l + 1} \\in \\mathbb{R}^{N \\times D}$，其中$D$是更新后的特征空间的维度。一维CNN $c(\\cdot)$ 使用$\\tilde{X}_l \\in \\mathbb{R}^{N \\times (k + 1) \\times C}$作为输入，输出一个$N \\times D$的矩阵，或是$N \\times 1 \\times D$的矩阵。$c(\\cdot)$可以将空间维度从$(k+1)$减小到$1$。 注意，$N$看作是batch size，与$c(\\cdot)$的设计无关。结果就是，我们只聚焦于一个样本，也就是图中的一个顶点。对于顶点$i$，变换得到的输出是$\\tilde{M}^i_l \\in \\mathbb{R}^{(k + 1) \\times C}$，是$c(\\cdot)$的输入。由于任何一个卷积核大于1且没有padding的卷积都会减少空间的大小，最简单的$c(\\cdot)$只有一个卷积核大小为$(k+1)$的卷积，没有padding。输入和输出的通道数分别为$C$和$D$。同时，可以部署任意一个多层CNN，得到最后的输出的维度是$1 \\times D$。图2右侧展示了一个两层CNN的例子。再对所有的$N$个顶点使用一次$c(\\cdot)$，输出$X_{l+1} \\in \\mathbb{R}^{N \\times D}$。总结一下，我们的LGCL使用$k$最大顶点选择以及传统的一维CNN，将图结构变换到网格数据，实现了对每个顶点进行的特征聚合和特征过滤。 3.3 可学习的图卷积网络越深的网络一般会产生越好的结果。然而，之前在图上的深度模型，如GCN，只有两层。尽管随着深度的增加，它们的性能有有所下降[Kipf &amp; Welling 2017]，我们的LGCL可以构造的很深，构造出图顶点分类的可学习的图卷积网络。我们基于densely connected convolutional networks(DCNNs)，构造了LGCNs，前者获得了ImageNet分类任务最好的成绩。 在LGCN中，我们先用一个图嵌入层来生成顶点的低维表示，因为原始输入一般都是高维特征，比如Cora数据集。第一层的图嵌入层本质上就是一个线性变换，表示为： $$\\tag{4}X_1 = X_0 W_0$$ 其中，$X_0 \\in \\mathbb{R}^{N \\times C_0}$表示高维的输入，$W_0 \\in \\mathbb{R}^{C_0 \\times C_1}$将特征空间从$C_0$映射到了$C_1$。结果就是，$X_1 \\in \\mathbb{R}^{N \\times C_1}$和$C_1 &lt; C_0$。或者，使用一个GCN层来做图嵌入。如第二部分描述的，GCN层中的参数数量等价于传统的图嵌入层中参数的数量。 在图嵌入层后，我们堆叠多个LGCL，多少个取决于数据的复杂程度。因为每个LGCL只能聚合一阶邻居的信息，也就是直接相连的邻居顶点，堆叠LGCL可以从一个更大的顶点集中获得信息，这也是传统CNN的功能。为了提升模型的性能，帮助训练过程，我们使用skip connections来拼接LGCL的输入和输出。最后，在softmax激活前使用一个全连接层。 就像LGCN的设计理念，$k$以及堆叠的LGCL的数量是最重要的超参数。顶点的平均度是选择$k$的一个重要参数。LGCL的数量应该依赖任务的复杂度，比如类别的个数，图的顶点数等。越复杂的模型需要越深的模型。 3.4 Sub-Graph Training on Large-Scale Data大部分图上的深度学习模型都有另一个限制。在训练的时候，输入的是所有顶点的特征向量以及整个图的邻接矩阵。图的尺寸大的时候这个矩阵就会变大。这些方法在小尺度的图上表现的还可以。但是对于大尺度的图，这些方法一般都会导致内存和计算资源极大的开销，限制了这些模型的一些应用。 其他类型的数据集也有相似的问题，比如网格数据。举个例子，图像分割上的深度模型通常使用随机切片的方式来处理大的图片。受到这种策略的启发，我们随机的将图“切分”，使用得到的小图进行训练。然而，尽管一张图片的一个矩形部分很自然地包含了像素的邻居信息。如何处理图中顶点的不规则连接还是一个问题。 我们提出了子图选择算法来解决大尺度图上计算资源的问题，如算法1所示。给定一个图，我们先采样出一些初始顶点。从它们开始，我们使用广度优先搜索算法，迭代地将邻接顶点扩充到子图内。经过一定次数的迭代后，初始顶点的高阶邻居顶点就会被加进去。注意，我们在算法1中使用一个简单的参数$N_m$。实际上在每个迭代中，我们将$N_m$设置为了不同的值。图4给出了子图选择过程的一个例子。 这样随机的切分子图，我们可以在大尺度的图上训练深层模型。此外，我们可以充分利用mini-batch训练方法来加速学习过程。在每轮训练中，我们可以使用子图训练方法采样多个子图，然后把它们放到batch中。对应的特征向量和邻接矩阵组成了网络的输入。 4. Experimental studies代码：https://github.com/divelab/lgcn/ 4.2 Experimental SetupTransduction Learning. 在transductive learning 任务中，我们像图3一样部署LGCN模型。因为transductive learning数据集使用高维的词袋表示作为顶点的特征向量，输入通过一个图嵌入层来降维。我们这里使用GCN层作为图嵌入层。","link":"/blog/2018/09/17/large-scale-learnable-graph-convolutional-networks/"},{"title":"Identity Mappings in Deep Residual Networks","text":"ECCV 2016, ResNet v2, 原文链接：Identity Mappings in Deep Residual Networks Identity Mappings in Deep Residual NetworksIntroductionDeep residual network (ResNets) consist of many stacked “Residual Units”. Each unit (Fig. 1(a)) can be expressed in a general form:$$y_l = h(x_l) + \\mathcal{F}(x_l, \\mathcal{W_l})$$$$x_{l+1}=f(y_l)$$where $x_l$ and $x_{l+1}$ are input and output of the $l$-th unit, and $\\mathcal{F}$ is a residual function.$h(x_l)=x_l$ is an identity mapping and $f$ is a ReLU function.The central idea of ResNets is to learn the additive residual function $\\mathcal{F}$ with respect to $h(x_l)$, with a key choice of using an identity mapping $h(x_l)=x_l$. This is realized by attaching an identity skip connection (“shortcut”).In this paper, we analyze deep residual networks by focusing on creating a “direct” path for propagating information – not only within a residual unit, but through the entire network. Our derivations reveal that if both h(x_l) and f(y_l) are identity mappings, the signal could be directly propagated from one unit to any other units, in both forward and backward passes.To understand the role of skip connections, we analyse and compare various types of $h(x_l)$. We find that the identity mapping $h(x_l) = x_l$ chosen in achieves the fastest error reduction and lowest training loss among all variants we investigated, whereas skip connections of scaling, gating, and $1 \\times 1$ convolutions all lead to higher training loss and error. These experiments suggest that keeping a “clean” information path (indicated by the grey arrows in Fig. 1,2, and 4) is helpful for easing optimization.Figure 1. Left: (a) original Residual Unit in [1]; (b) proposed Residual Unit. The grey arrows indicate the easiest paths for the information to propagate, corresponding to the additive term “x_l” in Eqn.(4) (forward propagation) and the additive term “1” in Eqn.(5) (backward propagation). Right: training curves on CIFAR-10 of 1001-layer ResNets. Solid lines denote test error (y-axis on the right), and dashed lines denote training loss (y-axis on the left). The proposed unit makes ResNet-1001 easier to train. To construct an identity mapping $f(y_l)=y_l$, we view the activation functions (ReLU and BN) as “pre-activation” of the weight layers, in constrast to conventional wisdom of “post-activation”. This point of view leads to a new residual unit design, shown in (Fig. 1(b)). Based on this unit, we present competitive results on CIFAR-10/100 with a 1001-layer ResNet, which is much easier to train and generalizes better than the original ResNet in [1]. We further report improved results on ImageNet using a 200-layer ResNet, for which the counterpart of [1] starts to overfit. These results suggest that there is much room to exploit the dimension of network depth, a key to the success of modern deep learning. Analysis of Deep Residual NetworksThe ResNets developed in [1] are modularized architectures that stack building blocks of the same connecting shape. In this paper we call these blocks “Residual Units”. The original Residual Unit in [1] performs the following computation:$$y_l = h(x_l) + \\mathcal{F}(x_l, \\mathcal{W-l})$$$$x_{l+1}=f(y_l)$$Here $x_l$ is the input feature to the $l$-th Residual Unit. $\\mathcal{W_l}=\\lbrace W_{l,k} \\mid 1 \\le k \\le K\\rbrace$ is a set of weights (and biases) associated with the $l$-th Residual Unit, and $K$ is the number of layers in a Residual Unit ($K$ is 2 or 3 in [1]). $\\mathcal{F}$ denotes the residual function, e.g., a stack of two $3 \\times 3$ convolutional layers in [1]. The function $f$ is the operation after element-wise addition, and in [1] $f$ is ReLU. The function $h$ is set as an identity mapping: $h(x_l)=x_l$.If $f$ is also an identity mapping: $x_{l+1} \\equiv y_l$, we can put Eqn.(2) into Eqn.(1) and obtain:$$x_{l+1}=x_l+\\mathcal{F}(x_l, \\mathcal{W_l})$$Recursively $(x_{l+2}=x_{l+1} + \\mathcal{F}(x_{l+1}, \\mathcal{W_{l+1}}) = x_l + \\mathcal{F}(x_l, \\mathcal{W_l}) + \\mathcal{F}(x_{l+1},\\mathcal{W_{l+1}}), etc.)$ we will have:$$x_L = x_l + \\sum_{i=1}^{L-1}\\mathcal{F}(x_i, \\mathcal{W_i})$$for any deeper unit $L$ and any shallower unit $l$. Eqn.(4) exhibits some $nice properties. The feature $x_L$ of any deeper unit $L$ can be represented as the feature $x_l$ of any shallower unit $l$ plus a residual function in a form of $\\sum_{i=1}^{L-1}\\mathcal{F}$, indicating that the model is in a residual fashion between any units $L$ and $l$. The feature $x_L = x_0 + \\sum_{i=0}^{L-1}\\mathcal{F}(x_i, \\mathcal{W_i})$, of any deep unit $L$, is the summation of the outputs of all preceding residual functions (plus $x_0$). This is in contrast to a “plain network” where a feature $x_L$ is a series of matrix-vector products, say, $\\prod_{i=0}^{L-1}W_ix_0$ (ignoring BN and ReLU).Eqn.(4) also leads to nice backward propagation properties. Denoting the loss function as $\\varepsilon$, from the chain rule of backpropagation [9] we have:$$\\frac{\\partial{\\varepsilon}}{\\partial{x_l}}=\\frac{\\partial{\\varepsilon}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}=\\frac{\\partial{\\varepsilon}}{\\partial{x_L}}(1+\\frac{\\partial}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i, \\mathcal{W_i}))$$Eqn.(5) indicates that the gradient $\\frac{\\partial{\\varepsilon}}{\\partial{x_i}}$ can be decomposed into two additive terms: a term of $\\frac{\\partial{\\varepsilon}}{\\partial{x_L}}$ that propagates information directly without concerning any weight layers, and another term of $\\frac{\\partial{\\varepsilon}}{\\partial{x_L}}(\\frac{\\partial}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F})$ that propagates through the weight layers. The additive term of $\\frac{\\partial{\\varepsilon}}{\\partial{x_L}}$ ensures that information is directly propagated back to any shallower unit $l$. Eqn.(5) also suggests that it is unlikely for the gradient $\\frac{\\partial{\\varepsilon}}{\\partial{x_l}}$ to be canceled out for a mini-batch, because in general the term $\\frac{\\partial}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}$ cannot be always -1 for all samples in a mini-batch. This implies that the gradient of a layer does not vanish even when the weights are arbitrarily small. On the Importance of Identity Skip ConnectionsLet’s consider a simple modification, $h(x_l)=\\lambda_lx_l$, to break the identity shortcut:$$x_{l+1}=\\lambda_lx_l+\\mathcal{F}(x_l, \\mathcal{W_l})$$where $\\lambda_l$ is a modulating scalar (for simplicity we still assume $f$ is identity).Recursively applying this forumulation we obtain an equation similar to Eqn. (4): $x_L=(\\prod_{i=l}^{L-1}\\lambda_i)x_l+\\sum_{i=1}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i, \\mathcal{W_i})$, or simply:$$x_L = (\\prod_{i=l}^{L-1}\\lambda_i)x_l+\\sum_{i=l}^{L-1}\\hat{\\mathcal{F}}(x_i, \\mathcal{W_i})$$where the notation $\\hat{\\mathcal{F}}$ absorbs the scalars into the residual functions. Similar to Eqn.(5), we have backpropagation of the following form:$$\\frac{\\partial{\\varepsilon}}{\\partial{x_l}}=\\frac{\\partial{\\varepsilon}}{\\partial{x_L}}((\\prod_{i=l}^{L-1}\\lambda_i)+\\frac{\\partial}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\hat{\\mathcal{F}}(x_i, \\mathcal{W_i}))$$For an extremely deep network ($L$ is large), if $\\lambda_i &gt; 1$ for all $i$, this factor can be exponentially large; if $\\lambda_i &lt; 1$ for all $i$, this factor can be expoentially small and vanish, which blocks the backpropagated signal from the shortcur and forces it to flow through the weighted layers. This results in optimization difficuties as we show by experiments.If the skip connection $h(x_l)$ represents more complicated transforms (such as gating and $1 \\times 1$ convolutions), in Eqn.(8) the first term becomes $\\prod_{i=l}^{L-1}h_i’$ where $h’$ is the derivative of $h$. This product may also impede information propagation and hamper the training procedure as witnessed in the following experiments. Experiments on skip ConnectionsWe experiments with the 110-layer ResNet as presented in [1] on CIFAR-10. Though our above analysis is driven by identity $f$, the experiments in this section are all based on $f = ReLU$ as in [1]; we address identity $f$ in the next section. Our baseline ResNet-110 has 6.61% error on the test set. The comparisons of other variants (Fig.2 and Table 1) are summarized as follows:Table 1. Classification error on the CIFAR-10 test set using ResNet-110 [1], with different types of shortcut connections applied to all Residual Units. We report “fail” when the test error is higher than 20%. Constant scaling. We set $\\lambda = 0.5$ for all shortcuts (Fig. 2(b)). We further study two cases of scaling $\\mathcal{F}$: $\\mathcal{F}$ is not scaled; $\\mathcal{F}$ is scaled by a constant scalar of $1-\\lambda = 0.5$, which is similar to the highway gating [6,7] but with frozen gates. The former case does not converge well; the latter is able to converge, but the test error (Table 1, 12.35%) is substantially higher than the original ResNet-110. Fig 3(a) shows that the training error is higher than that of the original ResNet-110, suggesting that the optimization has difficulties when the shortcut signal is scaled down. Exclusive gating. Following the Highway Networks [6,7] that adopt a gating mechanism [5], we consider a gating function $g(x)=\\sigma(W_gx+b_g)$ where a transform is represented by weights $W_g$ and biases $b_g$ followed by the sigmoid function $\\sigma(x)=\\frac{1}{1+e^{-x}}$. In a convolutional network $g(x)$ is realized by a $1 \\times 1$ convolutional layer. The gating function modulates the signal by element-wise multiplication.We investigate the “exclusive” gates as used in [6,7] – the $\\mathcal{F}$ path is scaled by $g(x)$ and the shortcut path is scaled by $1-g(x)$. See Fig 2(c). We find that the initialization of the biases $b_g$ is critical for training gated models, and following the guidelines in [6,7], we conduct hyper-parameter search on the initial value of $b_g$ in the range of 0 to -10 with a decrement step of -1 on the training set by cross-validation. The best value (-6 here) is then used for training on the training set, leading to a test result of 8.70% (Table 1), which still lags far behind the ResNet-110 baseline. Fig 3(b) shows the training curves. Table 1 also reports the results of using other initialized values, noting that the exclusive gating network does not converge to a good solution when $b_g$ is not appropriately initialized. Shortcut-only gating. In this case the function $\\mathcal{F}$ is not scaled; only the shortcut path is gated by $1-g(x)$. See Fig 2(d). The initialized value of $b_g$ is still essential in this case. When the initialized $b_g$ is 0 (so initially the expectation of $1-g(x)$ is 0.5), the network converges to a poor result of 12.86% (Table 1). This is also caused by higher training error (Fig 3(c)).When the initialized $b_g$ is very negatively biased (e.g., -6), the value of $1-g(x)$ is closer to 1 and the shortcut connection is nearly an identity mapping. Therefore, the result (6.91%, Table 1) is much closer to the ResNet-110 baseline. $1 \\times 1$ convolutional shortcut. Next we experiment with $1 \\times 1$ convolutional shortcut connections that replace the identity. This option has been investigated in [1] (known as option C) on a 34-layer ResNet (16 Residual Units) and shows good results, suggesting that $1 \\times 1$ shortcut connections could be useful. But we find that this is not the case when there are many Residual Units. The 110-layer ResNet has a poorer result (12.22%, Table 1) when using $1 \\times 1$ convolutional shortcuts. Again, the training error becomes higher (Fig 3(d)). When stacking so many Residual Units (54 for ResNet-110), even the shortest path may still impede signal propagation. We witnessed similar phenomena on ImageNet with ResNet-101 when using $1 \\times 1$ convolutional shortcuts. Dropout shortcut. Last we experiment with dropout [11] (at a ratio of 0.5) which we adopt on the output of the identity shortcut (Fig. 2(f)). The network fails to converge to a good solution. Dropout statistically imposes a scale of $\\lambda $ with an expectation of 0.5 on the shortcut, and similar to constant scaling by 0.5, it impedes signal propagation. On the Usage of Activation FunctionsWe want to make $f$ an identity mapping, which is done by re-arranging the activation function (ReLU and/or BN). The original Residual Unit in [1] has a shape in Fig.4(a) – BN is used after each weight layer, and ReLU is adopted after BN expect that the last ReLU in a Residual Unit is after element-wise addition ($f=ReLU$). Fig.4(b-e) show the laternatives we investigated, explained as following. Experiments on ActivationIn this section we experiment with ResNet-110 and a 164-layer Bottlenect [1] architecture (denoted as ResNet-164). A bottleneck Residual Unit consist of a $1 \\times 1$ layer for reducing dimension, a $3 \\times 3$ layer, and a $1 \\times 1$ layer for restoring dimension. As designed in [1], its computational complexity is similar to the two-$3 \\times 3$ Residual Unit. More details are in the appendix. The baseline ResNet-164 has a competitive result of 5.93% on CIFAR-10 (Table 2). BN after addition. Before turning $f$ into an identity mapping, we go the opposite way by adopting BN after addition (Fig. 4(b)). In this case $f$ involves BN and ReLU. The results become considerably worse than the baseline (Table 2). Unlike the original design, now the BN layer alters the signal that passes through the shortcut and impedes information propagation, as reflected by the difficulties on reducing training loss at the begining of training (Fib. 6 left). ReLU before addition. A naive choice of making $f$ into an identity mapping is to move the ReLU Implementation使用mxnet实现了一版123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114from mxnet import ndimport mxnet as mximport numpy as npimport picklefrom mxnet import imageimport matplotlib.pyplot as pltdef unpickle(file): with open(file, 'rb') as fo: dicts = pickle.load(fo, encoding='bytes') return dictsdef residual_unit(x, channels, name, same_shape = True): stride = 1 if same_shape else 2 net = mx.sym.BatchNorm(data = x, fix_gamma = False, name = '%s_bn1'%(name), momentum=0.9) net = mx.sym.Activation(data = net, act_type = 'relu', name = '%s_relu1'%(name)) net = mx.sym.Convolution(data = net, num_filter = channels, kernel = (3, 3),\\ pad = (1, 1), stride = (stride, stride), name = '%s_conv1'%(name)) net = mx.sym.BatchNorm(data = net, fix_gamma = False, name = '%s_bn2'%(name), momentum=0.9) net = mx.sym.Activation(data = net, act_type = 'relu', name = '%s_relu2'%(name)) net = mx.sym.Convolution(data = net, num_filter = channels, kernel = (3, 3),\\ pad = (1, 1), name = '%s_conv2'%(name)) if not same_shape: x = mx.sym.Convolution(data = x, num_filter = channels, pad = (0, 0),\\ stride = (stride, stride), kernel = (1, 1), name = \"%s_conv3\"%(name)) return net + xdef ResNet(units, nums): data = mx.sym.Variable('data') net = mx.sym.Convolution(data, num_filter = 16, kernel = (3, 3), pad = (1, 1)) for num in nums: net = residual_unit(net, num, 'r%s%s'%(num, 1), False) for i in range(2, units+1): net = residual_unit(net, num, 'r%s%s'%(num, i)) net = mx.sym.BatchNorm(net, name = 'batch1', momentum=0.9) net = mx.sym.Activation(net, act_type = 'relu', name = 'relu1') net = mx.sym.Pooling(net, pool_type = 'avg', kernel = (3, 3), name = 'pool1') net = mx.sym.Flatten(net, name = 'flat1') net = mx.sym.FullyConnected(net, name = 'fc1', num_hidden = 10) net = mx.sym.SoftmaxOutput(net, name = 'softmax') return netall_data = []for i in range(1, 6): data = unpickle('../data/cifar-10-batches-py/data_batch_%s'%(i)) all_data.append((nd.array(data[b'data']), nd.array(data[b'labels'])))X, y = zip(*all_data)trainX, trainY = nd.concat(*X, dim = 0).reshape(shape = (-1, 3, 32, 32)).astype('float32'),\\ nd.concat(*y, dim = 0)data = unpickle('../data/cifar-10-batches-py/test_batch')testX = nd.array(data[b'data']).reshape(shape = (-1, 3, 32, 32)).astype('float32')testY = nd.array(data[b'labels'])# batch_size = 128batch_size = 128train_iter = mx.io.NDArrayIter(trainX, trainY, batch_size, shuffle = True)test_iter = mx.io.NDArrayIter(testX, testY, batch_size, shuffle = False)net = ResNet(12, [64, 128, 256])mod = mx.mod.Module(symbol=net, context=[mx.gpu(i) for i in range(0, 2)],# context = mx.gpu(0), data_names=['data'], label_names=['softmax_label'])mod.bind(data_shapes = train_iter.provide_data, label_shapes = train_iter.provide_label)mod.init_params(initializer=mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=2))mod.init_optimizer(optimizer='nag', optimizer_params=(('learning_rate', 0.1), ('wd', 0.0001), ('momentum', 0.9)))# mod.init_optimizer(optimizer='adam', optimizer_params=(('learning_rate', 5e-4),# ('beta1', 0.9),# ('beta2', 0.99)))losses = []accuracy = []metrics = [mx.metric.create('acc'), mx.metric.CrossEntropy()]for epoch in range(10): train_iter.reset() [i.reset() for i in metrics] for batch in train_iter: mod.forward(batch, is_train = True) mod.update_metric(metrics[0], batch.label) mod.update_metric(metrics[1], batch.label) mod.backward() mod.update() if epoch % 1 == 0: score = mod.score(test_iter, ['acc', mx.metric.CrossEntropy()]) losses.append((metrics[1].get()[-1], score[1][-1])) accuracy.append((metrics[0].get()[-1], score[0][-1])) print('Epoch %d, Training acc %s, loss %s'%(epoch, accuracy[-1][0], losses[-1][0])) print('Epoch %d, Validation acc %s, loss %s'%(epoch, accuracy[-1][1], losses[-1][1])) print()plt.figure(figsize=(10, 8))train_loss, test_loss = zip(*losses)plt.plot(train_loss, '-', color = 'blue', label = 'training loss')plt.plot(test_loss, '-', color = 'red', label = 'testing loss')plt.legend(loc = 'upper right')plt.xlabel('iteration')plt.ylabel('loss')plt.show()plt.figure(figsize=(10, 8))train_acc, test_acc = zip(*accuracy)plt.plot(train_acc, '-', color = 'blue', label = 'training acc')plt.plot(test_acc, '-', color = 'red', label = 'testing acc')plt.legend(loc = 'upper right')plt.xlabel('iteration')plt.ylabel('acc')plt.show()","link":"/blog/2018/03/08/identity-mappings-in-deep-residual-networks/"},{"title":"Image Super-Resolution Using Deep Convolutional Networks","text":"PAMI 2016，大体思路：把训练集中的所有样本模糊化，扔到三层的卷积神经网络中，把输出和原始图片做一个loss，训练模型即可。原文链接：Image Super-Resolution Using Deep Convolutional Networks 首先是ill-posed problem，图像的不适定问题法国数学家阿达马早在19世纪就提出了不适定问题的概念:称一个数学物理定解问题的解存在、唯一并且稳定的则称该问题是适定的（Well Posed）.如果不满足适定性概念中的上述判据中的一条或几条，称该问题是不适定的。 Convolutional Neural Networks For Super-ResolutionFormulationWe first upscale a single low-resolution image to the desired size using bicubic interpolation. Let us denote the interpolated image as $Y$. Our goal is to recover from $Y$ an image $F(Y)$ that is as similar as possible to the ground truth high-resolution image $X$. For the ease of presentation, we still call $Y$ a “low-resolution” image, although it has the same size as $X$. We wish to learning a mapping $F, which conceptually consists of three operations: Patch extraction and representation. this operation extracts (overlapping) patches from the low-resolution image $Y$ and represents each patch as a high-dimensional vector. These vectors comprise a set of feature maps, of which the number equals to the dimensionality of the vectors. Non-linear mapping. this operation nonlinearly maps each high-dimensional vector onto another high-dimensional vector. Each mapped vector is conceptually the representation of a high-resolution patch. These vectors comprise another set of feature maps. Reconstruction. this operation aggregates the above high-resolution patch-wise representations to generate the final high-resolution image. This image is expected to be similar to the ground truth $X$. Patch Extraction and RepresentationA popular strategy in image restoration is to densely extract patches and then represent them by a set of pre-trained bases such as PCA, DCT, Haar, etc. This is equivalent to convolving the image by a set of filters, each of which is a basis. In our formulation, we involve the optimization of these bases into the optimization of the network. Formally, our first layer is expressed as an operation $F_1$:$$F_1(Y)=max(0, W_1 Y + B_1)$$where $W_1$ and $B_1$ represent the filters and biases respectively, and $\\$ denotes the convolution operation. Here, $W_1$ corresponds to $n_1$ filters of support $c \\times f_1 \\times f_1$, where $c$ is the number of channels in the input image, $f_1$ is the spatial size of a filter. Intuitively, $W_1$ applies $n_1$ convolutions on the image, and each convolution has a kernel size $c \\times f_1 \\times f_1$. The output is composed of $n_1$ feature maps. $B_1$ is an $n_1$-dimensional vector, whose each element is associated with a filter. We apply the ReLU on the filter responses. Non-Linear MappingThe first layer extracts an $n_1$-dimensional feature for each patch. In the second operation, we map each of these $n_1$-dimensional vectors into an $n_2$-dimensional one. This is equivalent to applying $n_2$ filters which have a trivial spatial support $1 \\times 1$. This interpretation is only valid for $1 \\times 1$ filters. But it is easy to generalize to larger filters like $3 \\times 3$ or $5 \\times 5$. In that case, the non-linear mapping is not on a patch of the input image; instead, it is on a $3 \\times 3$ or $5 \\times 5$ “patch” of the feature map. The operation of the second layer is:$$F_2(Y) = max(0, W_2 * F_1(Y) + B_2)$$Here $W_2$ contains $n_2$ filters of size $n_1 \\times f_2 \\times f_2$, and $B_2$ is $n_2$-dimensional. Each of the output $n_2$-dimensional vectors is conceptually a representation of a high-resolution patch that will be used for reconstruction. ReconstructionIn the traditional methods, the predicted overlapping high-resolution patches are often averaged to produce the final full image. The averaging can be considered as a pre-defined filter on a set of feature maps (where each position is the “flattened” vector form of a high-resolution patch). Motivated by this, we define a convolutional layer to produce the final high-resolution image:$$F(Y)=W_3 * F_2(Y) + B_3$$Here W_3 corresponds to $c$ filters of a size $n_2 \\times f_3 \\times f_3$, and $B_3$ is a $c$-dimensional vector. TrainingLoss function: given a set of high-resolution images ${X_i}$ and their corresponding low-resolution images ${Y_i}$, we use mean squared error (MSE) as the loss function:$$L(\\Theta ) = \\frac{1}{n}\\sum^n_{i=1}\\Vert F(Y_i;\\Theta)-X_i\\Vert ^2$$where $n$ is the number of training samples. Using MSE as the loss function favors a high PSNR. The PSNR is widely-used metric for quantitatively evaluating image restoration quality, and is at least partially related to the perceptual quality. Despite that the proposed model is trained favoring a high PSNR, we still observe satisfactory performance when the model is evaluated using alternative evaluation metrics, e.g., SSIM, MSSIM.PSNR: Peak Signal to Noise Ratio. 是一种评价图像的客观标准。$$PSNR = 10 \\times \\log_{10}(\\frac{(2^n-1)^2}{MSE})$$其中，MSE是原图像和处理图像之间的均方误差，n是每个采样值的比特数，单位是dB。The loss is minimized using stochastic gradient descent with the standard backpropagation. In particular, the weight matrices are updated as$$\\Delta_{i+1}=0.9 \\cdot \\Delta_i + \\eta \\cdot \\frac{\\partial{L}}{\\partial{W^\\ell_i}}, W^\\ell_{i+1}=W^\\ell_{i}+\\Delta_{i+1}$$where $\\ell \\in {1,2,3}$ and $i$ are the indices of layers and iterations, $\\eta$ is the learning rate, and $\\frac{\\partial{L}}{\\partial{W^\\ell_i}}$ is the derivative. The filter weights of each layer are initialized by drawing randomly from a Gaussian distribution with zero mean and standard deviation 0.0001 (and 0 for biases). The learning rate is $10^{-4}$ for the first two layers, and $10^{-5}$ for the last layer. We empirically find that a smaller learning rate in the last layer is important for the network to converge (similar to the denoising case).In the training phase, the ground truth images ${X_i}$ are prepared as $f_{sub} \\times f_{sub} \\times c$-pixel sub-images randomly cropped from the training images. By “sub-images” we mean these samples are treated as small “images” rather than “patches”, in the sense that “patches” are overlapping and require some averaging as post-processing but “sub-images” need not. To synthesize the low-resolution samples ${Y_i}$, we blur a sub-image by a Gaussian kernel, sub-sample it by the upscaling factor, and upscale it by the same factor via bicubic interpolation.To avoid border effects during training, all the convolutional layers have no padding, and the network produces a smaller output $((f_{sub}-f_1-f_2-f_3+3)^2 \\times c)$. The MSE loss function is evaluated only by the difference between the contral pixels of $X_i$ and the network output. Although we use a fixed image size in training, the convolutional nerual network can be applied on images of arbitrary sizes during testing.","link":"/blog/2018/03/02/image-super-resolution-using-deep-convolutional-networks/"},{"title":"Layer Normalization","text":"Layer Normalization，之前看到一篇论文用了这个LN层，看一下这个怎么实现。原文链接：Layer Normalization Abstract训练神经网络很费时，一个减少训练时间的方法是对神经元的激活值归一化。最近的一项技术称为小批量归一化，也就是 batch norm，使用一个神经元的输入的分布，在一个批量的样本上计算均值和方差，然后在神经元的每个训练样例上做归一化。这个能极大地缩短训练时间。但是，batch norm 的效果和 batch size 有关，而且还不知道怎么应用在 RNN 上。我们使用一个训练样例，将 BN 转置，计算一个层上面所有神经元的输入的均值和方差来归一化。就像 BN 一样，我们在归一化后激活之前给每个神经元它自己的可适应的bias和gain。不像 BN 的地方是，LN 在训练和测试的时候都有，通过在每个时间步上做归一化的统计，LN 也能应用在 RNN 上。LN 在稳定 RNN 隐藏状态的动态性上面很有效。经验表明，LN 与之前的技术对比能有效地减少训练时间。 1 Introduction很多深度神经网络要训练好多天。BN 除了提升了收敛速度，从批量统计量得到的随机性在训练的时候还会作为一个正则项。 尽管 BN 简单，但是它需要输入统计量之和的平均值。在定长的 FNN 中，把每个层的 BN 存起来就行。但是，RNN 的循环单元的输入通常随序列长度而变化，所以将 BN 应用在 RNN 上面，不同时间步需要不同的统计量。此外，BN 不能应用在在线学习等任务上，或是非常大的分布式模型上，因为 minibatch 会很小。 2 Background前向神经网络是从输入模式 $\\rm{x}$ 映射到输出向量 $y$ 的非线性变换。在深度前向神经网络中的第 $l$ 个隐藏层，$a^l$ 表示这层神经元的输入。汇总后的输入通过一个线性映射计算如下： $$\\tag{1}a^l_i = {w^l_i}^\\text{T} h^l\\\\h^{l+1}_i = f(a^l_i + b^l_i)$$ 其中 $f(\\cdot)$ 是激活函数，$w^l_i$ 和 $b^l_i$ 分别是第 $l$ 个隐藏层的权重和偏置参数。参数通过基于梯度的学习方法得到。 深度学习的一个挑战是：某一层权重的梯度和上一层的输出高度相关，尤其是当这些输出以一种高度相关的方式变化的时候。BN 提出来是减少这种不希望的 covariate shift 现象。这种方法在输入样例在每个隐藏单元的输入上做计算。详细来说，对于第 $l$ 层的第 $i$ 个输入，BN 根据他们在数据中的分布，将输入缩放了： $$\\tag{2}\\bar{a}^l_i = \\frac{g^l_i}{\\sigma^l_i}(a^l_i - \\mu^l_i)\\\\\\mu^l_i = \\mathbb{E}_{\\mathrm{x} \\sim P(\\mathrm{x})}[a^l_i]\\\\\\sigma^l_i = \\sqrt{\\mathbb{E}_{\\mathrm{x} \\sim P(\\mathrm{x})}[(a^l_i - \\mu^l_i)^2]}$$ 其中 $\\bar{a}^l_i$ 是第 $l$ 层第 $i$ 个输入的归一化结果，$g_i$ 是在非线性激活函数之前的一个增益参数，对归一化激活值进行缩放。注意，期望是在所有训练数据上的。事实上计算式2中的期望是不实际的，因为这需要用当前的参数，前向传播过所有的训练集。实际中是用当前的 mini-batch 来估计 $\\mu$ 和 $\\sigma$。这就给 batch size 增加了限制，而且很难应用到 RNN 上。 3 Layer normalization层归一化用来克服批量归一化的一些缺点。 一个层输出的变换倾向于导致下一层的输入之间有着关联度很高的变化，尤其是使用 ReLU 激活后，这些输出的变化很多。这表明 covariate shift 问题可以通过固定每层的输入的均值和方差解决。因此，在同一层中所有隐藏单元的层归一化统计量如下： $$\\tag{3}\\mu^l = \\frac{1}{H} \\sum^H_{i = 1}a^l_i\\\\\\sigma^l = \\sqrt{\\frac{1}{H} \\sum^H_{i=1} (a^l_i - \\mu^l)^2}$$ 其中 $H$ 表示层内的隐藏单元数。式2和式3的区别是在层归一化之下，层内所有隐藏单元共享相同的归一化项 $\\mu$ 和 $\\sigma$，但是不同的样本有着不同的归一化项。不像 BN，层归一化不会有 batch size 的限制，而且可以使用在 batch size 设为1的时候的在线学习上。 3.1 Layer normalized recurrent neural networks最近的序列到序列模型 [Sutskever et al., 2014] 利用了紧致的 RNN 来解决 NLP 中的序列预测问题。在 NLP 任务中不同的训练样例长度不一致是很常见的。RNN 在每个时间步使用的参数都是相同的。但是在使用 BN 来处理 RNN 时，我们需要计算并存储序列中每个时间步的统计量。如果一个测试的序列比任何训练的序列都长，那就会出问题了。层归一化不会有这样的问题，因为它的归一化项只依赖于当前时间步层的输入。它在所有的时间步上也有一组共享的 gain 和 bias 参数。 在标准的 RNN 中，循环层的输入通过当前的输入 $\\mathrm{x}^t$ 和前一层的隐藏状态 $\\mathrm{h}^{t-1}$，得到 $\\mathrm{a}^t = W_{hh}h^{t-1} + W_{xh} \\mathrm{x}^t$。层归一化后的循环层会将它的激活值使用像式3一样的归一化项缩放到： $$\\tag{4}\\mathrm{h}^t = f[\\frac{\\mathrm{g}}{\\sigma^t} \\odot (\\mathrm{a}^t - \\mu^t) + b]\\\\\\mu^t = \\frac{1}{H} \\sum^H_{i=1}a^t_i\\\\\\sigma^t = \\sqrt{\\frac{1}{H} \\sum^H_{i=1}(a^t_i - \\mu^t)^2}$$ 其中 $W_{hh}$ 是循环隐藏到隐藏的权重，$W_{xh}$ 是输入到隐藏的权重，$\\odot$ 是element-wise multiplication。$\\rm b$ 和 $\\rm g$是和 $\\mathrm{h}^t$ 同维度的 bias 和 gain 参数。 在标准的 RNN 中，每个时间步的循环单元的输入的数量级倾向于增大或减小，导致梯度的爆炸或消失问题。在一个层归一化的 RNN 里，归一化项使它对一个层的输入的缩放不发生变化，使得隐藏到隐藏动态性更稳定。","link":"/blog/2018/12/03/layer-normalization/"},{"title":"leetcode algorithms #2","text":"leetcode algorithms #2. Title: Add Two Numbers Add Two NumbersDescriptionYou are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list. You may assume the two numbers do not contain any leading zero, except the number 0 itself. Example123Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. Discussion用两个指针，分别指向两个list的首元素，加一个进位t，只能是0或者1，用来表示进位。指针指向的两个元素还有进位元素t相加，对10求模得到当前这位存到新的节点中，用这个和除以10，更新进位元素，两指针同时向后移动。最后剩余的一截的头节点与进位元素t相加后，直接接到当前的节点后面。 Solutionspython3 runtime:112ms123456789101112131415161718192021222324252627282930313233343536373839404142# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def addTwoNumbers(self, l1, l2): \"\"\" :type l1: ListNode :type l2: ListNode :rtype: ListNode \"\"\" t = 0 s = l1.val + l2.val + t result = ListNode(s%10) current = result t = s // 10 l1 = l1.next l2 = l2.next while l1 and l2: s = l1.val + l2.val + t current.next = ListNode(s%10) t = s // 10 current = current.next l1 = l1.next l2 = l2.next while l1: s = l1.val + t current.next = ListNode(s%10) t = s // 10 current = current.next l1 = l1.next while l2: s = l2.val + t current.next = ListNode(s%10) t = s // 10 current = current.next l2 = l2.next if t == 1: current.next = ListNode(t) return result","link":"/blog/2018/02/24/leetcode-algorithms-2/"},{"title":"leetcode algorithms #1","text":"leetcode algorithms #1. Title: Two sum Two sumDescriptionGiven an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Example1234Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. Discussion 枚举(brute force)：可以用枚举，时间复杂度为$O(n^2)$, 空间复杂度$O(1)$，$n$为问题规模。假设第一个元素为array中的第一个，从第二个开始遍历，找到和第一个数相加得到target的那个元素的下标，然后假设第一个元素是array中的第二个，如此反复遍历，直到求解成功或无解。 利用hash table求解首先弄个hash table，然后将这些数以及他们的下标以键值对的形式存入hash table中，然后用target分别减去array中的数字，减去后得到的差若在hash table中，那么当前的减数的下标，以及差在hash table中对应的值，即为答案。存入hash table时的时间复杂度为$O(n)$，用target减去array中元素进行搜索的时间复杂度为$O(n)$。故时间复杂度为$O(n)$，空间复杂度为$O(n)$。 看了论坛之后发现还有第三种解法，上述的方法2遍历了两次所有的数字，而方法3只需遍历一次。这个方法利用了这道题的一个特性，也就是这两个相加等于target的元素，一定是一前一后出现的，那么我们在构造hash table的时候，就可以利用这个特性：先插入一个元素，然后看target减去它在不在hash table中，一般情况，肯定是不在的，那么就将这个元素及其下标组成的key-value pair加入hash table中，然后继续遍历下一个元素，假设我们刚才已经将最终结果的第一个元素加入了hash table，那遍历到第二个结果的时候，与之相对的那个元素肯定在hash table中，只要将hash table中target减去它的值，和当前元素的下标返回即可。 SolutionsSolution2python3 runtime: 40ms1234567891011121314151617class Solution: def twoSum(self, nums, target): \"\"\" :type nums: List[int] :type target: int :rtype: List[int] \"\"\" # 建hash table table = {num: index for index, num in enumerate(nums)} for index, num in enumerate(nums): diff = target - num if diff in table: # 需要判断会不会有target = num + num的情况，有的话就跳过 if index == table[diff]: continue return [index, table[diff]] c++ runtime: 10ms12345678910111213141516171819202122class Solution {public: vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) { vector&lt;int&gt; results; map&lt;int, int&gt; table; int diff; for(int i=0; i&lt;nums.size(); i++) table[nums[i]] = i; for(int i=0; i&lt;nums.size(); i++) { diff = target - nums[i]; if(table.find(diff) != table.end()) { if(table[diff] == i) continue; results.push_back(i); results.push_back(table[diff]); return results; } } }}; Solution3c++ runtime: 10ms12345678910111213141516171819class Solution {public: vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) { vector&lt;int&gt; results; map&lt;int, int&gt; table; int diff; for(int i=0; i&lt;nums.size(); i++) { diff = target - nums[i]; if(table.find(diff) != table.end()) { results.push_back(table[diff]); results.push_back(i); return results; } table[nums[i]] = i; } }};","link":"/blog/2018/02/24/leetcode-algorithms-1/"},{"title":"leetcode algorithms #4","text":"leetcode algorithms #4. Title: 两个排序数组的中位数.这题还没搞 There are two sorted arrays nums1 and nums2 of size m and n respectively. Find the median of the two sorted arrays. The overall run time complexity should be $O(\\log{(m+n)})$. Example 1:1234nums1 = [1, 3]nums2 = [2]The median is 2.0 Example 2:1234nums1 = [1, 2]nums2 = [3, 4]The median is (2 + 3)/2 = 2.5","link":"/blog/2018/07/17/leetcode-algorithms-4/"},{"title":"Lattice LSTM 中文NER","text":"ACL 2018，基于LSTM+CRF，用word2vec对字符进行表示，然后用大规模自动分词的预料，将词进行表示，扔进LSTM获得细胞状态，与基于字符的LSTM的细胞状态相结合，得到序列的隐藏状态，然后套一个CRF。原文链接：Chinese NER Using Lattice LSTM 摘要我们调查了lattice-structured LSTM模型在中文分词上的表现，这个模型将输入的字符序列和所有可能匹配到词典中的词进行编码。对比基于字符的方法，我们的模型明显的利用了词与词序列的信息。对于基于词的方法，lattice LSTM不会受到错误分词的影响。门控循环细胞可以使模型从序列中选取最相关的字符和单词获得更好的NER结果。实验在各种数据集上都显示出lattice LSTM比基于词和基于字的LSTM要好，获得了最好的效果。 引言信息抽取中最基础的任务，NER近些年受到了广泛的关注。NER以往被当作一个序列标注问题来解决，实体的边界和类别标签是同时进行预测的。当前最先进的英文命名实体识别的方法是使用集成进单词表示的字符信息的LSTM-CRF模型（Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018）。中文NER与分词联系的很紧密。尤其是命名实体的边界也是词的边界。一个直观的想法是先分词，再标注词。然而这个pipeline会受到错误分词的影响，因为命名实体是分词中OOV中的很重要的一部分，而且不正确的实体边界划分会导致错误的NER。这个问题在open domain中很严重，因为跨领域的分词还是为解决的问题（Liu and Zhang, 2012; Jiang et al., 2013; Liu et al., 2014; Qiu and Zhang, 2015; Chen et al., 2017; Huang et al., 2017）。基于字符的方法比基于词的方法在中文NER中表现的好（He and Wang, 2008; Liu et al., 2010; Li et al., 2014）。然而，基于字符的NER的一个缺点是，词与词的序列信息不能被完全利用到，然而这部分信息可能很有用。为了解决这个问题，我们通过使用一个lattice LSTM表示句子中的lexicon words，在基于字符的LSTM-CRF模型中集成了latent word information。如图1所示，我们通过使用一个大型的自动获取的词典来匹配一个句子，构建了一个词-字lattice。结果是，词序列，像“长江大桥”，“长江”，“大桥”可以用来在上下文中区分潜在的相关的命名实体，比如人名“江大桥”。因为在lattice中有很多潜在的词-字路径，我们利用了一个lattice-LSTM结构来自动地控制句子的开始到结尾的信息流。如图2所示，门控细胞被用于动态规划信息从不同的路径到每个字符上。在NER数据上训练的lattice LSTM可以学习到如何从上下文中找到有用的单词，自动地提高NER的精度。对比基于字符的和基于单词的NER方法，我们的模型的优势在于利用在字符序列标签上的单词信息，且不会受到错误分词的影响。结果显示我们的模型比字符序列标注模型和使用LSTM-CRF的单词序列标注模型都要好很多，在很多中文跨领域的NER数据集上都获得了很好的结果。我们的模型和数据在https://github.com/jiesutd/LatticeLSTM。 相关工作我们的工作与当前处理NER的神经网络一致。Hammerton(2003)尝试解决使用一个单向的LSTM解决这个问题，这个第一个处理NER的神经网络。Collobert et al. (2011)使用了一个CNN-CRF的结构，获得了和最好的统计模型相当的结果。dos Santos et al. (2015)使用了字符CNN来增强CNN-CRF模型。大部分最近的工作利用了LSTM-CRF架构。Huang et al. (2015)使用手工的拼写特征；Ma和Hovy（2016）以及Chiu and Nichols（2016）使用了一个字符CNN来表示拼写的字符；Lample et al.（2016）使用一个字符LSTM，没有使用CNN。我们的baseline基于词的系统使用了与这些相似的架构。字符序列标注是处理中文NER的主要方法（Chen et al., 2006b; Lu et al., 2016; Dong et al., 2016）。已经有讨论基于词的和基于字符的方法的统计的方法对比，表明了后者一般有更好的表现（He and Wang, 2008; Liu et al., 2010; Li et al., 2014）。我们发现有着恰当的表示设定，结论同样适用于神经NER。另一方面，lattice LSTM相比于词LSTM和字符LSTM是更好的一个选择。如何更好的利用词的信息在中文NER任务中受到了持续的关注（Gao et al., 2015），分词信息在NER任务中作为soft features（Zhao and Kit, 2008; Peng and Dredze, 2015; He and Sun, 2017a），使用对偶分解的分词与NER联合学习也被人研究了（Xu et al., 2014），多任务学习（Peng and Dredze, 2016）等等。我们的工作也是，聚焦于神经表示学习。尽管上述的方法可能会被分词训练数据和分词的错误影响，我们的方法不需要一个分词器。这个模型不需要考虑多任务设定，因此从概念上来看就更简单。NER可以利用外部信息。特别地，词典特征已经被广泛地使用了（Collobert et al., 2011; Passos et al., 2014; Huang et al., 2015; Luo et al., 2015）。Rei(2017)使用了一个词级别的语言模型目的是增强NER的训练，在大量原始语料上实现多任务学习。Peters et al.(2017)预训练了一个字符语言模型来增强词的表示。Yang et al.(2017b)通过多任务学习探索了跨领域和跨语言的知识。我们通过在大量自动分词的文本上预训练文本嵌入词典利用了外部信息，尽管半监督技术如语言模型are orthogonal to而且也可以在我们的lattice LSTM模型中使用。Lattice结构的RNN可以被看作是一个树状结构的RNN（Tai et al., 2015）对DAG的自然扩展。他们已经有被用来建模运动力学（Sun et al., 2017），dependency-discourse DAGs(Peng et al., 2017)，还有speech tokenization lattice（Sperber et al., 2017）以及对NMT（neural machine translation）编码器的多粒度分词输出。对比现在的工作，我们的lattice LSTM在动机和结构上都是不同的。比如，对于以字符为中心的lattice-LSTM-CRF序列标注设计的模型，它有循环细胞但是没有针对词的隐藏向量。据我们所知，我们第一个设计了一个新型的lattice LSTM对字母和词进行混合的表示，也是第一个使用一个基于词的lattice处理不分词的中文NER任务的。 模型我们跟从最好的英语NER模型（Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016），使用LSTM-CRF作为主要的网络结构。使用$s=c_1, c_2, …, c_m$表示输入的句子，其中$c_j$表示第$j$个字符。$s$可以被看作一个单词序列$s=w_1, w_2, …, w_n$，其中$w_i$表示序列中的第$i$个单词，由一个中文分词器获得。我们使用$t(i, k)$表示句子中第$i$个单词的第$k$个字符表示下标$j$。取图1的句子作为例子。如果分词结果是“南京市 长江大桥”，下标从1开始，那么$t(2, 1)=4$（长），$t(1, 3)=3$（市）。我们使用BIOES标记（Ratinov and Roth, 2009）对基于词和基于字的NER进行标记。 基于字符的模型基于字符的模型如图3(a)所示。它在$c_1, c_2, …, c_m$上使用了LSTM-CRF模型。每个字符$c_j$表示为$$x^c_j = e^c(c_j)$$其中$e^c$表示一个字符嵌入到了lookup table中。一个双向LSTM（与式11同结构）被使用在$x_1, x_2, …, x_m$来获取从左到右的$\\overrightarrow{h}^c_1, \\overrightarrow{h}^c_2, …, \\overrightarrow{h}^c_m$和从右到左的$\\overleftarrow{h}^c_1, \\overleftarrow{h}^c_2, …, \\overleftarrow{h}^c_m$隐藏状态，这两个隐藏状态有两组不同的参数。每个字符的隐藏向量表示为$$h^c_j = [\\overrightarrow{h}^c_j, \\overleftarrow{h}^c_j]$$一个标准的CRF模型被用在$h^c_1, h^c_2, …, h^c_m$上来进行序列标注。 字符+双字符Character bigrams在分词中用来表示字符已经很有用了（Chen et al., 2015; Yang et al., 2017a）。我们提出了通过拼接双元字符嵌入和字符嵌入的基于字符的模型：$$x^c_j = [e^c(c_j); e^b(c_j, c_{j+1})]$$其中$e^b$表示一个character bigram lookup table。 字符+softword已经有实验表明使用分词作为soft features对于基于字符的NER模型可以提升性能（Zhao and Kit, 2008; Peng and Dredze, 2016）。我们提出的通过拼接分词标记嵌入和字符嵌入的带有分词信息的字符表示：$$x^c_j = [e^c(c_j); e^s(seg(c_j))]$$其中$e^s$表示一个分词标签嵌入查询表。$seg(c_j)$表示一个分词器在字符$c_j$上给出的分词标签。我们使用了BMES策略来表示分词（Xue, 2003）$$h^w_i = [\\overrightarrow{h^w_i}, \\overleftarrow{h^w_i}]$$与基于字符的情况类似，一个标准的CRF模型在序列标记中被用在了$h^w_1, h^w_2, …, h^w_m$上。 基于词的模型基于词的模型如图3（b）所示，它将word embedding $e^w(w_i)$作为每个词$w_i$的表示：$$x^w_i = e^w(w_i)$$其中$e^w$表示一个词嵌入查找表。一个双向LSTM被用来获取词序列$w_1, w_2, …, w_n$上一个从左到右的隐藏状态$\\overrightarrow{h}^w_1, \\overrightarrow{h}^w_2, …, \\overrightarrow{h}^w_n$和一个从右到左的隐藏状态序列$\\overleftarrow{h}^w_1, \\overleftarrow{h}^w_2, …, \\overleftarrow{h}^w_n$。最后，对于每个词$w_i$，$\\overrightarrow{h^w_i}$和$\\overleftarrow{h^w_i}$会被拼在一起成为它的表示：集成字符表示字符CNN（Ma and Hovy, 2016）和LSTM（Lample et al., 2016）两种方法都被用于过表示一个单词中的字符序列。我们在中文NER中对两个方法都进行了实验。我们使用$x^c_i$表示$w_i$中的字符，通过拼接$e^w(w_i)$和$x^c_i$可以获得一个新词的表示：$$x^w_i = [e^w(w_i; x^c_i)]$$ 词+字符LSTM将每个输入字符的嵌入记作$e^c(c_j)$，我们使用一个双向LSTM来学习词$w_i$的字符$c_{t(i, 1)}, …, c_{t(i, len(i))}$的隐藏状态$\\overrightarrow{h}^c_{t(i, 1)}, …, \\overrightarrow{h}^c_{t(i, len(i))}$和$\\overleftarrow{h}^c_{t(i, 1)}, …, \\overleftarrow{h}^c_{t(i, len(i))}$，其中$len(i)$表示词$w_i$的字符个数。最后$w_i$的字符表示为：$$x^c_i = [\\overrightarrow{h}^c_{t(i, len(i))};\\overleftarrow{h}^c_{t(i, 1)}]$$ 词+字符LSTM’我们调查了一种词+字符LSTM的变形，这个模型使用单向的LSTM对每个字符获取$\\overrightarrow{h}^c_j$和$\\overleftarrow{h}^c_j$。与Liu et al. (2018)的结构相似但是没有使用highway layer。使用了相同的LSTM结构和相同的方法集成字符隐藏状态进词嵌入中。 词+字符CNN我们使用标准的CNN（LeCun et al., 1989）应用在词的字符序列上获得字符表示$x^c_i$。将字符$c_j$的嵌入记为$e^c(c_j)$，向量$x^c_i$通过以下式子得到：$$x^c_i = \\max_{t(i,1) \\leq j \\leq t(i, len(i))}(W^T_{CNN} \\begin{bmatrix}e^c(c_{j-\\frac{ke-1}{2}}) \\\\… \\\\e^c(c_{j+\\frac{ke-1}{2}})\\end{bmatrix}+ b_{CNN})$$其中，$W_{CNN}$和$b_{CNN}$和参数，$ke=3$是核的大小，$max$表示最大池化。 Lattice模型图2中展示了词-字lattice模型的整个结构，可以看作是基于字的模型的扩展，集成了基于词的细胞和用来控制信息流的额外的门。图3（c）展示了模型的输入是一个字符序列$c_1, c_2, …, c_m$，与之一起的还有所有字符序列，字符都能在词典$\\mathbb{D}$中匹配到。如部分2中指示的，我们使用自动分词的大型原始语料来构建$\\mathbb{D}$。使用$w^d_{b,e}$来表示一个起始字符下标为$b$，结尾字符下标为$e$，图1中的$w^d_{1,2}$是“南京（Nanjing）”，$w^d_{7,8}$是“大桥（Bridge）”。模型涉及到了四种类型的向量，分别是输入向量、输出隐藏向量、细胞向量、门向量。作为基本的组成部分，一个字符输入向量被用来表示每个字符$c_j$，就像在基于字符的模型中：$x^c_j = e^c(c_j)$基本的循环结构是通过一个在每个字符$c_j$上的字符细胞向量$\\mathbf{c}^c_j$和一个隐藏向量$\\mathbf{h}^c_j$构造的，其中$\\mathbf{c}^c_j$提供句子的开始到$c_j$的信息流，$\\mathbf{h}^c_j$用于CRF序列标注。基础的循环LSTM函数如下：$$\\begin{bmatrix}i^c_j \\\\o^c_j \\\\f^c_j \\\\\\widetilde{c}^c_j\\end{bmatrix} =\\begin{bmatrix}\\sigma \\\\\\sigma \\\\\\sigma \\\\tanh\\end{bmatrix}({W^c}^T\\begin{bmatrix}x^c_j \\\\h^c_{j-1}\\end{bmatrix}+b^c)$$$$c^c_j = f^c_j \\odot c^c_{j-1} + i^c_j \\odot \\hat{c}^c_j$$$$h^c_j = o^c_j \\odot tanh(c^c_j)$$其中，$i^c_j$，$f^c_j$和$o^c_j$表示一组输入、遗忘和输出门。${w^c}^T$和$b^c$是模型参数。$\\sigma()$表示sigmoid function。不同于基于字符的模型，现在计算$c^c_j$的时候需要考虑句子中词典序列$w^d_{b,e}$。特别地，每个序列$w^d_{b,e}$被表示为：$$x^w_{b,e} = e^w(w^d_{b,e})$$其中$e^w$表示3.2节相同的词嵌入查询表。此外，一个词细胞$c^w_{b,e}$用来表示$x^w_{b,e}$从句子开始的循环状态。$c^w_{b,e}$通过以下式子计算得到：$$\\begin{bmatrix}i^w_{b,e} \\\\f^w_{b,e} \\\\\\widetilde{c}^w_{b,e}\\end{bmatrix} = \\begin{bmatrix}\\sigma \\\\\\sigma \\\\tanh\\end{bmatrix}({w^w}^T \\begin{bmatrix}x^w_{b,e} \\\\h^c_b\\end{bmatrix} + b^w)$$$$c^w_{b,e} = f^w_{b,e} \\odot c^c_b + i^w_{b,e} \\odot \\widetilde{c}^w_{b,e}$$其中$i^w_{b,e}$和$f^w_{b,e}$是一组输入和遗忘门。对于词细v胞来说没有输出门因为标记只在字符层面上做。有了$c^w_{b,e}$，会有很多路径可以使信息流向每个$c^c_j$。比如，在图2中，对于$c^c_7$的输入包含$x^c_7$（桥Bridge），$c^w_{6,7}$（大桥Bridge）和$c^w_{4,7}$（长江大桥Yangtze River Bridge）。我们将$c^w_{b,e}$和$b \\in \\lbrace b’ \\mid w^d_{b’,e} \\in \\mathbb{D}\\rbrace$连接到细胞$c^c_e$。我们使用额外的门$i^c_{b,e}$对每个序列细胞$c^w_{b,e}$来控制它对$c^c_{b,e}$的贡献：$$i^c_{b,e} = \\sigma({w^l}^T \\begin{bmatrix}x^c_e \\\\c^w_{b,e}\\end{bmatrix} + b^l)$$因此，$c^c_j$的计算变为：$$c^c_j = \\sum_{b \\in \\lbrace b’ \\mid w^d_{b’,j} \\in \\mathbb{D}\\rbrace } \\alpha^c_{b,j} \\odot c^w_{b,j} + \\alpha^c_j \\odot \\widetilde{c}^c_j$$在上式中，门$i^c_{b,j}$和$i^c_{j}$的值被归一化到$\\alpha^c_{b,j}$和$\\alpha^c_j$，和为1。$$\\alpha^c_{b,j} = \\frac{exp(i^c_{b,j})}{exp(i^c_j)+\\sum_{b’ \\in \\lbrace b’’ \\mid w^d_{b’’,j} \\in \\mathbb{D}\\rbrace}exp(i^c_{b’,j})}$$$$\\alpha^c_{j} = \\frac{exp(i^c_{j})}{exp(i^c_j)+\\sum_{b’ \\in \\lbrace b’’ \\mid w^d_{b’’,j} \\in \\mathbb{D}\\rbrace}exp(i^c_{b’,j})}$$最后的隐藏向量$h^c_j$仍然由之前的LSTM计算公式得到。在NER训练过程中，损失值反向传播到参数$w^c, b^c, w^w, b^w, w^l$和$b^l$使得模型可以动态地在NER标注过程中关注更相关的词。 解码和训练一个标准的CRF层被用在$h_1, h_2, …, h_{\\tau}$上面，其中$\\tau$对于基于字符的模型来说是$n$，对于基于词的模型来说是$m$。一个标签序列$y = l_1, l_2, …, l_{\\tau}$的概率是$$p(y \\mid s) = \\frac{exp(\\sum_i(w^{l_i}_{CRF} h_i + b^{(l_{i-1}, l_i)}_{CRF}))}{\\sum_{y’}exp(\\sum_i(w^{l’_i}_{CRF} h_i + b^{(l’_{i-1}, l’_i)}_{CRF}))}$$这里$y’$表示一个任意标签序列，$W^{l_i}_{CRF}$是针对于$l_i$的模型参数，$b^{(l_{i-1},l_i)}_{CRF}$是针对$l_{i-1}$和$l_i$的偏置。我们使用一阶维特比算法来寻找一个基于词或基于字符的输入序列中得分最高的标签序列。给定一组手动标注的训练数据$\\lbrace (s_i, y_i)\\rbrace \\mid^N_{i=1}$，带有L2正则项的句子层面的log-likelihood作为loss，训练模型：$$L = \\sum^N_{i=1} log(P(y_i \\mid s_i)) + \\frac{\\lambda}{2}\\Vert \\Theta \\Vert^2$$其中，$\\lambda$是L2正则项系数，$\\Theta$表示了参数集合。","link":"/blog/2018/05/23/lattice-lstm-中文ner/"},{"title":"leetcode algorithms #5","text":"leetcode algorithms #5. Title: Longest Palindromic SubstringGiven a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000. Example 1:123Input: &quot;babad&quot;Output: &quot;bab&quot;Note: &quot;aba&quot; is also a valid answer. Example 2:12Input: &quot;cbbd&quot;Output: &quot;bb&quot; 首先需要知道什么是回文串：如果一个字符串reverse后和原来一样，那就是回文串。 解法： 1. 枚举 遍历所有的字串，对每个字串判断是否为回文串。时间复杂度$O(n^3)$。因为字串的个数为$$n + (n - 1) + ··· + 1 = \\frac{n(n+1)}{2},$$所以遍历所有的字串的时间复杂度为$O(n^2)$，检查每个字符串是否是回文串，时间复杂度为$O(n)$，故时间复杂度为$O(n^3)$。 2. 动态规划 $P(i)$表示字符串$S$的第$i$个字符，$P(i,j)$表示第$i$个字符到第$j$个字符的字符串，假设字符串$S$长度为$n$，如果$P(2, n-1)$是回文串，且$P(1) == P(n)$，则字符串$S$也是回文串。这个不需要证明吧。所以即可用动态规划的思路求解。首先，每个单个字符都是回文串，然后判断长度为2的字符串是否是回文串，是的话可以存到hash表里面。然后判断长度为3的字符串，掐头去尾后看中间的是不是回文串，如果不是，直接跳过，是的话判断首尾两个字符是否相同，不相同就跳过，每次都记录当前已经判断的回文串的最大长度，最后即可得到最长的回文串长度。","link":"/blog/2018/07/18/leetcode-algorithms-5/"},{"title":"MnDOT traffic data","text":"MnDOT的全称是Minnesota Department of Transportation。RTMC traffic data是其的一个子集。美国明尼苏达州双子城交通管理中心的交通数据。地址：http://www.d.umn.edu/~tkwon/TMCdata/TMCarchive.html数据是RTMC采集的连续数据，是MnDOT的一个子集，超过4500个每30秒为间隔的线圈检测器部署Twin Cities Metro freeways。最近加入了Rochester线圈数据。每天的数据都会UMD的服务器被打包进一个zip文件，之后存入这个仓库。文件名是”yyyymmdd.traffic”，分别是年月日。使用unzip软件直接解压即可。解压后有9000个文件，4500个是流量数据，文件名是”###.v30”，另外4000个文件是占用率文件，是”###.o30”或者”###.c30”。###表示检测器的id。数据服务由UMD的Transportation Research Data Lab(TDRL)提供，旨在与学者分享资源与思路。我们鼓励数据使用者与我们联系，分享研究成果与想法。这个数据是免费的，但是禁止用于商业用途。最后，感谢RTMC如此慷慨地提供交通数据。 数据格式：http://www.d.umn.edu/~tkwon/TMCdata/Traffic.htmlMN/DOT已经收集了路中的检测器的数据很多年了。从2000年3月开始，在Twin Cities metro area，超过4000个检测器每30秒都会收集一次数据。原始数据包括了流量和占有率。每天都会有大量的数据，将这些数据存储进传统数据库的价值比乱放大很多。因此，这些数据的存储促使了MN/DOT交通数据文件格式的发展。这个格式现在是TDRL的UTSDF的一个特例。 UTSDF的优点很多。最重要的好处就是简单。早期的文件格式有复杂的bit操作，对数据分析工具很难操作。后来所有的数据存成8bit或16bit的整数解决了这个问题。这个格式的另一个好处是它的紧凑性。早期的格式，数据33M。现在这个格式，同样的数据只有13M（精度不变）。早期格式的另一个问题是30秒、5分钟的区分使得获取数据很麻烦。现在这个数据把所有数据融合到一个文件中，简化了读取数据的过程。 另一个重要的好处是可扩展性，未来可能在不牺牲紧凑性的情况下增加其他类型的数据（比如速度）。 每个traffic数据文件包含了一天的交通数据。文件一版命名为8个数字的日期加.traffic的后缀。压缩成了zip格式。每个检测器有两个文件，一个是整天的流量，另一个是占用率。这些文件的命名是检测器的id。流量的后缀是.v30，占用率是.o30。所以如果有个编号为100的检测器，那就有两个文件，100.v30和100.o30。 流量文件（.v30）共2880个字节。每字节是一个8比特带符号的流量值，每天30秒为一个周期。-1表示缺失值。最开始的8bit表示一天最开始的值，也就是午夜0点0分30秒，最后一个值是11点59分30秒。 占用率文件（.o30）和流量文件很相似，除了每个值是16bit。每个文件是5760字节。占用率值是从0到1000（百分点的十分之一）的fixed-point interger。-1表示缺失，16bit是高位优先（high-byte first order）。 以上格式说明修订于: 23 March 2000 附录：2001年8月3日 .c30文件是记录在”scans”中，并且比.o30文件更精确。不久所有的数据都会使用.c30格式。Scans定义为$\\frac{1}{60}$秒，所以数据的范围是0到1800（30秒 $\\times$ 60 scans/second），老版的文件.o30表示的是千分之一为单位的占有率，所以范围是0到1000。这是这两个文件的区别。如果你想要0到100的数据，将scan数据除以18，或者将占用率数据除以10。任何在这个范围外的数据都是有问题的数据。 对于流量数据，把他当成有符号或者没符号无所谓。因为样本是30秒的流量数据，如果有40量车通过那就说明每小时会通过4800量车，平均车与车之间差了0.75秒。肯定是不可能，所以我建议如果数据不在0到40之间，那就说明是异常值。 那些不同的负数是数据采集软件的小bug。未来我们会修复他们，所以对于流量数据，任何不在合理范围的数据都应该被当成异常值。 以上格式信息由TMC Mn/DOT的Doug Lau提供。","link":"/blog/2018/07/11/mndot-traffic-data/"},{"title":"leetcode algorithms #3","text":"leetcode algorithms #3. Title: Longest Substring Without Repeating Characters Longest Substring Without Repeating CharactersDescriptionGiven a string, find the length of the longest substring without repeating characters. Examples: Given “abcabcbb”, the answer is “abc”, which the length is 3. Given “bbbbb”, the answer is “b”, with the length of 1. Given “pwwkew”, the answer is “wke”, with the length of 3. Note that the answer must be a substring, “pwke” is a subsequence and not a substring. Discussion枚举：对于第一个字母，从它开始向后找，如果遍历的途中发现有重复的字母，停止，记录长度。然后从第二个字母开始，用同样的方法找。最后取所有长度中的最大值即可。时间复杂度: $O(n^2)$。 第二种方法滑动窗扫描对于第一个字母，从它开始向后找，记录此时首字符的下标为$i$，找到第一个重复的字母后，记录末字符小标为$j$。得到长度$j-i$。然后$i++$，将首字符下标向后移动一位，$j$变为$i+1$，若碰到重复的字符，计算长度$j-i$，$i++$，反复如此，最后取大的那个数。这个时间复杂度我不会计算，不过应该是比线性要慢一点点，但是趋近于线性扫描的速度。下面的sliding window是我实现的，看了一下别人的solutionAlgorithm The naive approach is very straightforward. But it is too slow. So how can we optimize it? In the naive approaches, we repeatedly check a substring to see if it has duplicate character. But it is unnecessary. If a substring $s_{ij}$ from index $i$ to $j - 1$ is already checked to have no duplicate characters. We only need to check if $s[j]$ is already in the substring $s_{ij}$. To check if a character is already in the substring, we can scan the substring, which leads to an $O(n^2)$ algorithm. But we can do better. By using HashSet as a sliding window, checking if a character in the current can be done in $O(1)$. A sliding window is an abstract concept commonly used in array/string problems. A window is a range of elements in the array/string which usually defined by the start and end indices, i.e. $[i, j)$ (left-closed, right-open). A sliding window is a window “slides” its two boundaries to the certain direction. For example, if we slide $[i, j)$ to the right by $1$ element, then it becomes $[i+1, j+1)$ (left-closed, right-open). Back to our problem. We use HashSet to store the characters in current window $[i, j)$ ($j = i$ initially). Then we slide the index $j$ to the right. If it is not in the HashSet, we slide $j$ further. Doing so until $s[j]$ is already in the HashSet. At this point, we found the maximum size of substrings without duplicate characters start with index $i$. If we do this for all $i$, we get our answer. 时间复杂度: $O(2n)=O(n)$，最坏情况下每个元素被访问两次空间复杂度：$O(min(m, n))$，$n$是字符串$s$的长度，$m$是字符种类个数 123456789101112131415161718public class Solution { public int lengthOfLongestSubstring(String s) { int n = s.length(); Set&lt;Character&gt; set = new HashSet&lt;&gt;(); int ans = 0, i = 0, j = 0; while (i &lt; n &amp;&amp; j &lt; n) { // try to extend the range [i, j] if (!set.contains(s.charAt(j))){ set.add(s.charAt(j++)); ans = Math.max(ans, j - i); } else { set.remove(s.charAt(i++)); } } return ans; }} 第三种方法是优化的sliding windowThe above solution requires at most $2n$ steps. In fact, it could be optimized to require only $n$ steps. Instead of using a set to tell if a character exists or not, we could define a mapping of the characters to its index. Then we can skip the characters immediately when we found a repeated character. The reason is that if $s[j]$ have a duplicate in the range $[i, j)$ with index $j’$, we don’t need to increase $i$ little by little. We can skip all the elements in the range $[i, j’]$ and let $i$ to be $j’ + 1$ directly.第三种方法的意思是，如果在当前的滑动窗$i$到$j$中，现在的j指向的元素与$j’$相同，这个$j’$在$i$和$j$之间，那我们就可以直接让$i=j+1$。假设字符串是’pwawb’，$i$是0，$j$是3，此时$j’$是1，我们可以让$i$直接跳到$j’+1$，因此我们需要一种能马上查询到$j’$的方法，也就是Hash table。$j$这个下标在移动过程中，如果碰到没有的见过的元素，就加入table中，如果见过，就获得当前的长度$j-i$，但是这里涉及到一个更新延时的问题。比如字符串”abba”，table中{‘a’: 0, ‘b’: 1}，此时$j$为2，那么$j’$为1，需要将$i$挪到table中$j$对应的值$+1$，即$i=2$，而且要更新table为{‘a’: 0, ‘b’: 2}。接下来$j=3$，$a$已经在table中了，但是$j’$在$i$前面，此时就不应该移动$i$了，直接更新$a$即可。 123456789101112131415public class Solution { public int lengthOfLongestSubstring(String s) { int n = s.length(), ans = 0; Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); // current index of character // try to extend the range [i, j] for (int j = 0, i = 0; j &lt; n; j++) { if (map.containsKey(s.charAt(j))) { i = Math.max(map.get(s.charAt(j)), i); } ans = Math.max(ans, j - i + 1); map.put(s.charAt(j), j + 1); } return ans; }} implementationsliding window 1 runing time: 816ms12345678910111213141516171819202122232425class Solution: def lengthOfLongestSubstring(self, s): \"\"\" :type s: str :rtype: int \"\"\" lengths = [] chars = set() i, j = 0, 1 if len(s) == 0: return 0 chars.add(s[i]) while i &lt; len(s): while j &lt; len(s): if s[j] in chars: lengths.append(j-i) i += 1 j = i + 1 chars = set([s[i]]) else: chars.add(s[j]) j += 1 if j == len(s): lengths.append(j - i) return max(lengths) sliding window 2 runing time: 120ms1234567891011121314151617class Solution: def lengthOfLongestSubstring(self, s): \"\"\" :type s: str :rtype: int \"\"\" chars = set() ans, i, j = 0, 0, 0 while i &lt; len(s) and j &lt; len(s): if s[j] not in chars: chars.add(s[j]) j += 1 ans = max(ans, len(chars)) else: chars.remove(s[i]) i += 1 return ans sliding window 3 runing time: 128ms12345678910111213141516class Solution: def lengthOfLongestSubstring(self, s): \"\"\" :type s: str :rtype: int \"\"\" chars = dict() ans, i, j = 0, 0, 0 while i &lt; len(s) and j &lt; len(s): if s[j] in chars: if chars[s[j]] &gt;= i: i = chars[s[j]] + 1 chars[s[j]] = j j += 1 ans = max(ans, j - i) return ans","link":"/blog/2018/02/24/leetcode-algorithms-3/"},{"title":"Multistep Speed Prediction on Traffic Networks: A Graph Convolutional Sequence-to-Sequence Learning Approach with Attention Mechanism","text":"AGC-Seq2Seq，投的是TRC。清华大学和高德地图合作的一项研究。作者采用了 GCN + Seq2Seq + Attention 的混合模型，将路网中的边构建成图中的结点，在 GCN 上做了改进，将邻接矩阵扩展到 k 阶并与一个权重矩阵相乘，类似 HA-GCN(2016)，实现了邻居信息聚合时权重的自由调整，可以处理有向图。时间关系上使用 Seq2Seq + Attention 建模，完成了北京市二环线的多步的车速预测，对比的方法中没有近几年出现的时空预测模型。 摘要为了在多步交通预测的任务中，捕获复杂的非平稳的时间动态性和空间依赖关系，我们提出了一个叫注意力图卷积序列到序列模型（AGC-Seq2Seq）。空间和时间用图卷积和序列到序列模型分开建模。注意力机制用来解决序列到序列模型在多步预测上的困难，同时来捕获交通流的异质性。 2. LITERATURE REVIEW如 Li et al. 2017 所述，统计模型、shallow machine learning models 和 深度学习模型是三个主要的方法。 统计模型基于过去的时间序列观测值对未来进行预测。ARIMA 模型，Kalman filter，还有它们衍生出的算法。然而，简单的时间序列模型通常依赖平稳假设，这与城市交通的动态性不符。特别是对于多步时间预测，后面的预测值是基于前面的预测值的，因此，预测的误差会逐渐的传播。使用简单的时间序列预测模型很难满足高精度的预测需求。 同时，机器学习方法在交通预测研究中表现的很好。神经网络模型，贝叶斯网络，支持向量机模型，K 近邻摩西那个，随机森林模型在交通流预测中表现的很好。然而，机器学习算法的表现依赖于手工选取特征，而且选取特征的方法是不存在的，因为关键特征一般因问题而异。因此，使用元素级别的机器学习方法在复杂的预测任务上不会产生好的效果。 最近，深度学习算法成功的应用在计算机科学中，同时，它在运输学科也吸引了很多人的注意。Huang et al. 2014 使用深度置信网络用于无监督学习，证明了在交通流预测上的有效性。Lv et al. 2015 使用一个堆叠的自编码器模型学习交通流特征。Ma et al 2015 使用 LSTM 有效地捕获了交通流的动态性。Polson and Sokolov 2017 融合了 $L_1$ 正则和 $\\text{tanh}$ 激活的多层网络来检测交通流的极端的非线性。然而，这些方法主要聚焦于对单个序列建模，不能反映交通网络的空间关系。 卷积神经网络提供了一个有效的架构来提取大尺度、高维的数据集中有效的统计模式。在学习局部平稳结构中，CNN 的能力在图像和视频识别任务中获得了很大的突破。在运输领域，也有学者使用 CNN 捕获交通网络上的空间关系。Ma et al. (2017) 提出了一个预测车速的深度卷积神经网络，将交通的时空动态性转换成图像。Wang et al. (2017) 将高速公路处理成一个 band image，提出了误差回传的循环卷积神经网络结构用于连续的交通速度预测。Ke et al. (2017) 将城市区域划分成均匀的网格，通过将卷积和 LSTM 层合并来预测每个网格内的乘客需求。上述的研究将交通网络转换为网格是因为 CNN 受限于处理欧氏空间的数据。然而，在交通预测上，路网上的时间序列是分布在一个拓扑图上连续的序列，是一种非欧式结构数据的典型 (Narang et al., 2013)；原本的 CNN 结构是不能使用的。为了解决这个问题，基于谱图理论的图卷积网络 (GCN) 可以用于在非欧式空间上使用卷积 (Kipf and Welling, 2016)。几个刚刚发表的研究在交通预测上使用了图卷积模型。基于谱的图卷积和时间上的卷积相结合 (Yu et al., 2017)，还有图卷积与循环神经网络 (RNN) 的结合 (Li et al., 2017) 来用于预测交通状态。之后，Cui et al. (2018) 使用高阶图卷积来学习路网上不同路段间的交互关系。上述研究没有在路网上直接定义图卷积，而是通过高斯核根据任意两个监测器间的距离构建了监测器之间的网络。此外，交通状况的时间关联也没有考虑。 总结一下，城市路网上交通状况的变化展示出了时空的依赖性。我们提出了一个定制版的深度学习框架，在 Seq2Seq 框架中继承了注意力机制和图卷积模型，同时捕获复杂的非平稳的空间动态性和多步交通预测的空间依赖性。 3. AGC-SEQ2SEQ DEEP LEARNING FRAMEWORK3.1 Preliminaries(1) Road network topology 路网根据驾驶方向构建成有向图 $\\mathcal{G}(\\mathcal{N}, \\mathcal{L})$ ，顶点集 $\\mathcal{N}$ 表示路口 (监测器或选择的高速公路的划分点)，边集 $\\mathcal{L}$ 表示路段，如图1所示。$\\boldsymbol{A}$ 是边集的邻接矩阵，$\\boldsymbol{A}(i, j)$ 表示边 $i$ 和 $j$ 是否相连，即 $$\\boldsymbol{A}(i, j) = \\begin{cases}1, &amp;\\text{if } \\quad l_i \\quad \\text{and} \\quad l_j \\quad \\text{are} \\quad \\text{connected} \\quad \\text{along} \\quad \\text{driving} \\quad \\text{direction}\\\\0, &amp;\\text{if } \\quad \\text{otherwise}\\end{cases}$$ (2) Traffic speed 路段 $l_i (\\forall l_i \\in \\mathcal{L})$ 的第 $t$ 个时段（比如 5 分钟）定义为路段上这个时间段浮动车的平均速度，表示为 $v^i_t$。路网在第 $t$ 个时段的速度定义为向量 $\\boldsymbol{V}_t \\in \\mathbb{R}^{\\vert \\mathcal{L} \\vert}$（$\\vert \\mathcal{L} \\vert$ 是边集 $\\mathcal{L}$ 的基数），第 $i$ 个元素是 $(\\boldsymbol{V}_t)_i = v^i_t$。 作为典型的时间序列预测问题，最近邻的 $m$ 步观测值可以对多步预测提供有价值的信息。除了实时的车速信息，一些外部变量，如时间、工作日还是周末，历史的统计信息也对预测有帮助。 (3) Time-of-day and weekday-or-weekend 因为路段的车速是聚合 5 分钟得到的平均值，时间会被转化为一个有序的实数，比如 00:00-00:05 转化为 $N_t = 1$，7:00-7:05 转化为 $N_t = 85(7 * 12 + 1)$，工作日或周末表示为 $p_t$，区分工作日和周末的不同特性。 (4) Historical statistic information 交通状态的每日趋势可以通过引入历史的统计数据捕获。历史的平均车速，中值车速，最大车速，最小车速，路段 $l_i$ 的 $t$ 时段的标准差，分别定义为训练集中的平均值、中位数、最大、最小、标准差，表示为 $v^i_{t,average}, v^i_{t,median}, v^i_{t,max}, v^i_{t,min}, d^i_t$。 (5) Problem formulation 车速预测是用之前观测到的速度预测一个确定时段每个路段上的车速。多步速度预测问题定义为： $$\\tag{1}\\hat{V}_{t+n} = \\mathop{\\arg\\max}\\limits_{V_{t+n}} \\text{Pr}(V_{t+n} \\mid V_t, V_{t-1}, \\dots, V_{t-m};\\mathcal{G})$$ 其中 $\\hat{V}_{t+n}(n=1,2,3,\\dots)$ 表示第 $n$ 步的预测速度，$\\lbrace V_t, V_{t-1}, \\dots, V_{t-m} \\mid m=1,2,\\dots \\rbrace$ 是之前观测到的值。$\\text{Pr}(·\\mid·)$ 是条件概率。 3.2 Graph Convolution on Traffic Networks图卷积通过谱域，将传统的卷积从网格上扩展到了图上。为了引入一般的 $K$ 阶图卷积，我们首先给每个路段 $l_i \\in \\mathcal{L}$ 定义了 $K$ 阶邻居 $\\mathcal{H}_i(K) = \\lbrace l_j \\in \\mathcal{L} \\mid d(l_i, l_j) \\leq K \\rbrace$，其中 $d(l_i, l_j)$ 表示所有从 $l_i$ 到 $l_j$ 的路径中最短路径的长度。 邻接矩阵就是一阶邻居，$K$ 次幂就是 $K$ 阶邻居。为了模仿拉普拉斯矩阵，我们在对角线上加了1，定义为： $$\\tag{2}\\boldsymbol{A}^K_{GC} = \\text{Ci}(\\boldsymbol{A}^K + \\boldsymbol{I})$$ 其中 $\\text{Ci}(·)$ 是clip function，将非0元素变成1；因此 $\\boldsymbol{A}^K_{GC}(i, j) = 1 \\quad for \\quad l_j \\in \\mathcal{H}_i(K) \\quad or \\quad i = j$；否则 $\\boldsymbol{A}^K_{GC}(i, j) = 0$。单位阵 $\\boldsymbol{I}$ 增加了自连接，卷积的时候可以考虑到自身。 基于上述的邻居矩阵，一个简单版本的图卷积(e.g., Cui et al., 2018)可以定义为： $$\\tag{3}\\boldsymbol{V}_t(K) = (\\boldsymbol{W}_{GC} \\odot \\boldsymbol{A}^K_{GC})\\cdot \\boldsymbol{V}_t$$ 其中 $\\boldsymbol{W}_{GC}$ 是一个和 $\\boldsymbol{A}$ 一样大小的可训练的矩阵。$\\odot$ 表示哈达玛积。通过哈达玛乘积，$(\\boldsymbol{W}_{GC} \\odot \\boldsymbol{A}^K_{GC})$ 可以得到一个在 $K$ 阶邻居上有参数，其他地方为0的新矩阵。因此，$(\\boldsymbol{W}_{GC} \\odot \\boldsymbol{A}^K_{GC})\\cdot \\boldsymbol{V}_t$ 可以理解成是一个对 $\\boldsymbol{V}_t$ 的空间离散的卷积。结果就是，$\\boldsymbol{V}_t(K)$ 是时间 $t$ 的融合空间的速度向量。它的第 $i$ 个元素 $v^i_t(K)$ 表示路段 $l_i \\in \\mathcal{L}$ 在时间 $t$ 的空间融合速度，这个速度集成了其邻居路段 $\\mathcal{H}_i(K)$ 的信息。 此外，式3可以分解成一个一维卷积。 $$\\tag{4}v^i_t(K) = (\\boldsymbol{W}_{GC}[i] \\odot \\boldsymbol{A}^K_{GC}[i])^T \\cdot \\boldsymbol{V}_t$$ $\\boldsymbol{W}_{GC}[i]$ 和 $\\boldsymbol{A}^K_{GC}[i]$ 分别是 $\\boldsymbol{W}_{GC}$ 和 $\\boldsymbol{A}^K_{GC}$ 的第 $i$ 行。图2是路网上 $\\boldsymbol{A}^K_{GC}[i]$ 的一个例子，路段 $i$ 在红线，邻居是蓝线。 3.3 Attention Graph Convolutional Sequence-to-Sequence Model (AGC-Seq2Seq)我们提出了 AGC-Seq2Seq 模型将时空变量和外部信息集成至深度学习架构中，用来做多步车辆速度预测。 为了捕获时间序列特征和获得多步输出，我们使用 Seq2Seq 作为整个方法的基础结构，由两个参数独立的 RNN 模块组成(Sutskever et al., 2014; Cho et al., 2014)。为了克服 RNN 输出的长度不可变，Seq2Seq 模型将输入进编码器的时间序列编码，解码器从 context vector 中解码出预测值。我们提出的 AGC-Seq2Seq 模型如图3所示。首先用图卷积来捕获空间特征，然后将时空变量 $v^i_{t-j}(K)$ 和外部信息 $\\boldsymbol{E}_{t-j}$（包括时间和工作日或周末信息）融合构成输入向量，然后放入 Seq2Seq的编码模型中。上述过程如下： $$\\tag{5}v^i_{t-j}(K) = (\\boldsymbol{W}_{GC}[i] \\odot \\boldsymbol{A}^K_{GC}[i])^T \\cdot \\boldsymbol{V}_{t-j}, \\quad 0 \\leq j \\leq m$$ $$\\tag{6}\\boldsymbol{E}_{t-j} = [N_{t-j};p_{t-j}]$$ $$\\tag{7}\\boldsymbol{X}^i_{t-j} = [v^i_{t-j}(K);\\boldsymbol{E}_{t-j}]$$ 其中 $N_{t-j}$ 和 $p_{t-j}$ 如3.1节定义，$[·;·]$ 操作是将两个张量拼接。 编码部分如式8-9，在时间步 $t-j, j\\in \\lbrace 0, \\dots, m \\rbrace$，前一个隐藏状态 $\\boldsymbol{h}_{t-j-1}$ 传入到当前时间戳和 $\\boldsymbol{X}_{t-j}$ 计算得到 $\\boldsymbol{h}_{t-j}$。因此，背景向量 $\\boldsymbol{C}$ 存储了包括隐藏状态 $(\\boldsymbol{h}_{t-m}, \\boldsymbol{h}_{t-m+1}, \\boldsymbol{h}_{t-1})$ 和输入向量 $(\\boldsymbol{X}_{t-m}, \\boldsymbol{X}_{t-m+1}, \\boldsymbol{X}_t)$ 的信息。 $$\\tag{8}\\boldsymbol{h}_{t-j} = \\begin{cases}\\text{Cell}_{encoder}(\\boldsymbol{h}_0, \\boldsymbol{X}_{t-j}), \\quad &amp;j = m\\\\\\text{Cell}_{encoder}(\\boldsymbol{h}_{t-j-1}, \\boldsymbol{X}_{t-j}), \\quad &amp;j \\in \\lbrace 0, \\dots, m-1 \\rbrace\\end{cases}$$ $$\\tag{9}\\boldsymbol{C} = \\boldsymbol{h}_t$$ 其中 $\\boldsymbol{h}_0$ 是初始隐藏状态，通常是 0 向量；$\\text{Cell}_{encoder}(·)$ 是编码器的计算函数，由使用的 RNN 结构决定。 在解码器的部分，关键是利用背景向量 $\\boldsymbol{C}$ 作为初始的隐藏向量，一步一步地解码。时间 $t+j, j \\in \\lbrace 1, \\dots, n \\rbrace$ 步，隐藏状态 $\\boldsymbol{h}_{t+j}$ 不仅包括输入信息，还考虑之前的输出状态 $(\\boldsymbol{h}_{t+1}, \\boldsymbol{h}_{t+2}, \\dots, \\boldsymbol{h}_{t+j-1})$。 解码器的输入依赖于训练方法。Teacher forcing 在 NLP 中是一个流行的训练策略。在 teacher-forcing 训练策略中，真值在训练的时候输入到解码器，测试的时候将预测值输入进解码器。这种方法不适合时间序列预测主要是因为在训练和测试的时候，输入到解码器的分布不一致。Li et al. (2017) 使用 scheduled sampling 缓解了这个问题，通过设定概率 $\\epsilon$，随机的将真值或预测值放入到解码器中。但这会增加模型的复杂度，给计算造成负担。 为了解决这个问题，我们提出了一个新的训练策略，将历史的统计信息和时间信息作为输入。在时间序列预测问题中，历史信息可以通过训练和测试阶段获得；这样解码器在训练和测试的时候，其输入的分布就可以相互同步，解决 teacher forcing 的问题。此外，因为历史统计信息在多步预测中很重要，增加这个可以提高模型的预测精度。下面的等式用来计算 $t+j,j \\in \\lbrace 1, \\dots, n \\rbrace$ 这个时间步解码器的隐藏状态。 $$\\tag{10}\\boldsymbol{v}^i_{t+j}(H) = [N_{t+j}; v^i_{t+j, average};v^i_{t+j, median}; v^i_{t+j, max}; v^i_{t+j, min}; d^i_{t+j}]$$ $$\\tag{11}\\boldsymbol{h}_{t+j} = \\begin{cases}\\text{Cell}_{decoder}(\\boldsymbol{C}, \\boldsymbol{v}^i_{t+j}(H)), \\quad &amp;j = 1\\\\\\text{Cell}_{decoder}(\\boldsymbol{h}_{t+j-1}, \\boldsymbol{v}^i_{t+j}(H)), \\quad &amp;j \\in \\lbrace 2, \\dots, n \\rbrace\\end{cases}$$ 其中 $\\text{Cell}_{decoder}$ 是解码器的计算公式，与编码器类似。 我们使用 GRU (Chung et al., 2014) 作为编码和解码的结构，如图4。实验效果比标准的 LSTM 好很多。编码器和解码器的计算过程如式12-17所示： $$\\tag{12}z_t = \\sigma(\\boldsymbol{W}_z \\cdot [\\boldsymbol{h}_{t-1}; x_t] + b_z)$$ $$\\tag{13}r_t = \\sigma(\\boldsymbol{W}_r \\cdot [\\boldsymbol{h}_{t-1}; x_t] + b_r)$$ $$\\tag{14}c_t = \\text{tanh}(\\boldsymbol{W}_c \\cdot [r_t \\odot \\boldsymbol{h}_{t-1}; x_t] + b_c)$$ $$\\tag{15}\\boldsymbol{h}_t = (1 - z_t) \\odot \\boldsymbol{h}_{t-1} + z_t \\odot c_t$$ $$\\tag{16}\\sigma(x) = \\frac{1}{1 + e^{-x}}$$ $$\\tag{17}\\text{tanh}(x) = \\frac{e^x - e^{-x}}{e^x+e^{-x}}$$ 在上式中，$z_t$ 和 $r_t$ 分别是更新门和重置门。$c_t$ 是候选输出，$\\sigma(\\cdot)$ 和 $\\text{tanh}(\\cdot)$ 是两个激活函数。$W_z$，$W_r$ 和 $W_c$ 是权重矩阵，$b_z$，$b_r$ 和 $b_c$ 是偏置。 为了捕获交通模式的外部信息，我们还集成了注意力机制 (Bahdanau et al., 2014; Luong et al., 2015)。注意力机制的关键在于在每个时间步增加捕获了源信息的相关性的注意力向量来帮助交通速度。在时间步 $t+j, j \\in \\lbrace 1, \\dots, n \\rbrace$，注意力函数定义为式 18-20，将 query $\\boldsymbol{h}_{t+j}$ 和 一组 key $(\\boldsymbol{h}_{t-m}, \\dots, \\boldsymbol{t-1}, \\boldsymbol{h}_t)$ 映射起来组成注意力向量 $\\boldsymbol{S}_{t+j}$。如下式 18-20，$\\boldsymbol{S}_{t+j}$ 通过计算这些 key 的带权和得到，权重通过计算得到： $$\\tag{18}u^{t-i}_{t+j} = \\boldsymbol{q}^T \\text{tanh} (\\boldsymbol{h}_{t+j} \\boldsymbol{W}_f \\boldsymbol{h}_{t-j}), \\quad i = 0,1,\\dots,m$$ $$\\tag{19}a^{t-i}_{t+j} = \\text{softmax}(u^{t-i}_{t+j}) = \\frac{\\text{exp}(u^{t-i}_{t+j})}{\\sum^m_{r=1} \\text{exp} (u^{t-r}_{t+j})}, \\quad i=0,1,\\dots,m$$ $$\\tag{20}\\boldsymbol{S}_{t+j} = \\sum^m_{i=1} a^{t-i}_{t+j} \\boldsymbol{h}_{t-i}$$ 其中，式 18 计算出的 $u^{t-i}_{t+j}$ 可以用来衡量 $\\boldsymbol{h}_{t+j}$ 和 $\\boldsymbol{h}_{t-i}$ 之间的相似性，我们使用 Luong Attention form (Luong et al., 2015) 作为注意力的计算公式，$\\boldsymbol{W}_f$ 和 $\\boldsymbol{q}^T$ 是参数，用来调节结果的维数；$a^{t-i}_{t+j}$ 是 $u^{t-i}_{t+j}$ 归一化的结果，用作对应编码器隐藏状态 $\\boldsymbol{h}_{t-i}$ 的权重来计算 $\\boldsymbol{S}_{t+j}$。 如图3所示，注意力隐藏状态 $\\tilde{\\boldsymbol{h}}_{t+j}$ 由注意力向量 $\\boldsymbol{S}_{t+j}$ 和原始隐藏状态 $\\boldsymbol{h}_{t+j}$ 通过一个简单拼接组成，如式 21 所示。式 22 表示从隐藏状态到输出的线性变换。参数 $\\boldsymbol{W}_v$ 和 $b_v$ 的维度与输出一致。 $$\\tag{21}\\tilde{\\boldsymbol{h}}_{t+j} = \\text{tanh} (\\boldsymbol{W}_h \\cdot [\\boldsymbol{S}_{t+k};\\boldsymbol{h}_{t+j}])$$ $$\\tag{22}\\hat{v}_{t+j} = \\boldsymbol{W}_v \\tilde{h}_{t+j} + b_v$$ 为了减少多步预测中的误差，我们定义了所有要预测的时间步上的平均绝对误差： $$\\tag{23}loss = \\frac{1}{n} \\sum^n_{j=1} \\vert \\hat{v}^i_{t+j} - v^i_{t+j} \\vert$$ 所有的参数通过随机梯度下降训练。 4 NUMERICAL EXAMPLES4.1 Dataset数据集是从 A-map 的用户收集的，是中国的一个手机导航应用提供的 (Sohu, 2018)。研究范围选择在了北京 2 环，是北京最堵的地方。如图5(a)所示，我们将 33km 长的二环以 200m 一段分成 163 个路段。此外，我们通过用户的轨迹点计算每个路段上 5 分钟的平均速度。2环上工作和和周末的车速如图5(b)(c)所示，x 轴是经度，y 轴是纬度，z 轴是时间和速度的颜色表。 数据范围是2016年10月1日到2016年11月30日。10月1日到11月20日做训练，11月21日到27日做测试。预测的范围是 06:00 到 22:00，因此，每条路段每天包含 192 个数据点。图6展示了划分的数据集。在数据清理后，缺失值通过线性插值的方法填补。 4.2 Model comparisons在每个部分，提出的模型对比的是其他的 benchmark 模型，包括传统的时间序列分析方法（如 HA 和 ARIMA），还有一些先进的机器学习方法（ANN, KNN, SVR, XGBOOST），深度学习模型（LSTM, GCN, Seq2Seq-Att）。 HA：历史均值模型通过训练集的统计值预测测试集的未来车速。举个例子，路段 $l_i \\in \\mathcal{L}$ 在 8:00-8:05 的平均车速通过训练集同时段同路段的历史速度均值估计。 ARIMA：$(p, d, q)$ 模型 (Box and Pierce, 1970)，差分的阶数设定为 $d = 1$，自回归部分的阶数和移动平均部分的阶数 $(p, q)$ 通过计算对应的 Akaike information criterion 决定，$p \\in [0, 2], q \\in [7, 12]$。 ANN：我们用了三层神经网络，sigmoid 激活，隐藏单元数是特征数的 2 倍。因为 ANN 不能区分时间步上的变量，所以它不能捕获时间依赖。 KNN：k 近邻，获取训练集中特征空间最相近的 k 个观测值。预测值通过对应的特征向量进行线性组合得到。超参数 $K$ 通过 5 到 25 折的交叉验证选定。 SVR：支持向量回归 (Suykens and Vandewalle, 1999)，通过核函数将特征向量映射到高维空间得到拟合曲线。核函数和超参数通过交叉验证选定。 XGBOOST：(Chen and Guestrin, 2016) 在很多机器学习任务上表现出了很好的效果；基于树结构可以扩展成端到端的系统。所有的特征 reshape 后输入到 XGBOOST 来训练。 LSTM：(Hochreiter and Schmidhuber, 1997)，每个路段的所有特征都 reshape 成一个矩阵，一个轴是时间，另一个轴是特征。LSTM 考虑时间依赖，但是没有捕获空间依赖。 GCN：GCN 中所有的路段的特征 reshape 成一个矩阵，一个轴是路段，另一个轴是特征。GCN 通过拉普拉斯矩阵将卷积泛化到非欧空间；因此，只考虑了空间关联，没有捕获时间依赖。 Seq2Seq-Att: 和 AGC-Seq2Seq 的区别是图卷积层。 为了保证共鸣，之前提到的预测模型都有和 AGC-Seq2Seq 同样的输入特征（特征类型和窗口长度），尽管传统的时间序列模型利用了训练集的全部速度记录。窗口长度为 12，也就是用过去一小时预测未来。19 维特征如表 1 所示。 所有的符号如 3.1 节定义，$n$ 是定值。我们通过三个错误指标评价模型，MAPE, MAE, RMSE, $\\text{MAPE} = \\frac{1}{Q} \\sum^Q_{i=1} \\frac{\\vert v_i - \\hat{v}_i \\vert}{v_i}$, $\\text{MAE} = \\frac{1}{Q} \\sum^Q_{i=1} \\vert v_i - \\hat{v}_i \\vert$, $\\text{RMSE} = \\sqrt{\\frac{1}{Q} \\sum^Q_{i=1} (v_i - \\hat{v}_i)^2}$，其中 $v_i$ 和 $\\hat{v}^i$ 分别是真值和预测值；$Q$ 是测试集大小。","link":"/blog/2019/01/21/multistep-speed-prediction-on-traffic-networks-a-graph-convolutional-sequence-to-sequence-learning-approach-with-attention-mechanism/"},{"title":"MXNet: Dependency Engine for Deep Learning","text":"MXNet: Dependency Engine for Deep Learning 我们总是想让深度学习框架跑的更快，能适应更大的数据集。一个自然的想法是我们能否通过堆更多的硬件解决问题，也就是同时使用多个 GPU。 框架设计者就会问：我们怎么才能让计算在设备间并行？而且，当我们引入多线程的时候，如何同步计算？一个运行环境依赖引擎是这些问题的解决方案。 在这篇文档中，我们检验了使用运行环境依赖调度装置来加速深度学习的方法，解释了运行环境依赖调度器如何同时加速和简化多设备深度学习。我们还探索了框架独立或操作独立的通用依赖引擎可能的设计方案。 这里的很多讨论都是源于 MXNet 依赖引擎。我们讨论的依赖追踪算法主要由 Yutian Li 和 Mingjie Wang 设计。 Dependency Scheduling尽管大多数用户想利用并行计算，但大部分人更熟悉串行编程。所以一个问题是：我们如何能写串行程序，构建一个库，自动地并行我们的程序？ 举个例子，下面的代码，我们可以以任意顺序运行 B = A + 1 和 C = A + 2 这两个命令，或是并行运行： 1234A = 2B = A + 1C = A + 2D = B * C 但是由于最后一个操作 D = B * C，导致手动编码序列很麻烦，最后一个操作需要等待前面的操作完成才能继续。下面的依赖图/数据流图展示了这个过程。 一个依赖引擎可以获取一个操作序列并且根据依赖关系调度他们，更可能以并行的方式。所以在这个例子中，一个依赖库可以并行运算 B = A + 1 和 C = A + 2，然后在这两个操作完成后运行 D = B * C。 Problems in Dependency Scheduling一个依赖引擎减轻了编写并发程序的负担。但是，由于操作可以并行化，新的依赖追踪问题产生了，这节我们讨论这些问题。 Data Flow Dependency数据流依赖表述了一个计算的输出如何用于其他的计算。每个依赖引擎必须解决数据流依赖问题。 因为我们在前面的部分讨论过这个问题，我们这里使用同一张图。包含数据流追踪引擎的框架包括 Minerva 和 Purine2。 Memory Recycling我们什么时候回收分配给 array 的内存？在串行程序中这个问题很简单。我们在变量在作用域中消失后回收即可。但是，下面的图展示了并行程序中这有多麻烦。 在这个例子中，两个操作都需要 A 的值，我们需要等两个操作都完成才能回收。引擎必须根据依赖来调度回收器，确保在 B = A + 1 和 C = A + 2 都完成后再执行。 Random Number Generation随机数生成器是机器学习中常用的，给依赖引擎提出了有趣的挑战。考虑下面的问题： 再这个例子中，我们以序列形式生成了随机数。尽管看起来两个随机数生成过程是并行的，但实际上不是。一个伪随机数生成器 (PRNG) 不是线程安全的，因为在生成新的随机数时，可能会导致一些内部状态的变化。即使 PRNG 是线程安全的，我们也希望数字的生成是串行的，因为我们可以得到可重现的随机数序列。","link":"/blog/2019/04/07/mxnet-dependency-engine-for-deep-learning/"},{"title":"Hadoop HA安装二：MySQL双机热备","text":"Hadoop HA安装二：MySQL双机热备 安装MySQL安装前先安装一下MySQL的依赖ubuntu:1# apt-get install libaio-dev CentOS:1# yum install libaio 看了很多教程都不靠谱。。。还是官方教程最靠谱：Installing MySQL on Unix/Linux Using Generic Binaries 下载mysql-5.6.37-linux-glibc2.12-x86_64# cp mysql-5.6.37-linux-glibc2.12-x86_64.tar.gz /usr/local/ 解压到/usr/local/# tar -zxvf mysql-5.6.37-linux-glibc2.12-x86_64.tar.gz 改名为mysql# mv mysql-5.6.37-linux-glibc2.12-x86_64 mysql 删除安装包# rm mysql-5.6.37-linux-glibc2.12-x86_64.tar.gz 修改环境变量# vi /etc/profile在最下面添加12export MYSQL_HOME=/usr/local/mysqlexport PATH=$MYSQL_HOME/bin:$PATH 新建用户和用户组：mysql# groupadd mysql# useradd -r -g mysql -s /bin/false mysql # cd /usr/local/mysql 修改目录的拥有者# chown -R mysql .(重要！)# chgrp -R mysql .(重要！) 安装MySQL# scripts/mysql_install_db --user=mysql 修改当前目录拥有者为root用户# chown -R root . 修改当前data目录拥有者为mysql用户# chown -R mysql data 启动MySQL进程# bin/mysqld_safe --user=mysql &amp; 此时这个窗口会卡住，新建一个terminal，进入/usr/local/mysql中 进入mysql控制台# bin/mysql 退出exit; 进行MySQL的root用户密码的修改等操作# ./bin/mysql_secure_installation首先要求输入root密码，由于我们没有设置过root密码，括号里面说了，如果没有root密码就直接按回车。是否设定root密码，选y，设定密码为cluster，是否移除匿名用户：y。然后有个是否关闭root账户的远程登录，选n，删除test这个数据库？y，更新权限？y，然后ok。 # cp support-files/mysql.server /etc/init.d/mysql.server 查看MySQL的进程号# ps -ef | grep mysql 如果有的话就kill掉，保证MySQL已经中断运行了，一般kill掉/usr/local/mysql/bin/mysqld开头的即可# kill 进程号 启动MySQL# /etc/init.d/mysql.server start -user=mysql# exit 还需要配置一下访问权限：123$ mysql -u root -pmysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;cluster&apos; WITH GRANT OPTION;mysql&gt; FLUSH PRIVILEGES; 这样就可以使用root用户在任意地点登陆，而不再限于localhost了关于服务的三个命令。启动mysql：# /etc/init.d/mysql.server start -user=mysql停止mysql：# mysqladmin -u root -p shutdown 修改MySQL的数据存储位置在装系统进行分区的时候，有些时候会创建比较大的分区开存数据，我们可以将MySQL的数据存放到这个区内，假设这个区为/file0 $ su root 把MySQL服务进程停掉：# mysqladmin -u root -p shutdown 新建新的dataDir# mkdir /file0/mysql_data 把/usr/local/mysql/data里面的东西移到/file0/mysql_data下# cp /usr/local/mysql/data/* /file0/mysql_data 编辑MySQL的配置文件/etc/my.cnf如果没有的话，就把/usr/local/mysql里面的my.cnf复制过去# cp /usr/local/mysql/my.cnf /etc/# vi /etc/my.cnf 把里面的basedir, datadir, port修改成下面的内容123basedir=/usr/local/mysqldatadir=/file0/mysql_dataport=3306 修改MySQL启动脚本/etc/init.d/mysql.server# vi /etc/init.d/mysql.server把里面的basedir和datadir作如上修改 修改新目录的权限：# chown –R mysql /file0/mysql_data# chgrp –R mysql /file0/mysql_data退出root用户重新启动MySQL服务$ /etc/init.d/mysql.server start –user=mysql 进入mysql$ mysql –u root -p 查看目录是否已经更改mysql&gt; show variables like “datadir”; 高可用的MySQL双机热备安装教程开启二进制日志，设置id# vi /etc/my.cnf1234567[mysqld]server-id = 1 #backup这台设置2log-bin = mysql-binbinlog-ignore-db = mysql,information_schema #忽略写入binlog日志的库auto-increment-increment = 2 #字段变化增量值auto-increment-offset = 1 #初始字段ID为1slave-skip-errors = all #忽略所有复制产生的错误 重启MySQL服务# mysqladmin -u root -p shutdown# /etc/init.d/mysql.server start –user=mysql 先查看下log bin日志和pos值位置里面有个File和Position，分别是log_file和log_pos的值，一会儿要填mysql&gt; show master status; master配置如下：123456789mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &apos;replication&apos;@&apos;192.168.1.%&apos; IDENTIFIED BY &apos;replication&apos;;mysql&gt; flush privileges;mysql&gt; change master to -&gt; master_host=&apos;192.168.1.212&apos;, # 此处输入slave的ip地址 -&gt; master_user=&apos;replication&apos;, -&gt; master_password=&apos;replication&apos;, -&gt; master_log_file=&apos;mysql-bin.000001&apos;, -&gt; master_log_pos=120; #对端状态显示的值mysql&gt; start slave; #启动同步 backup配置如下：123456789mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &apos;replication&apos;@&apos;192.168.1.%&apos; IDENTIFIED BY &apos;replication&apos;;mysql&gt; flush privileges;mysql&gt; change master to -&gt; master_host=&apos;192.168.1.211&apos;, # 此处输入master的ip地址 -&gt; master_user=&apos;replication&apos;, -&gt; master_password=&apos;replication&apos;, -&gt; master_log_file=&apos;mysql-bin.000001&apos;, -&gt; master_log_pos=120;mysql&gt; start slave; MySQL双击热备安装完成 测试在一台机器上建立一个数据库，创建一个表，在另一台机器上查询是有结果的，说明安装成功。","link":"/blog/2017/08/21/mysql双机热备/"},{"title":"MXNet 与 cuda 版本兼容的问题","text":"最近在做实验的时候发现了一个非常神奇的问题，搞得我一度很郁闷。我在 kaggle 上面写了个 mxnet symbolic 的程序，在测试集上效果不错，论文都写完了，结果拿回实验室的 GPU 上一跑，发现结果复现不了了，差了两个点。但我所有的实验都做了 10 次，如果说 1 次实验效果好还可以说是巧合，但这是 10 次实验啊。 尝试找了一下问题在哪里，首先是 GPU 型号，kaggle 上面提供的是 Tesla P100，非常强劲的 GPU，16G 的显存，而且好像还支持半精度浮点运算。我在实验室使用了 RTX 2080 跑实验。在 4 台 RTX 2080 上面搭建了 OpenPAI，微软的一个开源深度学习资源调度平台。 我在 kaggle 上跑 10 次，测试集指标是 18.039，做了 10 次实验取的平均值，方差是 0.075，非常稳定，也就是对于随机性不敏感，所以不需要指定随机种子什么的，我也不爱指定随机种子，因为我觉得好的模型就应该对随机性不敏感。 为了验证是哪里出了问题，我打印了 kaggle 的环境配置，kaggle 使用的 mxnet_cu100 1.5.0，numpy 1.16.4。 我用 Docker 构建了 4 个镜像： softwares cuda 100 cuda 101 mxnet 1.41 mx1.41_cu100 mx1.41_cu101 mxnet 1.50 mx1.50_cu100 mx1.50_cu101 在安装的时候没有安装mkl。 每个镜像跑同一个实验 3 次吧，最近没什么时间跑 10 次，结果等我跑完了再更新。 2019年8月9日更新： 跑完了，发现结果全都一样，和显卡，cuda，mxnet 版本都无关。。。 后来找了一下问题，问题出在 training set 的 dataloader，忘了给 training set shuffle 了，所以效果变差了。。。","link":"/blog/2019/08/02/mxnet-与-cuda-版本兼容的问题/"},{"title":"Perceptual Losses for Real-Time Style Transfer and Super-Resolution","text":"ECCV 2016，实时风格迁移与超分辨率化的感知损失，这篇论文是在cs231n里面看到的，正好最近在研究风格迁移。一作是Justin Johnson，2017春的cs231n的主讲之一。这篇论文的主要内容是对Gatys等人的风格迁移在优化过程中进行了优化，大幅提升了性能。主要原理就是，之前Gatys等人的论文是利用已经训练好的VGG19，求loss并利用VGG的结构反向求导更新图片。由于VGG结构复杂，这样反向更新速度很慢，改进方法是再另外设计一个神经网络，将内容图片作为输入，输出扔到VGG中做两个loss，然后反向传播更新当前这个神经网络的参数，这样训练出来的神经网络就可能将任意的内容图片扔进去，输出为风格迁移后的图片，这也就解决了速度的问题。这也就是将Feed-forward image transformation与style transfer结合在一起。原文链接：Perceptual Losses for Real-Time Style Transfer and Super-Resolution Image Transformation NetworkArchitectureWe do not use any pooling layers, instead using strided and fractionally strided convolutions for in-network downsampling and upsampling. Our network body consists of five residual blocks using the architecture of http://torch. ch/blog/2016/02/04/resnets.html. All non-residual convolutional layers are followed by spatial batch normalization and ReLU nonlinearities with the exception of the output layer, which instead uses a scaled tanh to ensure that the output image has pixels in the range [0, 255]. Other than the first and last layers with use $9 \\times 9$ kernels, all convolutional layers use $3 \\times 3$ kernels. The exact architectures of all our networks can be found in the supplementary material. Inputs and OutputsFor style transfer the input and output are both color images of shape #3 \\times 256 \\times 256#.For super-resolution with an upsampling factor of $f$, the output is a high-resolution patch of shape $3 \\times 288/f \\times 288/f$. Since the image transformation networks are fully-convolutional, at test-time they can be applied to images of any resolution. Downsampling and UpsamplingFor super-resolution with an upsampling factor of $f$, we use several residual blocks followed by $log_2f$ convolutional layers with stride $1/2$. This is different from [1] who use bicubic interpolation to upsample the low-resolution input before passing it to the network. Our style transfer networks use the architecture shown in Table 1 and our super-resolution networks use the architecture shown in Table 2. In these tables “$C \\times H \\times W$ conv” denotes a convolutional layer with $C$ filters size $H \\times W$ which is immeidately followed by spatial batch normalization [1] and a ReLU nonlinearity.Our residual blocks each contain two $3 \\times 3$ convolutional layers with the same number of filters on both layer. We use the residual block design of Gross and Wilber [2] (shown in Figure 1), which differs from that of He et al [3] in that the ReLU nonlinearity following the addition is removed; this modified design was found in [2] to perform slightly better for image classification.For style transfer, we found that standard zero-padded convolutions resulted in severe artifacts around the borders of the generated image. We therefore remove padding from the convolutions in residual blocks. A $3 \\times 3$ convolution with no padding reduces the size of a feature map by 1 pixel on each side, so in this case the identity connection of the residual block performs a center crop on the input feature map. We also add spatial reflection padding to the beginning of the network so that the input and output of the network have the same size. Perceptual Loss FunctionsWe define two perceptual loss functions that measure high-level perceptual and semantic differences between images. They make use of a loss network $\\phi$ pretrained for image classification, meaning that these perceptual loss functions are themselves deep convolutional neural networks. In all our experiments $\\phi$ is the 16-layer VGG network pretrained on the ImageNet dataset. Featue Reconstruction LossRather than encouraging the pixels of the output image $\\hat{y} = f_W(x)$ to exactly match the pixels of the target image $y$, we instead encourage them to have similar feature representations as computed by the loss network $\\phi$. Let $\\phi_j(x)$ be the activations of the jth layer of the network $\\phi$ when processing the image $x$; if $j$ is a convolutional layer then $\\phi_j(x)$ will be a feature map of shape $C_j \\times H_j \\times W_j$. The feature reconstruction loss is the (squared, normalized) Euclidean distance between feature representations:$$\\ell_{feat}^{\\phi,j}(\\hat{y}, y)=\\frac{1}{C_jH_jW_j}\\Vert \\phi_j(\\hat{y})-\\phi_j(y)\\Vert_2^2$$As demonstrated in [6] and reproduced in Figure 3, finding an image $\\hat{y}$ that minimizes the feature reconstruction loss for early layers tends to produce images that are visually indistinguishable from $y$. Style Reconstruction LossThe feature reconstruction loss penalizes the output image $\\hat{y}$ when it deviates in content from the target $y$. We also wish to penalize differences in style: colors, textures, common patterns, etc. To achieve this effect, Gatys et al propose the following style reconstruction loss.As above, let $\\phi_j(x)$ be the activations at the $j$th layer of the network $\\phi$ for the input $x$, which is a feature map of shape $C_j \\times H_j \\times W_j$. Define the Gram matrix $G^\\phi_j(x)$ to be the $C_j \\times C_j$ matrix whose elements are given by$$G^\\phi_j(x)_{c,c’}=\\frac{1}{C_jH_jW_j}\\sum^{H_j}_{h=1}\\sum_{w=1}^{W_j}\\phi_j(x)_{h,w,c}\\phi_j(x)_{h,w,c’}$$ ExperimentsStyle TransferSingle-Image Super_ResolutionThis is an inherently ill-posed problem, since for each low-resolution image there exist multiple high-resolution images that could have generated it. The ambiguity becomes more extreme as the super-resolution factor grows; for larger factors ($\\times 4$, $\\times 8$), fine details of the high-resolution image may have little or no evidence in its low-resolution version.To overcome this problem, we train super-resolution networks not with the per-pixel loss typically used [1] but instead with a feature reconstruction loss to allow transer of semantic knowledge from the pretrained loss network to the super-resolution network. We focus on $\\times 4$ and $\\times 8$ super-resolution since larger factors require more semantic reasoning about the input.The traditional metrics used to evaluate super-resolution are PSNR and SSIM, both of which have been found to correlate poorly with human assessment of visual quality. PSNR and SSIM rely only on low-level differences between pixels and operate under the assumption of additive Gasussian noise, which may be invalid for super-resolution. In addition, PSNR is equivalent to the per-pixel loss $\\mathcal{l_{pixle}}$, so as measured by PSNR a model trained to minimize feature reconstruction loss should always outperform a model trained to minimize feature reconstruction loss. We therefore emphasize that the goal of these experiments is not to achieve state-of-the art PSNR or SSIM results, but instead to showcase the qualitative difference between models trained with per-pixel and feature reconstruction losses. code我用gluon实现了一个2x的超分辨率网络，训练后感觉效果一般，只有一次loss降到了40附近，那次效果挺好，但是颜色并不是很好以下是代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128import mxnet as mxfrom mxnet import ndimport numpy as npimport osfrom mxnet import autogradfrom mxnet import gluonfrom mxnet.gluon import nnfrom mxnet import initfrom mxnet.gluon.model_zoo import vision as modelsimport matplotlib.pyplot as plt# get_ipython().run_line_magic('matplotlib', 'inline')import logginglogger = logging.getLogger(__name__)data_filenames = ['trainx_%s.params'%(i) for i in range(7)]target_filenames = ['trainy_%s.params'%(i) for i in range(7)]load_params = Trueepochs = 500batch_size = 4ratio = 0.1learning_rate = 1e-5start_index, end_index = 0, 7num_samples = end_index - start_indexctx = [mx.gpu(i) for i in range(1)]if load_params == False: with open('training.log', 'w') as f: f.write('')class residual_unit(gluon.HybridBlock): def __init__(self, channels, **kwargs): super(residual_unit, self).__init__(**kwargs) self.conv1 = nn.Conv2D(channels = channels, padding=1, kernel_size=3, strides=1, use_bias=False) self.bn1 = nn.BatchNorm(momentum=0.9) self.act1 = nn.Activation('relu') self.conv2 = nn.Conv2D(channels = channels, padding=1, kernel_size=3, use_bias=False) self.bn2 = nn.BatchNorm(momentum=0.9) self.act2 = nn.Activation('relu') self.conv3 = nn.Conv2D(channels = channels, kernel_size=1, strides=1, use_bias=False) self.act3 = nn.Activation('relu') def hybrid_forward(self, F, x): t = self.act1(self.bn1(self.conv1(x))) t = self.act2(self.bn2(self.conv2(t))) x2 = self.conv3(x) return self.act3(t + x2)class plsr_network(gluon.HybridBlock): def __init__(self, **kwargs): super(plsr_network, self).__init__(**kwargs) with self.name_scope(): conv1 = nn.Conv2D(channels=64, padding=4, kernel_size=9, strides=1, use_bias=False) residual_sequential = nn.HybridSequential() for i in range(4): residual_sequential.add(residual_unit(64)) deconv1 = nn.Conv2DTranspose(channels=64, kernel_size=3, strides=2, padding=1, output_padding=1, use_bias=False) conv2 = nn.Conv2D(channels=3, padding=4, kernel_size=9, strides=1, use_bias=False) self.net = nn.HybridSequential() self.net.add( conv1, residual_sequential, deconv1, conv2 ) def hybrid_forward(self, F, x): out = x for i in self.net: out = i(out) return outdef tv_loss(x): data1 = nd.mean(nd.abs(x[:, :, 1:, :] - x[:, :, :-1, :])) data2 = nd.mean(nd.abs(x[:, :, :, 1:] - x[:, :, :, :-1])) return data1 + data2def get_vgg_loss_net(pretrained_net): net = nn.HybridSequential() for i in range(9): net.add(pretrained_net.features[i]) return netdef get_loss(vgg_loss_net, output, target, ratio = 0.1): return nd.mean(nd.square(vgg_loss_net(output) - target)) + ratio * tv_loss(output)rgb_mean = nd.array([0.485, 0.456, 0.406]).reshape(shape = (3,1,1))rgb_std = nd.array([0.229, 0.224, 0.225]).reshape(shape = (3,1,1))data = nd.empty(shape = (num_samples*1000, 3, 72, 72))target = nd.empty(shape = (num_samples*1000, 128, 72, 72))total_size = 0for index, (i, j) in enumerate(list(zip(data_filenames, target_filenames))[:num_samples]): x, y = nd.load(i)[0], nd.load(j)[0] assert x.shape[0] == y.shape[0] total_size += x.shape[0] data[index*1000: (index+1)*1000], target[index*1000: (index+1)*1000] = x, ydata = data[:total_size]target = target[:total_size]data[:] -= data.mean(0)plsr = plsr_network()if load_params == True: plsr.load_params('plsr.params', ctx = ctx)else: plsr.initialize(ctx = ctx, init=init.Xavier())plsr.hybridize()pretrained_net = models.vgg16(pretrained=True)vgg_loss_net = get_vgg_loss_net(pretrained_net)vgg_loss_net.collect_params().reset_ctx(ctx)trainer = gluon.trainer.Trainer(plsr.collect_params(), 'adam', {'learning_rate': learning_rate, 'beta1': 0.9, 'beta2': 0.99})dataloader = gluon.data.DataLoader(gluon.data.ArrayDataset(data, target), batch_size = batch_size, shuffle=True)for epoch in range(epochs): training_loss = 0. for data, target in dataloader: data_list = gluon.utils.split_and_load(data, ctx) target_list = gluon.utils.split_and_load(target, ctx) with autograd.record(): losses = [] for index, (data, target) in enumerate(zip(data_list, target_list)): losses.append(get_loss(vgg_loss_net, (plsr(data)-rgb_mean.copyto(data.context))/rgb_std.copyto(data.context), target, ratio)) for loss in losses: loss.backward() trainer.step(batch_size) training_loss += sum([l.asscalar() for l in losses]) print(epoch, training_loss) with open('training.log', 'a') as f: f.write(str(training_loss)+'\\n') plsr.save_params('plsr.params') 在实现的时候，超分辨率后需要一个后处理——直方图匹配，这里参考的是rio-hist。实验数据最开始用的是Microsoft的coco2017，将每张图随机截取$144 \\times 144$像素的大小，然后使用宽度为1的高斯核进行模糊处理后，downsampling了一下，得到了$72 \\times 72$的图片，作为网络的输入。后来发现效果不是很好，就打算向waifu2x一样，只训练动漫图片，上konachan上爬了一万张图，做同样的处理。此时的loss降到了31.这是训练的最好的一次，最左侧是输入的模糊图片，第二列是网络的输出，第三列是做了直方图匹配得到的图片，第四列是ground truth。可以看到有很多小点点，我分析是tv loss占比太小的原因，当前tv loss乘以了0.1。于是将tv loss乘以0.5后又训练了一次，loss降到了58，结果如下：感觉没法看了。。。","link":"/blog/2018/03/01/perceptual-losses-for-real-time-style-transfer-and-super-resolution/"},{"title":"pyspark中的HBaseConverters","text":"最近项目上有个需求，使用 pyspark 读取 HBase 中存储的 java.math.BigDecimal。 最近甲方让我们写一个 pyspark 的教程，他们以后打算使用 pyspark 开发。他们的数据是那种精度要求比较高的数据，我们使用 java.math.BigDecimal 表示数字，然后转成 byte[] 后存入了 HBase，但是 python 是没法直接读取这个 BigDecimal，所以需要使用 spark-examples 中 HBaseConverters.scala 读取。 我们讨论的 spark 版本是 1.6，因为用的是 CDH 5，所以是这个版本。 原理实际上是，pyspark 在读取 HBase 的时候需要借助 org.apache.spark.examples.pythonconverters 这么一个类，这个类实际上是 scala 将 HBase 中的数据读取后，转换成 json 字符串返回，这样 pyspark 可以通过这个类从 HBase 中直接获取到 json 字符串这样的返回值。 可以从 HBaseConverters.scala 这里看到 HBaseConverters.scala 的源码，我们感兴趣的是从 HBase 中查询 value 这一部分： 1234567891011121314151617181920212223242526272829303132package org.apache.spark.examples.pythonconvertersimport scala.collection.JavaConverters._import scala.util.parsing.json.JSONObjectimport org.apache.spark.api.python.Converterimport org.apache.hadoop.hbase.client.{Put, Result}import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.hbase.KeyValue.Typeimport org.apache.hadoop.hbase.CellUtil/** * Implementation of [[org.apache.spark.api.python.Converter]] that converts all * the records in an HBase Result to a String */class HBaseResultToStringConverter extends Converter[Any, String] { override def convert(obj: Any): String = { val result = obj.asInstanceOf[Result] val output = result.listCells.asScala.map(cell =&gt; Map( \"row\" -&gt; Bytes.toStringBinary(CellUtil.cloneRow(cell)), \"columnFamily\" -&gt; Bytes.toStringBinary(CellUtil.cloneFamily(cell)), \"qualifier\" -&gt; Bytes.toStringBinary(CellUtil.cloneQualifier(cell)), \"timestamp\" -&gt; cell.getTimestamp.toString, \"type\" -&gt; Type.codeToType(cell.getTypeByte).toString, \"value\" -&gt; Bytes.toStringBinary(CellUtil.cloneValue(cell)) ) ) output.map(JSONObject(_).toString()).mkString(\"\\n\") }} 这段代码很简单，实际上就是使用 java HBase 的 API 读取 HBase 中的值，将所有的值转换为 String 返回，我需要做的，只是将 value 这个字段的值，先从 byte[] 转到 BigDecimal，再转换为 String 即可。 12345678910111213141516class MyHBaseResultToStringConverter extends Converter[Any, String] { override def convert(obj: Any): String = { val result = obj.asInstanceOf[Result] val output = result.listCells.asScala.map(cell =&gt; Map( \"row\" -&gt; Bytes.toStringBinary(CellUtil.cloneRow(cell)), \"columnFamily\" -&gt; Bytes.toStringBinary(CellUtil.cloneFamily(cell)), \"qualifier\" -&gt; Bytes.toStringBinary(CellUtil.cloneQualifier(cell)), \"timestamp\" -&gt; cell.getTimestamp.toString, \"type\" -&gt; Type.codeToType(cell.getTypeByte).toString, \"value\" -&gt; Bytes.toBigDecimal(CellUtil.cloneValue(cell)).toString() ) ) output.map(JSONObject(_).toString()).mkString(\"\\n\") }} 这样代码就改完了，然后需要编译，打成 jar 包。 装好 maven，我装的是 3.6.0，不需要配置什么。 下载 spark 的源码，最开始我从 github 上面下载的，发现速度很慢，然后就去 spark 官网，找仓库中的源代码下载下来。 编译 spark-examples 的时候需要先从根目录中把 scalastyle-config.xml 拷贝到 examples 目录下再进行编译 cd 到 examples 目录下，使用以下命令编译 spark-examples 1mvn clean install -pl :spark-examples_2.10 编译的时候没有遇到错误，编译好的包在同级目录下的 target 中，有个叫 spark-examples_2.10-1.6.0.jar 的文件。 然后就是使用这个包读取 HBase 中的 BigDecimal了： 我们使用 standalone 模式运行 pyspark：1pyspark --master spark://host1:7077 --jars spark-examples_2.10-1.6.0.jar 123456789101112131415161718192021import jsonzookeeper_host = 'host1'hbase_table_name = 'testTable'conf = {\"hbase.zookeeper.quorum\": zookeeper_host, \"hbase.mapreduce.inputtable\": hbase_table_name}keyConv = \"org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter\"# 注意这里，使用自己定义的Converter读取valueConv = \"org.apache.spark.examples.pythonconverters.MyHBaseResultToStringConverter\"hbase_rdd = sc.newAPIHadoopRDD( \"org.apache.hadoop.hbase.mapreduce.TableInputFormat\", \"org.apache.hadoop.hbase.io.ImmutableBytesWritable\", \"org.apache.hadoop.hbase.client.Result\", keyConverter=keyConv, valueConverter=valueConv, conf=conf)hbase_rdd = hbase_rdd.flatMapValues(lambda v: v.split(\"\\n\")).mapValues(json.loads)hbase_rdd.take(1) 然后就可以看到结果了。 以上就是如何通过修改 HBaseConverters.scala 让 pyspark 从 HBase 中读取 java 的特殊类型。","link":"/blog/2019/04/06/pyspark中的hbaseconverters/"},{"title":"MXNet: Optimizing Memory Consumption in Deep Learning","text":"MXNet: Optimizing Memory Consumption in Deep Learning 过去的十年，深度学习模型趋向于更深更大的网络。尽管硬件性能的快速发展，前沿的深度学习模型还是继续将 GPU 显存的限制推向极限。所以即便在今天，人们仍在寻找办法消耗更少的内存，训练更大的模型。这样可以让我们训练的更快、使用更大的批量、获得更高的 GPU 利用率。 在这篇文档中，我们讨论集中优化内存分配的技术。尽管我们的讨论不彻底，但这些方案具有指导意义，使我们能够介绍主要的设计问题。 Computation Graph计算图描述了操作间的依赖。图中的操作要么是细粒度的，要么是粗粒度的。下图展示了两个计算图的例子。 计算图的概念被明确地编码进了库中，如 Theano 和 CGT。其他库中，计算图隐式地作为网络的配置文件。主要区别是如何计算梯度。主要有两种方法：在同一个图上做反向传播或明确地表示出一个回溯的路径来计算需要的梯度。 像 Caffe，CXXNet，Torch这样的框架使用前者，在原图上做反向传播。Theano 和 CGT 使用后者，显示地表示反向路径。我们讨论显示地反向路径方法，因为它对于优化有几个优势。 然而，我们应该强调一下选择显示反向路径方法并不会限制我们使用符号式的库，如 Theano 和 CGT。我们也可以用显示反向路径对基于层（将前向和反向绑起来）的库进行梯度计算。下面的图表示了这个过程。基本上来说，我们引入反向结点，连接图中的前向节点，在反向操作的时候调用 layer.backward。 这个讨论可以应用在几乎所有现存的深度学习框架上。 为什么显示反向路径更好？我们可以看两个例子。第一个原因是显示反向路径清晰地描述了计算间的依赖关系。考虑一种情况，我们想获得 A 和 B 的梯度。我们可以从图中清楚地看到，d(C) 梯度的计算不依赖于 F。这意味着我们可以在前向传播完成后释放 F 的内存。类似的，C 的内存也可以被回收。 拥有不同的反向路径而不是前向传播的镜像的能力是其另一个优点。一个常见的例子是分离连接的情况，如下图： 在这个例子中，B 的输出由两个操作引用。如果我们想在同一个网络中计算梯度，我们需要引入一个显示的分割层。这意味着我们需要对前向也做一次分离。如图，前向不包含一个分割层，但是图会自动地在将梯度传回 B 之前插入一个梯度聚合结点。这有助于我们节省分配分割输出的内存成本，以及在前向传递中复制数据的操作成本。 如果我们应用显示反向方法，在前向和反向的时候就没有区别。我们简单地按时间顺序进入计算图，开始计算。这使得显示反向路径容易去分析。我们仅需要回答一个问题：我们如何对计算图每个输出结点分配内存？ What Can Be Optimized?计算图是一种讨论内存分配优化技术有用的方式。我们已经想你展示了如何通过显示反向图节省内存。现在我们讨论些进一步的优化，看看如何确定基准测试的合理基线。 假设我们想构建 n 层神经网络。一般来说，在我们实现神经网络的时候，我们需要同时为每层的输出和反向传播时的梯度分配空间。这意味着我们需要差不多 2n 的内存。在显示反向图方法中我们面对的是同样的需求因为反向传播时结点数与前向传播差不多。 In-place Operations我们可以使用的一个最简单的技术就是跨操作的原地内存共享。对于神经网络，我们通常将这个技术应用在对应操作的激活函数上。考虑下面的情况，我们想计算三个链式 sigmoid 函数的值： 因为我们可以原地计算 sigmoid，使用同样的内存给输入和输出，我们可以使用固定的内存大小计算任意长度的链式 sigmoid 函数。 注意：在实现原地优化时很容易犯错误。考虑下面的情况，B 的值不仅用于 C，还用于 F。 我们不能使用原地优化因为 B 的值在 C = sigmoid(B) 计算之后仍然需要。如果一个算法简单地对所有 sigmoid 函数都做这个原地优化就会掉进这个陷阱，所以在使用的时候，我们需要注意这个问题。 Standard Memory Sharing除了原地操作还有其他地方可以共享内存。下面的例子中，因为 B 的值在计算 E 之后不再需要，我们可以重新使用 B 的内存来存储 E。 内存共享不需要相同大小的数据。注意再上面的例子中，B 和 E 的 shape 可以不一样。为了处理这样的情况，我们可以分配一个等价于 B 和 E 中大的那个元素的大小，然后让他们共享这个区域。","link":"/blog/2019/04/07/mxnet-optimizing-memory-consumption-in-deep-learning/"},{"title":"pyspark读写HBase","text":"应甲方需求，写一个 pyspark 读写 HBase 的教程。主要包含了基本读写方法和自定义 Converter 的方法。 pyspark 读取 HBase以下内容的环境：python 3.5，spark 1.6 pyspark 读取 HBase 需要借助 Java 的类完成读写。 首先需要明确的是，HBase 中存储的是 byte[]，也就是说，不管是什么样的数据，都需要先转换为 byte[] 后，才能存入 HBase。 基本方法pyspark 读取 HBase 需要使用 SparkContext 的 newAPIHadoopRDD 这个方法，这个方法需要使用 Java 的类，用这些类读取 HBase 下面的示例代码默认 HBase 中的行键、列族名、列名和值都是字符串转成的 byte 数组： read_hbase_pyspark.py1234567891011121314151617181920212223242526272829303132333435363738394041424344# -*- coding:utf-8 -*-import jsonfrom pyspark import SparkContextfrom pyspark import SparkConfif __name__ == \"__main__\": conf = SparkConf().set(\"spark.executorEnv.PYTHONHASHSEED\", \"0\")\\ .set(\"spark.kryoserializer.buffer.max\", \"2040mb\") sc = SparkContext(appName='HBaseInputFormat', conf=conf) # 配置项要包含 zookeeper 的 ip zookeeper_host = 'zkServer' # 还要包含要读取的 HBase 表名 hbase_table_name = 'testTable' conf = {\"hbase.zookeeper.quorum\": zookeeper_host, \"hbase.mapreduce.inputtable\": hbase_table_name} # 这个Java类用来将 HBase 的行键转换为字符串 keyConv = \"org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter\" # 这个Java类用来将 HBase 查询得到的结果，转换为字符串 valueConv = \"org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter\" # 第一个参数是 hadoop 文件的输入类型 # 第二个参数是 HBase rowkey 的类型 # 第三个参数是 HBase 值的类型 # 这三个参数不用改变 # 读取后的 rdd，每个元素是一个键值对，(key, value) hbase_rdd = sc.newAPIHadoopRDD( \"org.apache.hadoop.hbase.mapreduce.TableInputFormat\", \"org.apache.hadoop.hbase.io.ImmutableBytesWritable\", \"org.apache.hadoop.hbase.client.Result\", keyConverter=keyConv, valueConverter=valueConv, conf=conf) # 读取后，将键值对 (key, value) 中的值 value，使用\\n切分，用 flatMap 展开 # 然后将键值对 (key, value) 中的值 value 使用 json.loads 解析，得到 dict hbase_rdd = hbase_rdd.flatMapValues(lambda v: v.split(\"\\n\")).mapValues(json.loads) output = hbase_rdd.collect() for (k, v) in output: print((k, v)) 上述代码在提交给 spark 集群的时候，要指名用到的 Java 类的位置，这些类都在 spark-examples 这个包里面，这个包在 spark 目录下的 lib 里面。以 CDH 5.7.2 为例，CDH 集群中这个包的位置在 /opt/cloudera/parcels/CDH-5.7.2-1.cdh5.7.2.p0.18/lib/spark/lib/spark-examples-1.6.0-cdh5.7.2-hadoop2.6.0-cdh5.7.2.jar，所以提交命令为： 1spark-submit --master yarn --jars /opt/cloudera/parcels/CDH-5.7.2-1.cdh5.7.2.p0.18/lib/spark/lib/spark-examples-1.6.0-cdh5.7.2-hadoop2.6.0-cdh5.7.2.jar read_hbase_pyspark.py 所以，上述的 Java 类，核心都是认为 HBase 中所有的值，原本都是字符串，然后转换成 byte 数组后存入的 HBase，它在解析的时候，将读取到的 byte[] 转换为字符串后返回，所以我们拿到的值就是字符串。 进阶方法对于其他类型的数据，转换为 byte 数组后存入 HBase，如果我们还使用上面的 Java 类去读取 HBase，那么我们拿到的字符串的值就是不正确的。 为了理解这些内容，我们首先要讨论 HBase 中值的存储结构。 HBase 是非结构化数据库，以行为单位，每行拥有一个行键 rowkey，对应的值可以表示为一个 map（python 中的 dict），举个例子，如果我们有一条记录，行键记为 “r1”，里面有 1 个列族(columnFamily) “A”，列族中有两列(qualifier)，分别记为 “a” 和 “b”，对应的值分别为 “v1” 和 “v2”，那么表示成 json 字符串就是下面的形式： 12345678{ \"r1\": { \"A\" : { \"a\": \"v1\", \"b\": \"v2\" } }} 上面这个 json 字符串就是上面那条记录在 HBase 中存储的示例，第一层的键表示行键(rowkey)，对应的值表示这一行的值；第二层的键表示列族名(columnFamily)，值表示这个列族下列的值；第三层的键表示列名(qualifier)，对应的值(value)表示这个由行键、列族名、列名三项确定的一个单元格(Cell)内的值。所以上面这个例子中，只有一行，两个单元格。 下面我们针对 pyspark 读取 HBase 使用到的 org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter 来讨论。 Java 的 API 在读取 HBase 的时候，会得到一个 Result 类型，这个 Result 就是查询结果。Result 可以遍历，里面拥有多个 Cell，也就是单元格。上面我们说了，每个单元格至少有 4 个内容：行键、列族名、列名、值。 HBaseResultToStringConverter 是由 scala 实现的一个类，它的功能是将 Java HBase API 的 Result 转换为 String，源码如下： 123456789101112131415161718192021222324252627package org.apache.spark.examples.pythonconvertersimport scala.collection.JavaConverters._import scala.util.parsing.json.JSONObjectimport org.apache.spark.api.python.Converterimport org.apache.hadoop.hbase.client.{Put, Result}import org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.hbase.KeyValue.Typeimport org.apache.hadoop.hbase.CellUtilclass HBaseResultToStringConverter extends Converter[Any, String] { override def convert(obj: Any): String = { val result = obj.asInstanceOf[Result] val output = result.listCells.asScala.map(cell =&gt; Map( \"row\" -&gt; Bytes.toStringBinary(CellUtil.cloneRow(cell)), \"columnFamily\" -&gt; Bytes.toStringBinary(CellUtil.cloneFamily(cell)), \"qualifier\" -&gt; Bytes.toStringBinary(CellUtil.cloneQualifier(cell)), \"timestamp\" -&gt; cell.getTimestamp.toString, \"type\" -&gt; Type.codeToType(cell.getTypeByte).toString, \"value\" -&gt; Bytes.toStringBinary(CellUtil.cloneValue(cell)) ) ) output.map(JSONObject(_).toString()).mkString(\"\\n\") }} 它完成的工作是遍历 Result 中的 Cell，每个 Cell 转换成一个 scala Map，键分别是行键、列族名、列名、时间戳、HBase 操作类型、值。最后每个 scala Map 被转换成 json 字符串，之间用 ‘\\n’ 分隔。 这里的 CellUtil.CloneRow，CellUtil.cloneFamily，CellUtil.cloneQualifier，CellUtil.cloneValue 是我们主要使用的四个方法，这四个方法生成的都是 byte[]，然后这四个 byte[] 都被 Bytes.toStringBinary 转换成了 String 类型。 所以，如果我们存入 HBase 的数据是 String 以外类型的，如 Float, Double, BigDecimal，那么这里使用 CellUtil 的方法拿到 byte[] 后，需要使用 Bytes 里面的对应方法转换为原来的类型，再转成字符串或其他类型，生成 json 字符串，然后返回，这样我们通过 pyspark 才能拿到正确的值。 下面是一个示例，我们的数据都是 java.math.BigDecimal 类型的值，存 HBase 的时候将他们转换为 byte[] 后进行了存储。那么解析的时候，就需要自定义一个处理 BigDecimal 的类：HBaseResultToBigDecimalToStringConverter 123456789101112131415161718192021222324252627282930package org.apache.spark.examples.pythonconvertersimport java.math.BigDecimalimport scala.collection.JavaConverters._import scala.util.parsing.json.JSONObjectimport org.apache.spark.api.python.Converterimport org.apache.hadoop.hbase.client.{Put, Result}import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.hbase.KeyValue.Typeimport org.apache.hadoop.hbase.CellUtilclass HBaseResultToBigDecimalToStringConverter extends Converter[Any, String] { override def convert(obj: Any): String = { val result = obj.asInstanceOf[Result] val output = result.listCells.asScala.map(cell =&gt; Map( \"row\" -&gt; Bytes.toStringBinary(CellUtil.cloneRow(cell)), \"columnFamily\" -&gt; Bytes.toStringBinary(CellUtil.cloneFamily(cell)), \"qualifier\" -&gt; Bytes.toStringBinary(CellUtil.cloneQualifier(cell)), \"timestamp\" -&gt; cell.getTimestamp.toString, \"type\" -&gt; Type.codeToType(cell.getTypeByte).toString, \"value\" -&gt; Bytes.toBigDecimal(CellUtil.cloneValue(cell)).toString() ) ) output.map(JSONObject(_).toString()).mkString(\"\\n\") }} 上述代码中，引入了 java.math.BigDecimal，将 value 的解析进行了简单的修改，通过 CellUtil.cloneValue 拿到 byte[] 后，通过 Bytes.toBigDecimal 转换成 java.math.BigDecimal，然后使用 toString 方法转换成字符串。 这个类写完后，我们就可以对其进行编译，导出成 jar 包，在 pyspark 程序中指明，读取的时候，使用这个类解析 value。 这样源代码就改完了，需要编译成 jar 包。 首先安装 maven 3.6.0，下载后，解压，配置环境变量即可。 下载 spark 的源码，去 Apache Spark 官网，下载仓库中的源代码 spark-1.6.0.tgz 。 下载后解压，将根目录中的 scalastyle-config.xml 拷贝到 examples 目录下。 修改 examples/src/main/scala/org/apache/spark/examples/pythonconverters/HBaseConverters.scala，增加自己用的类。 修改 examples/pom.xml，将 &lt;artifactId&gt;spark-examples_2.10&lt;/artifactId&gt; 修改为 &lt;artifactId&gt;spark-examples_2.10_my_converters&lt;/artifactId&gt;。 cd 到 examples 目录下，使用以下命令编译 spark-examples 1mvn clean install -pl :spark-examples_2.10_my_converters 编译途中保证全程联网，编译的时候会有一些警告，编译好的包在同级目录下的 target 中，有个叫 spark-examples_2.10_my_converters-1.6.0.jar 的文件。 然后就是使用这个包读取 HBase 中的 BigDecimal了： 我们使用 standalone 模式运行 pyspark 交互式界面： 1pyspark --master spark://host1:7077 --jars spark-examples_2.10_my_converters-1.6.0.jar 执行以下内容： 123456789101112131415161718192021import jsonzookeeper_host = 'host1'hbase_table_name = 'testTable'conf = {\"hbase.zookeeper.quorum\": zookeeper_host, \"hbase.mapreduce.inputtable\": hbase_table_name}keyConv = \"org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter\"# 注意这里，使用自己定义的Converter读取valueConv = \"org.apache.spark.examples.pythonconverters.HBaseResultToBigDecimalToStringConverter\"hbase_rdd = sc.newAPIHadoopRDD( \"org.apache.hadoop.hbase.mapreduce.TableInputFormat\", \"org.apache.hadoop.hbase.io.ImmutableBytesWritable\", \"org.apache.hadoop.hbase.client.Result\", keyConverter=keyConv, valueConverter=valueConv, conf=conf)hbase_rdd = hbase_rdd.flatMapValues(lambda v: v.split(\"\\n\")).mapValues(json.loads)hbase_rdd.take(1) 然后就可以看到结果了，如何验证读取的对不对呢，可以尝试将 valueConv 改回 HBaseResultToStringConverter，然后观察 value 的值。 以上就是如何通过修改 HBaseConverters.scala 让 pyspark 从 HBase 中读取 java.math.BigDecimal 的示例。 pyspark 写入 HBasepyspark 写入 HBase 使用 SparkContext 的 saveAsNewAPIHadoopDataset，和读取的方法类似，也需要使用 Java 的类。 下面的方法要求存入 HBase 中的数据，行键、列族名、列名、值都为字符串 write_into_hbase_pyspark.py123456789101112131415161718192021222324252627282930313233343536# -*- coding:utf-8 -*-from pyspark import SparkContextfrom pyspark import SparkConfif __name__ == \"__main__\": conf = SparkConf().set(\"spark.executorEnv.PYTHONHASHSEED\", \"0\")\\ .set(\"spark.kryoserializer.buffer.max\", \"2040mb\") sc = SparkContext(appName='HBaseOutputFormat', conf=conf) # 配置项要包含 zookeeper 的 ip zookeeper_host = 'zkServer' # 还要包含要写入的 HBase 表名 hbase_table_name = 'testTable' conf = {\"hbase.zookeeper.quorum\": zookeeper_host, \"hbase.mapred.outputtable\": hbase_table_name, \"mapreduce.outputformat.class\": \"org.apache.hadoop.hbase.mapreduce.TableOutputFormat\", \"mapreduce.job.output.key.class\": \"org.apache.hadoop.hbase.io.ImmutableBytesWritable\", \"mapreduce.job.output.value.class\": \"org.apache.hadoop.io.Writable\"} keyConv = \"org.apache.spark.examples.pythonconverters.StringToImmutableBytesWritableConverter\" valueConv = \"org.apache.spark.examples.pythonconverters.StringListToPutConverter\" records = [ ['row1', 'f1', 'q1', 'value1'], ['row2', 'f1', 'q1', 'value2'], ['row3', 'f1', 'q1', 'value3'], ['row4', 'f1', 'q1', 'value4'] ] sc.parallelize(records)\\ .map(lambda x: (x[0], x))\\ .saveAsNewAPIHadoopDataset( conf=conf, keyConverter=keyConv, valueConverter=valueConv) 首先在控制台启动 HBase-shell 1hbase shell 然后创建表，表名为 testTable，只有一个列族，列族名为 f1： 1create 'testTable', 'f1' 使用 quit 退出 HBase-shell 提交 pyspark 程序： 1spark-submit --master spark://master:7077 --jars /opt/cloudera/parcels/CDH-5.7.2-1.cdh5.7.2.p0.18/lib/spark/lib/spark-examples-1.6.0-cdh5.7.2-hadoop2.6.0-cdh5.7.2.jar write_into_hbase_pyspark.py 运行完成后，再次进入 HBase-shell，运行： 1scan 'testTable' 可以看到类似下面的输出结果： 1234567hbase(main):001:0&gt; scan &apos;testTable&apos;ROW COLUMN+CELL row1 column=f1:q1, timestamp=1554892784494, value=value1 row2 column=f1:q1, timestamp=1554892784494, value=value2 row3 column=f1:q1, timestamp=1554892816961, value=value3 row4 column=f1:q1, timestamp=1554892816961, value=value44 row(s) in 0.3330 seconds 这就完成了写入 HBase 的过程。 需要注意的是：rdd 中的每个元素，都必须是一个列表(list)，不能是其他类型，如 tuple，而且每个列表内必须是 4 个元素，分别表示 [行键、列族名、列名、值]，且每个元素都为 str 类型。 原因是 StringListToPutConverter 这个类做转换的时候需要将 rdd 中的元素，看作是一个 java.util.ArrayList[String] 1234567class StringListToPutConverter extends Converter[Any, Put] { override def convert(obj: Any): Put = { val output = obj.asInstanceOf[java.util.ArrayList[String]].asScala.map(Bytes.toBytes).toArray val put = new Put(output(0)) put.add(output(1), output(2), output(3)) }} StringListToPutConverter 的工作原理是，将传入的元素强制类型转换为 java.util.ArrayList[String]，将第一个元素作为行键、第二个元素作为列族名、第三个元素作为列名、第四个元素作为值，四个值都转换为 byte[] 后上传至 HBase。 所以我们可以修改这个类，实现存入类型的多样化。 举个例子，如果我想存入一个 java.math.BigDecimal，那实现的方法就是：在 pyspark 程序中，将数字转换成 str 类型，调用我们自己写的一个 converter： 12345678910111213import java.math.BigDecimalclass StringListToBigDecimalToPutConverter extends Converter[Any, Put] { override def convert(obj: Any): Put = { val output = obj.asInstanceOf[java.util.ArrayList[String]].asScala.toArray val put = new Put(Bytes.toBytes(output(0))) put.add( Bytes.toBytes(output(1)), Bytes.toBytes(output(2)), Bytes.toBytes(new BigDecimal(output(3))) ) }} 就可以实现存入的值是 java.math.BigDecimal 了。 CDH 5.9 以前的版本，python3，master 选定为 yarn 时的 bugCDH 5.9 以前的版本在使用 yarn 作为 spark master 的时候，如果使用 python3，会出现 yarn 內部 topology.py 这个文件引发的 bug。这个文件是 python2 的语法，我们使用 python3 运行任务的时候，python3 的解释器在处理这个文件时会出错。 解决方案是：将这个文件重写为 python3 的版本，每次在重启 yarn 之后，将这个文件复制到所有机器的 /etc/hadoop/conf.cloudera.yarn/目录下。 以下是 python3 版本的 topology.py。 topology.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#!/usr/bin/env python## Copyright (c) 2010-2012 Cloudera, Inc. All rights reserved.#'''This script is provided by CMF for hadoop to determine network/rack topology.It is automatically generated and could be replaced at any time. Any changesmade to it will be lost when this happens.'''import osimport sysimport xml.dom.minidomdef main(): MAP_FILE = '{{CMF_CONF_DIR}}/topology.map' DEFAULT_RACK = '/default' if 'CMF_CONF_DIR' in MAP_FILE: # variable was not substituted. Use this file's dir MAP_FILE = os.path.join(os.path.dirname(__file__), \"topology.map\") # We try to keep the default rack to have the same # number of elements as the other hosts available. # There are bugs in some versions of Hadoop which # make the system error out. max_elements = 1 map = dict() try: mapFile = open(MAP_FILE, 'r') dom = xml.dom.minidom.parse(mapFile) for node in dom.getElementsByTagName(\"node\"): rack = node.getAttribute(\"rack\") max_elements = max(max_elements, rack.count(\"/\")) map[node.getAttribute(\"name\")] = node.getAttribute(\"rack\") except: default_rack = \"\".join([ DEFAULT_RACK for _ in range(max_elements)]) print(default_rack) return -1 default_rack = \"\".join([ DEFAULT_RACK for _ in range(max_elements)]) if len(sys.argv)==1: print(default_rack) else: print(\" \".join([map.get(i, default_rack) for i in sys.argv[1:]])) return 0if __name__ == \"__main__\": sys.exit(main())","link":"/blog/2019/04/10/pyspark读写hbase/"},{"title":"Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time","text":"WWW 2018. 对随机游走进行了改进，提出了Pixie随机游走，实际上就是一个有偏的随机游走，根据相似度进行偏离，从而实现个性化推荐，而且使用了早停策略。原文链接：Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time Abstract现代内容挖掘应用中，用户体验依赖于高质量的个性化推荐。但是构建一个能提供这种推荐的系统由于商品和用户的数量太大，以及推荐结果需要对用户的实时操作进行响应等问题变得比较困难。我们提出了Pixie，一个基于图的，可扩展的实时推荐系统，部署在了Pinterest上。给定一组用户指定的pins作为查询，Pixie会实时地从十亿个可能的pins中选择那些与查询的query最相关的pins。我们使用Pixie随机游走算法，利用Pinterest图中的30亿个顶点以及170亿条边来生成推荐。实验表明，我们的算法对比之前基于hadoop的推荐系统，提升了50%以上的user engagement。此外，我们使用一种图剪枝策略使得推荐性能提升了58%。最后，我们在系统层面上讨论了Pixie，单台服务器每秒在延时60毫秒的情况下可以处理1200个推荐请求。现在，这个系统为Pinterest提供了超过80%的贡献。 Related Work推荐系统是一个大且经充分研究的研究领域。我们总结了以下相关工作，聚焦于大规模的工业界推荐系统。 Web-scale recommender systems. 一些基于web的生产系统刚才已经提到了[1, 7, 21]。不像Pixie，他们不是实时的，他们的推荐结果都是提前计算出来的。实际中，响应时间小于100毫秒的就可以算作是实时，因为这样的系统可以和实时的服务流相结合。举个例子，如果我们推荐一个结果需要1秒钟，用户就得等太长的时间了。这些系统的推荐结果都是提前计算出来的，比如提前一天，然后存储成key-value对。但老旧的推荐系统都不怎么好。实时推荐系统因为它能对用户的反应实时的反应，所以变得很重要。对用户行为的实时反应可以提供更好的推荐结果。我们的实验表明实时推荐系统表现的比需要几天或几个小时才能更新的推荐系统好30-50%。 其他的实时推荐系统包括新闻推荐[9, 26]。然而，这样的系统只推荐最新的内容。我们这里的主要差别是规模，Pinterest的目录比传统的推荐系统多了1000倍。 Random-walk-based approaches. 很多算法使用随机游走利用图结构进行推荐[2, 3, 28]。或许，我们的工作和Twiiter的”who to follow”系统最相近[14, 15]，它们的系统将整个图放到一台机器中，运行一个个性化的SALSA算法[19]。这些蒙特卡洛方法考虑了一个顶点相对于其他顶点的重要性，推荐结果会根据这些分数生成[13]。我们开发了一种新的随机游走方法，更快，效果更好。 Traditional collaborative filtering approaches. 更一般地来讲，我们的方法与协同过滤相关，协同过滤通过挖掘用户图和商品图之间的交集部分，匹配用户与相似的商品偏好来生成推荐结果。系统过滤依赖于用户-商品矩阵的矩阵分解，生成表示用户和商品的隐向量[16, 17, 25, 30]。然而，基于矩阵分解的协同过滤算法的时间与空间复杂度最低是用户-商品图中顶点数的线性关系，使得使用这些算法在数十亿的商品以及百亿用户的问题上变得很有挑战。相比之下，我们的随机游走推荐算法与数据集的大小无关。 Content-based methods. 在纯基于内容的推荐系统中，对商品的表示只依赖于它们的内容特征[24]。许多先进的大规模推荐系统是基于内容的，经常使用深度神经网络[6, 8, 11, 29]。尽管这些算法凭借着参数空间的维度只依赖于特征空间的维度，能扩展到大的数据集上，这些方法并不能利用图结构的信息，而这点（图结构信息）对Pinterest很重要。 3. Proposed MethodPinterest是一个用户与pins交互的平台。用户可以保存相关的pins到board中。这些board是相似pins的集合体。举个例子，一个用户可以创建一个关于食谱的board，将食物相关的pins放进去。Pinterest可以看作是一些boards，每个board是一组pins，每个pin又组成了成千上万不同的boards。形式化来说，Pinterest可以组成一个无向二分图 $G = (P, B, E)^2$ 。其中，$P$ 表示一组pins，$B$ 表示boards的集合。集合 $P \\cup B$是 $G$ 的顶点集。如果用户 $p$ 保存了 $b$，那么在pin $p \\in P$ 与 board $b \\in B$ 之间就有一条边 $e \\in E$。我们使用 $E(p)$ 表示连接到pin $p$ 的board顶点，$E(b)$ 表示连接到 $b$ 的pins。我们认为图 $G$ 是连通的，在实际中也是这样的。 Pixie接受的输入是一组带权的pins $Q = \\lbrace (q, w_q) \\rbrace$，其中 $q$ 是查询的pin，$w_q$是在查询集合中的重要度。查询集合 $Q$ 是用户指定的，并且在每次用户的行为后动态生成的，最近操作的pins有高的权重，随着时间的增长，权重会降低。给定查询 $Q$ Pixie会通过模拟新式的有重启的有偏随机游走来生成推荐结果。 3.1 Pixie Random Walk为了更好的解释Pixie，我们首先解释最简单的随机游走，然后讨论如何将它扩展到Pixes使用的新的随机游走算法上。所有基于基本随机游走的创新对于Pixie提升性能来说都是至关重要的。 Basic Random Walk. 考虑一个简单的例子，用户指定了查询 $Q$，包含一个pin $q$。给定一个输入pin $q$，可以在 $G$ 上模拟出很多短的随机游走，都是从 $q$ 开始的，记录每个pin $p$ 的访问次数(visit count)，表示随机游走访问到 $p$ 的次数。如果一个pin被访问的次数越多，那么这个 $q$ 就和它越相关。 我们在算法1中描述了最基本的随机游走 $\\rm BASICRANDOMWALK$ [28]。每个随机游走生成了一个 steps 的序列。每个 step 由3个操作组成。首先，给定当前pin $p$，(初始为 $q$ )，我们从 $E(p)$ 中选择一条连接 $q$ 和 $b$ 的边 $e$。然后我们通过从 $E(b)$ 中采样一条连接 $b$ 和 $p’$ 的一条边，得到pin $pin’$。第三步，当前的pin更新到 $p’$，然后重复之前的步骤。 游走的长度由参数 $\\alpha$ 决定。所有的 这样的随机游走的 steps 的数量决定了这个步骤的时间复杂度，我们用 $N$ 表示这个和。我们维护一个 counter $V$ 用来映射 pin 和访问次数。为了获得推荐的pins，我们可以从返回的 counter 提取访问次数最高的pins，将它们返回作为推荐结果。这个过程的时间复杂度是固定的，与图的大小无关。 Pixie随机游走算法由算法2和算法3组成，在基础随机游走上有以下提升： 对用户指定的pin的有偏随机游走 多个查询的pin，每个都有不同的权重 multi-hit booster对多个查询pins的增强 在保持预测性能的情况下使用早停减少随机游走的步数 (1)Biasing the Pixie Random Walk. 针对用户对随机游走进行偏离很重要。对于同样的查询集合 $Q$，推荐结果对于不同的用户应该不同。举个例子，Pinterest图包含了不同语言、不同主题，以及不同兴趣的用户的pins和boards，给用户使用的语言以及他们兴趣相关的推荐是非常重要的。 我们通过使用基于用户特征的随机的边选择方法解决了有偏随机游走的问题。随机游走倾向于遍历和用户相关的边。可以看作这些边对于图中的其他边有更高的权重/重要性。我们将随机游走在图的一个特定区域内进行偏离，使得它可以专注于pins的一个子集。这个修改在提高个性化、质量、以及推荐内容的话题性上的提升很重要，得到了更高的用户满意度。 Pixie算法使用一组用户特征 $U$ 作为特征（算法2）。注意，不同的Pixie调用不同的用户和查询，我们可以使用动态基于用户和边的特征的有偏边选择方法，这种方法可以提高Pixie推荐结果的灵活性。特别地， $\\rm PIXIERANDOMWALK$ 使用 $\\rm PersonalizedNeighbor(E, U)$ 选择对用户 $U$ 重要的边。这能使边匹配用户的特征/偏好，比如语言或话题。从概念上来看，这使得我们在针对用户进行游走偏离时可以获得最小的存储以及计算开销。本质上来看，可以将这种方法看作是针对每个用户，使用一个不同的图，这个图的边权重是针对这个用户定制的（但是不需要存储起来）。在实际情况中，由于性能原因，我们将权重限制在了一个可能值组成的离散集合内。我们避免了在内存中存储相似语言和话题的的边导致的开销，因此 $\\rm PersonalizedNeighbor(E, U)$ 是一个子范围操作器。 (2) Multiple Query Pins with Weights. 为了全面地对用户建模，基于给定用户的全部历史信息是很重要的。我们通过基于多个pins，而不是一个pin的查询方式完成了这个要求。查询集合 $Q$ 中的每个pin $q$ 被分配了一个不同的权重 $w_q$。权重基于用户对这个pin进行操作之后的时间，以及这个操作的类型。我们使用以下方法生成一组查询 $Q$ 的推荐结果。我们对 $q \\in Q$ 使用Pixie Random Walk（算法2），使用相互独立的计数器，如 pin $p$ 的计数器 $V_q[p]$ 对每个查询 pin $q$ 进行记录。最后，通过使用一会儿要提到的新的公式融合访问次数。 这里有个重要的见解，获取有意义的访问次数需要的steps的数量依赖于pin的度。从一个高度的pin，也就是出现在很多boards中的pin的推荐结果需要的步数要远多于从一个低度的pin。因此，我们给每个pin分配的步数正比于它的度。但是，如果我们线性的分配步数给每个pin，那最后有可能给低度的pin连一步都分配不了。 我们基于一个根据度数亚线性增长的函数来分配步数，通过一个缩放因子 $s_q$ 给每个pin的权重 $w_q$进行缩放。我们给每个 pin 构建如下的缩放因子： $$\\tag{1}s_q = \\vert E(q) \\vert \\cdot (C - \\log{\\vert E(q) \\vert})$$ 其中 $s_q$ 是 pin $q \\in Q$ 的缩放因子，$\\vert E(q) \\vert$ 是 $q$ 的度，$C = max_{p \\in P}(E(p))$ 是 pin 度的最大值。这个函数不会不成比例地给流行的pins高的权重。我们通过下面的公式分配步数： $$\\tag{2}N_q = w_q N \\frac{s_q}{\\sum_{r \\in Q} s_r}$$ 其中，$N_q$是分配给从pin $q$ 起始的随机游走的总步数。这个分布有我们想要的性质：给高度数的pins更多的步数，给低度数的pin充分的步数。我们在算法3的第二行实现了这个。 (3) Multi-hit Booster. Pixie算法的另一个创新是，通常来说对于查询集合 $Q$，我们想要的推荐结果是与 $Q$ 中的多个 pins 相关的结果。直观上来看，候选的查询与越多的 pins 相关，那么这个候选项与整个查询就越相关。换句话说，从多个 pins 得来的具有高访问次数的候选项比从单一 pin 得来的具有很高的访问次数的候选项对于这个查询更相关。 这里我们的见解是：让Pixie增强从多个查询 pins 得到的候选 pins 的分数。我们通过一种新的方法聚合一个给定的 pin $p$ 的访问次数 $V_q[p]$。而不是简单的对所有的查询 pins $q \\in Q$，加和给定 pin $p$ 的访问次数 $V_q[p]$，我们将他们变换，这种方式会奖励那些从多个不同的查询 pins $q$ 多次访问的 pins： $$\\tag{3}V[p] = ( \\sum_{q \\in Q} \\sqrt{V_q[p]})^2$$ 其中，$V[p]$ 是 pin $p$合并后的访问次数。注意，当一个候选 pin $p$通过游走从一个单独的查询 pin $q$ 起始后访问到时，访问次数是不变的。但是，如果候选 pin 从多个查询 pins 访问到，那么这个值就会增强。就会导致，选择计数器 $V$ 得到的最高的访问 pins 时，multi-hit pins 的比例会更高。我们在算法3的第5行实现了这个。 (4) Early Stopping. 我们刚才描述的步骤会跑一个固定 steps 个数 $N_q$ 的随机游走。然而，因为 Pixie 的运行时间依赖于 steps 的个数，我们希望跑尽可能少的步数。这里我们可以通过对查询 $q$ 调整步数 $N_q$，而不是对所有的查询使用固定的 $N_q$ 来减少运行时间。 我们的解决方案是一旦前几个候选项稳定后就停止游走。因为 Pixie 推荐几千个pins，如果朴素地实现，这个监控（判断候选项变不变）会比随机游走本身还费时。我们的方法通过两个实数 $n_p$ 和 $n_v$ 克服了这个困难。至少 $n_p$ 个候选项被访问了至少 $n_v$ 次就停止游走。这个监控简单而且高效，因为我们只需要一个计数器来追踪候选项是否被访问了 $n_v$ 即可（算法2的12到15行）。 我们在4.2节展示了早停策略与长的随机游走差不多的结果，但是差不多少了一般的步数，加速了一倍左右。 3.2 Graph Pruning另一个重要的创新是图的清理与剪枝。图剪枝提升了推荐质量，也减少了图的尺寸，所以这个算法可以在更小的图、更便宜的机器上，提供更好的缓存表现。 原始的Pinterest图有70亿个顶点和超过1000亿条边。但是，不是所有的 boards 都是有主题的。大的多样的boards会让游走向多个方向扩散，使得推荐结果质量下降。相似地，很多 pins 被错误的分到其他的 boards 中。图剪枝过程会清理图，并且让它变得主题更专一。另一个好处是，图剪枝也会让图变小，可以放到单台机器的内存中。不需要将图分布到多台机器上就可以得到很好的性能，因为随机游走不需要在不同的机器上“跳跃”。 步骤如下：首先，通过计算 boards 的主题分布的熵，量化每个 board 的内容多样性。然后在每个 pin 的描述上使用LDA主题模型获取概率主题向量。使用加入到 board 中最新的 pins 的主题向量作为输入信号，计算 board 的熵。删除掉有很大的熵的 boards 以及他们的边。 另一个挑战是，真实环境的数据集有很偏的长尾分布。在 Pinterest中，意味着一些 pins 非常流行，有可能被存入了几百万个 boards 中。对于这样的顶点，随机游走需要运行很多步，因为它需要在一个有大量邻居的网络中扩散。我们通过有系统性地丢弃高度 pins 的边解决这个问题。我们丢弃了那种 pin 的主题不属于那个 board 的边，而不是随机地丢弃边。我们使用和上面同样的主题向量，使用特征向量的余弦相似度计算一个 pin 对于一个 board 的相似度，然后只保留具有高相似度的边。实际上，剪枝的程度取决于 $pruning \\ factor \\ \\delta$。我们将每个 pin $p$ 的度更新为 $\\vert E(p) \\vert ^ \\delta$，丢弃连接 $p$ 和 与 $p$ 相似度低的 boards 的边。 在剪枝后，图包含了10亿个 boards，20亿个 pins，170亿条边。比较有意思的是，我们发现剪枝有两个优点：1. 图的大小减小了6倍（内存开销）；2. 得到了58%的相关推荐结果。","link":"/blog/2018/09/21/pixie-a-system-for-recommending-3-billion-items-to-200-million-users-in-real-time/"},{"title":"Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks","text":"NIPS 2015. 在训练seq2seq的时候，比如像机器翻译，训练的时候，每个输出y，它所依据的前一个词，都是正确的。但是在预测的时候，输出的这个词依照的上一个词，是模型输出的词，无法保证是正确的，这就会造成模型的输入和预测的分布不一致，可能会造成错误的累积。本文提出了scheduled sampling来处理这个问题。原文链接：Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks Abstract循环神经网络可以训练成给定一些输入，输出一些token的模型，比如最近的机器翻译和图像描述。现在训练这些的方法包括给定当前状态和之前的token，最大化序列中每个token的似然。在推理阶段，未知的token会被模型生成的token替代。这种在训练和推断之间的差异，会在生成序列时产生快速积累的误差。我们提出了一个递进学习策略(curriculum learning strategy)轻微地将训练过程进行一些改变，从之前的完全使用一个真实的token进行引导的策略，变成了基本使用生成的token来引导的策略。在几个序列预测的实验上表明我们的方法有很大的提升。此外，它成功的在MSCOCO图像描述2015任务上获得冠军。 1 Introduction循环神经网络可以用于处理序列，要么输入，要么输出，或者都可以。尽管他们很难在长期依赖的数据上训练，一些如LSTM的版本可以更好的适应这种问题。事实是，最近在一些序列预测问题上，包括机器翻译，contextual parsing，图像描述甚至视频描述上，这些模型表现的很好。 在这篇论文中，我们考虑生成可变大小的token的序列的问题，比如机器翻译，目标是给定源语言翻译成目标语言。我们也考虑当输入不是一个序列的问题，如图像描述问题，目标是对给定的图像生成一个文本描述。 在这两种情况，循环神经网络一般都是通过给定的输入，最大化生成目标序列的似然。实际上，这个是通过给定当前模型的状态和之前的目标token，最大化每个目标token的似然，这使得模型可以在目标token上学习一种语言模型。然而，在推断过程中，真的previous目标token是无法获取的，这就使得模型需要用它自己生成的token，导致模型在训练和预测时会有差异。通过使用beam search启发式的生成几个目标序列可以缓解这种差异，但是对于连续的状态空间模型，如RNN，不存在动态规划的方法，所以即便是使用beam search，考虑的序列的数量仍然会很小。 主要问题是，在生成序列时越早出现错误，会导致将这个错误输入进模型，然后会扩大模型的误差，因为模型会将它在训练时未见过的错误考虑到状态空间内。 我们提出了一个递进学习方法，在对序列预测任务上使用RNN的训练和推测时构建了桥梁。我们提出，改变训练过程，为了逐渐地使模型处理它的错误，使它在推断时也可以进行。这样，模型在训练时会探索更多的情况，因此在推断时会更鲁棒的纠正它的错误，因为它在训练时就学习过这个。我们会展示这个方法在几个序列预测问题上的结果。 2 Proposed Approach我们考虑一个监督学习任务，训练集是给定的 $N$ 个样本的输入输出对，$\\lbrace X^i, Y^i \\rbrace^N_{i=1}$，$X^i$ 是输入，要么静态（图像），要么动态（序列），输出 $Y^i$ 是一个可变数量的token的序列 $y^i_1, y^i_2, …, y^i_{T_i}$，token属于一个已知的词典。 2.1 Model给定一个输入/输出对儿 $(X, Y)$，log 概率 $P(Y \\mid X)$ 可由下式计算： $$\\tag{1}\\begin{aligned}\\mathrm{log} P(Y \\mid X) &amp;= \\mathrm{log} P(y^T_1 \\mid X) \\\\&amp;= \\sum^T_{t = 1} \\mathrm{log} P(y_t \\mid y^{t-1}_1, X)\\end{aligned}$$ 其中，$Y$ 是长度为 $T$ 的序列，$y_1, y_2, …, y_T$。在前面的等式中，后面的项通过一个参数为 $\\theta$ 的循环神经网络，通过一个状态向量 $h_t$估计得到，也就是通过前一个输出 $y_{t-1}$ 和前一个状态 $h_{t-1}$估计得到： $$\\tag{2}\\mathrm{log} P(y_t \\mid y^{t-1}_1, X; \\theta) = \\mathrm{log} P(y_t \\mid h_t; \\theta)$$ 其中，$h_t$ 通过如下的一个循环神经网络计算得到： $$\\tag{3}h_t = \\begin{cases}f(X; \\theta) \\ \\ \\mathrm{if} t = 1\\\\f(h_{t-1}, y_{t-1}; \\theta) \\mathrm{otherwise}.\\end{cases}$$ $P(y_t \\mid h_t; \\theta)$ 经常通过状态向量 $h_t$ 的一个线性变换，变换到一个 vector of scores 实现，这个向量是输出字典的每个token的分数，然后用一个 softmax 确保分数适当的归一化。$f(h, y)$通常是一个非线性函数，这个函数融合了之前的状态和之前的输出来生成当前的状态。 这就意味着模型专注于给定模型当前状态，学习预测下一个输出。因此，模型会以最普通的形式表示序列的概率分布——不像条件随机场以及其他的模型，在给定隐变量状态后，假设不同时间步的输出相互独立。模型的容量只会被循环层和前向传播层的表示容量限制。LSTM，因为他们能学习长范围的结构，所以对这种问题来说非常适合，也就可以学习序列上的rich distributions。 为了学习边长序列，一个特殊的token，，表示序列的结束被添加进字典和模型中。在训练的过程中，会拼接在每个序列的结尾处。在推理的时候，模型会生成tokens直到它生成了。 2.2 Training训练循环神经网络来解决这样的问题通常通过mini-batch随机梯度下降求解，通过给定输入数据 $X^i$，为所有的训练对儿 $(X^i, Y^i)$最大化生成正确的目标序列 $Y^i$ 的似然，找到一组参数$\\theta^*$。 $$\\tag{4}\\theta^* = \\mathop{\\arg \\max_\\theta} \\sum_{(X^i, Y^i)} \\mathrm{log} P(Y^i \\mid X^i; \\theta)$$ 2.3 Inference在推理的过程中，模型可以在给定 $X$ 的情况下通过一次生成一个token，生成整个序列 $y^T_1$。生成一个 后，它标志着序列的结束。对于这个过程，在时间 $t$，模型为了生成 $y_t$，需要从最后一个时间戳讲输出的token $y_{t-1}$ 作为输入。因为我们没法知道真正的上一个token是什么，我们可以要么选择模型给出的最可能的那个，要么根据这个来抽样。 给定 $X$ 搜索最大概率的序列 $Y$ 非常费时，因为序列的长度是组合地上升的。我们使用一个beam search来生成 $k$ 个最好的序列。我们通过维护 $m$ 个最优候选序列组成的一个堆来做这个。每次通过给每个候选序列扩充一个token并把它增加到堆内，都能得到一个新的候选序列。在这步的结尾，堆重新地剪枝到只有 $m$ 个候选序列。beam searching在没有新的序列增加的时候就会截断，然后返回 $k$ 个最优的序列。 尽管beam search通常用于基于HMM这样的模型的离散状态，这些模型可以使用动态规划，但是对于像RNN这样的连续状态模型就很难了，因为没有办法再连续空间内factor the followed state paths，因此在beam search解码的时候，可以控制的候选序列的实际数量是很小的。 在所有的情况里，如果在时间 $t-1$ 有一个错误生成，那么模型就会在一个和训练分布不同的状态空间中，而且它会在这个空间中不知所措。更糟的是，这会导致模型在决策的时候导致不好的决策的累计——a classic problem in sequential Gibbs sampling type appraoches to sampling, where future samples can have no influence on the past. 2.4 Bridging the Gap with Scheduled Sampling在预测token $y_t$ 时，训练和推断的主要差别是我们是否使用前一个真实的token $y_{t-1}$，还是使用一个从模型得到的估计值 $\\hat{y}_{t-1}$。 我们在这里提出了一个采样机制，会在训练的时候，随机地选择 $y_{t-1}$ 或 $\\hat{y}_{t-1}$。假设我们使用mini-batch随机梯度下降，对于训练算法的第 $i$ 个mini-batch中预测 $y_t \\in Y$ 的每个token，我们提出用抛硬币的方法，设使用真实token的概率为$\\epsilon_i$，使用它估计的token的概率为$(1 - \\epsilon_i)$。模型的估计值可以根据模型的概率分布$P(y_{t-1} \\mid h_{t-1})$ 来采样获得，或是取 $\\mathop{\\arg \\max_s} P(y_{t-1} = s \\mid h_{t-1})$。这个过程由图1所示。 当 $\\epsilon_i = 1$ 时，模型就像之前一样训练，但是当 $\\epsilon_i = 0$ 时，模型就会和推断时一样训练。我们这里提出了一个递进学习策略，在训练的开始接断，从模型可能生成的token中进行采样，因为此时模型还没有训练好，这可能会使模型的收敛速度变慢，所以这里选择较多的真实token会帮助训练；另一方面，在训练快结束的时候，$\\epsilon_i$ 应该更倾向于从模型的生成结果中采样，因为这个对应了推测的场景，这时我们会期望模型已经有足够好的能力来处理这个问题，并且采样出有效的tokens。 因此我们提出使用一个规则来减少 $\\epsilon_i$ 来作为 $i$ 的函数，就像现在很多随机梯度下降算法那样降低学习率一样。这样的规则如图2所示： · Linear decay: $\\epsilon_i = \\mathrm{max} (\\epsilon, k - ci)$，其中 $0 \\leq \\epsilon &lt; 1$ 是给模型的真值最小的数量，$k$ 和 $c$ 提供了衰减的截距和斜率，这些依赖于收敛速度。 · Exponential decay: $\\epsilon_i = k^i$，其中 $k &lt; 1$ 是一个依赖于期望收敛速度的常量。 · Inverse sigmoid decay: $\\epsilon_i = k/(k + \\mathrm{exp}(i/k))$，其中 $k \\geq 1$ 依赖于期望收敛速度。 我们的方法命名为 Scheduled Sampling。需要注意的是在模型训练时从它的输出采样到前一个token $\\hat{y}_{t-1}$时，我们可以在时间 $t \\rightarrow T$ 内进行梯度的反向传播。在实验中我们没有尝试，会在未来的工作中尝试。","link":"/blog/2018/08/11/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/"},{"title":"Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction","text":"AAAI 2019。网格流量预测，两个问题：空间依赖动态性，另一个是周期平移。原文链接：Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction Abstract由于大规模的交通数据越来越多，而且交通预测在实际中很重要，交通预测在 AI 领域引起越来越多的关注。举个例子，一个精确的出租车需求预测可以协助出租车公司预分配出租车。交通预测的关键在于如何对复杂的空间依赖和时间动态性建模。尽管两个因素在建模的时候都会考虑，当前的方法仍会做很强的假设，即空间依赖在时间上是平稳的，时间依赖是严格周期的。然而，实际中的空间依赖可能是动态的（即随时间的变化而变化），而且时间动态性可能从一个时段到另一个时段有波动。在这篇文章中，我们有两个重要发现：（1）区域间的空间依赖是动态的；（2）时间依赖虽说有天和周的模式，但因为有动态时间平移，它不是严格周期的。为了解决这两个问题，我们提出了一个新的时空动态网络（STDN），我们用一个门控机制学习区域间的动态相似性，用一个周期性平移的注意力机制来处理长期周期时间平移现象。据我们所知，这是第一个在一个统一的框架中解决这两个问题的工作。我们的实验结果证明了提出的方法是有效的。 Introduction交通预测是一个时空预测问题。精确的预测模型对很多应用都很重要。在传统交通预测问题中，给定历史数据（比如一个区域的流量，或一个交叉卡前几个月每小时的流量），预测未来一段时间的数据。这方面的研究已经有几十年了。在时间序列社区，ARIMA 和 Kalman filtering被广泛地应用到这一领域。然而这些早期方法是针对每个区域分别预测，最近的方法考虑了空间信息（比如针对近邻区域增加正则项）和外部因素（如地点信息，天气状况，地区活动）。然而，这些方法仍基于机器学习中的传统时间序列模型，不能很好的捕获复杂的非线性时空依赖）。 最近，深度学习方法在很多任务上取得了成功。比如，一些研究将城市交通看作是热力图的图片，使用 CNN 对非线性空间关系建模。为了对非线性时间关系建模，人们提出了基于 RNN 的框架。Yao et al。 更是提出了用 CNN 和 LSTM 同时处理时间和空间依赖的框架。 尽管考虑了同时对时空建模，现存的方法主要有两点不足。首先，区域间的空间依赖依赖于历史数据的相似性，模型学习到了一个静态的空间依赖。然而，区域间的依赖随时间是改变的。举个例子，早上，居民区和商业区之间的依赖关系强；深夜，关系就弱了。然而，这样的动态依赖在之前的研究中没有考虑。 另一个限制是现存的研究忽略了长期周期依赖。交通数据又很强的日和周周期性，基于这种周期性的依赖关系可能用于预测。然而，一个挑战是交通数据不是严格周期的。举个例子，周末的高峰通常发生在下午的后半段，不同的日子时间不一致，从4:30pm到6:00pm变化。尽管之前的研究考虑了周期性，他们没能考虑序列性的依赖和周期性中的时间平移。 为了解决前面提出的问题，我们提出了新的深度学习框架，时空动态网络用于交通预测。STDN 是基于时空神经网络的，使用局部 CNN 和 LSTM 分别处理时空信息。一个门控局部 CNN 使用区域间的动态相似性对空间依赖建模。一个周期平移的注意力机制用来学习长期周期依赖。通过注意力机制对长期周期信息和时间平移建模。我们的方法还用 LSTM 以层次的方式处理序列依赖。 我们再大型的真实数据集上做了评测，纽约出租车数据和纽约的共享单车数据。和 state-of-the-art 的全面对比展示了我们模型的性能。我们的贡献如下： 我们提出了一个流式门控机制对动态空间相似性建模。门控制信息在邻近区域传播。 我们提出了一个周期平移注意力机制，通过同时时可用长期周期信息和时间平移。 我们在几个真实数据集上开展了实验，效果好。 Notations and Problem Formulation我们将整个城市分为 $a \\times b$ 个网格，一共 $n$ 个区域（$n = a \\times b$），使用 $\\lbrace 1,2,\\dots,n \\rbrace$ 表示他们。我们将整个时间周期分为 $m$ 个登场的连续时段。任何一个个体的移动，其本质上是整个城市交通的一部分，总是从一个区域出发，过一段时间到达目的地。我们在一个时段内给每个区域定义一个开始/结束流量作为区域出发/到达的移动发生次数。$y^s_{i,t}$ 和 $y^e_{i,t}$ 表示开区域 $i$ 在时段 $t$ 的开始/结束流量。此外，对个体旅行的聚合形成交通流，描述了区域对之间的考虑时间的移动。时段 $t$ 从区域 $i$ 开始的交通流在时段 $\\tau$ 于区域 $j$ 结束，表示为 $f^{j,\\tau}_{i,t}$。显然，交通流反映了区域间的连通性，也反映了个体的移动。图1（c）给出了流量和流动的展示。 Problem(Traffic Volume Prediction) 给定知道时段 $t$ 的数据，交通流预测问题目标是预测时段 $t+1$ 的起始和结束流量。 Spatial-Temporal Dynamic Network*这部分，我们讲一下细节。图1是我们模型的架构。 Local Spatial-Temporal Network为了捕获时空序列依赖，在出租车需求预测上，融合局部 CNN 和 LSTM 展示出了非常好的表现。我们这里使用局部 CNN 和 LSTM 处理空间和短期时间依赖。为了手动地提升两种流量的预测（起始和结束），我们将他们集成起来。这部分称为 Local Spatial-Temporal Network (LSTN)。 Local spatial dependency 卷积神经网络用来捕获空间关系。将整个城市看作一张图片，简单地使用 CNN 不能获得最好的性能。包含了弱关系的区域会导致预测性能下降。因此，我们使用局部 CNN 对空间依赖建模。 对于每个时段 $t$，我们将目标区域 $i$ 和它周围的邻居看作是 $S \\times S$ 大小的图片，两个通道 $\\mathbf{Y}_{i,t} \\in \\mathbb{R}^{S \\times S \\times 2}$。一个通道包含起始流量信息，另一个是结束流量信息。目标区域在图像的中间。局部 CNN 使用 $\\mathbf{Y}_{i,t}$ 作为输入 $\\mathbf{Y}^{(0)}_{i,t}$，每个卷积层的定义如下： $$\\tag{1}\\mathbf{Y}^{(k)}_{i,t} = \\text{ReLU}(\\mathbf{W}^{(k)} \\ast \\mathbf{Y}^{(k-1)}_{i,t} + \\mathbf{b}^{(k)}),$$ 其中 $\\mathbf{W}^{(k)}$ 和 $\\mathbf{b}^{(k)}$ 是参数。堆叠 $K$ 层卷积后，用一个全连接来推测区域 $i$ 的空间表示，记为 $\\mathbf{y}_{i,t}$。 Short-term Temporal Dependency 我们使用 LSTM 捕获空间序列依赖。我们使用原始版本的 LSTM： $$\\tag{2}\\mathbf{h}_{i,t} = \\text{LSTM}([\\mathbf{y}_{i,t};\\mathbf{e}_{i,t}], \\mathbf{h}_{i,t-1}),$$ 其中，$\\mathbf{h}_{i,t}$ 是区域 $i$ 在时段 $t$ 的输出表示。$\\mathbf{e}_{i,t}$ 表示外部因素。因此，$\\mathbf{h}_{i,t}$ 包含空间和短期时间信息。 Spatial Dynamic Similarity: Flow Gating Mechanism局部 CNN 用于捕获空间依赖。CNN 通过局部连接和权重共享处理局部结构相似性。在局部 CNN 中，局部空间依赖依靠历史交通流量的相似度。然而，流量的空间依赖是平稳的，不能完全地反映目标区域和其邻居间的关系。一个直接的表示区域间关系的方式是交通流。如果两个区域间有很多流量，那么他们之间的关系强烈（也就是他们更相似）。交通流可以用于控制流量信息在区域间的转移。因此，我们设计了一个 Flow Gating Mechanism (FGM)，以层次的方式对动态空间依赖建模。 类似局部 CNN，我们构建了局部空间流量图来保护流量的空间依赖。一个时段和一个区域相关的流量分两种，流入和流出，两个流量矩阵可以如上构建，每个元素表示对应区域的流入流出流量。图1(c) 给了一个流出矩阵。 给定一个区域 $i$，我们获得过去 $l$ 个时段的相关的流量（即从 $t-l+1$ 到 $t$）。需要的流量矩阵拼接起来，表示为 $\\mathbf{F}_{i,t} \\in \\mathbb{R}^{S \\times S \\times 2l}$，$2l$ 是流量矩阵的数量。因为堆叠的流量矩阵包含了过去与区域 $i$ 相关的矩阵，我们使用 CNN 对区域间的空间流量关系建模，将 $\\mathbf{F}_{i,t}$ 作为输入 $\\mathbf{F}^{(0)}_{i,t}$。对于每层 $k$，公式如下： $$\\tag{3}\\mathbf{F}^{(k)}_{i,t} = \\text{ReLU}(\\mathbf{W}^{(k)}_f \\ast \\mathbf{F}^{(k-1)}_{i,t} + \\mathbf{b}^{(k)}_f),$$ 其中 $\\mathbf{W}^{(k)}_f$ 和 $\\mathbf{b}^{(k)}_f$ 是参数。 每层，我们使用流量信息对区域间的动态相似性进行捕获，通过一个流量门限制空间信息。特别地，空间表示 $\\mathbf{Y}^{i,k}_t$ 作为每层的输出，受流量门调整。我们重写式1为： $$\\tag{4}\\mathbf{Y}^{(k)}_{i,t} = \\text{ReLU}(\\mathbf{W}^{(k)} \\ast \\mathbf{Y}^{(k-1)}_{i,t} + \\mathbf{b}^{(k)}) \\otimes (\\mathbf{F}^{i,k-1}_t),$$ $\\otimes$ 是element-wise product。 $K$ 个门控卷积层后，我们用一个全连接得到流量门控空间表示 $\\mathbf{y}_{i,t}$。 我们将式2中的空间表示 $\\mathbf{y}_{i,t}$ 替换为 $\\hat{\\mathbf{y}}_{i,t}$。 Temporal Dynamic Similarity: Periodically Shifted Attention Mechanism在上面的局部时空网络中，只有前几个时段用于预测。然而，这会忽略长期依赖（周期），但周期在时空预测问题中又很重要。这部分我们考虑长期依赖。 训练 LSTM 来处理长期信息不是一个简单的任务，因为序列长度的增加导致梯度消失，因此会减弱周期性的影响。为了解决这个问题，预测目标的相对时段（即昨天的这个时候，前天这个时候）应该被建模。然而，单纯地融入相对时段是不充分的，会忽略周期的平移，即交通数据不是严格周期的。举个例子，周末的高峰通常发生在下午的后半段，不同的日子时间不一致，从4:30pm到6:00pm变化。周期的平移在交通序列中很普遍，因为事故或堵塞的发生。图 2a 和 2b 分别是不同的天和周的时间平移的例子。这两个时间序列是从 NYC 的出租车数据中算的从 Javits Center出发的流量。显然，交通序列是周期性的，但是这些序列的峰值（通过红圈标记）在不同的日子里时间不一样。此外，对比两张图，周期性不是严格按日或按周的。因此，我们设计了一个 Periodically Shifted Attention Mechanism (PSAM) 来解决问题。详细的方法如下。 我们专注于解决日周期的平移问题。如图 1(a) 所示，从前 $P$ 天获得的相对时段用来处理周期依赖。对于每天，为了解决时间平移问题，我们从每天中额外的选择 $Q$ 个时段。举个例子，如果预测的时间是9:00-9:30pm，我们选之前的一个小时和之后的一个小时，即8:00-19:30pm，$\\vert Q \\vert = 5$。这些时段 $q \\in Q$ 用来解决潜在的时间平移问题。此外，我们使用对每天 $p \\in P$ 保护每天的序列信息，公式如下： $$\\tag{5}\\mathbf{h}^{p,q}_{i,t} = \\text{LSTM}([\\mathbf{y}^{p,q}_{i,t}; \\mathbf{e}^{p,q}_{i,t}], \\mathbf{h}^{p,q-1}_{i,t}),$$ 其中，$\\mathbf{h}^{p,q}_{i,t}$ 是对于区域 $i$ 的预测时间 $t$，时段 $q$ 在前一天 $p$ 的表示。 我们用了一个注意力机制捕获时间平移并且获得了前几天的每一天的一个表示。前几天每一天的表示 $\\mathbf{h}^p_{i,t}$ 是时段 $q$ 每一个选中时间的带权加和，定义为： $$\\tag{6}\\mathbf{h}^p_{i,t} = \\sum_{q \\in Q} \\alpha^{p,q}_{i,t} \\mathbf{h}^{p,q}_{i,t},$$ 权重 $\\alpha^{p,q}_{i,t}$ 衡量了在 $p \\in P$ 这天时段 $q$ 的重要性。重要值 $\\alpha^{p,q}_{i,t}$ 通过对比从短期记忆（式2）得到的时空表示和前一个隐藏状态 $\\mathbf{h}^{p,q}_{i,t}$ 得到。权重定义为： $$\\tag{7}\\alpha^{p,q}_{i,t} = \\frac{\\text{exp}(\\text{score}(\\mathbf{h}^{p,q}_{i,t}, \\mathbf{h}_{i,t}))} {\\sum_{q \\in Q} \\text{exp}(\\text{score} (\\mathbf{h}^{p,q}_{i,t}, \\mathbf{h}_{i,t})}$$ 类似 (Luong, Pham and Manning 2015)，注意力分数的定义可以看作是基于内容的函数： $$\\tag{8}\\text{score}(\\mathbf{h}^{p,q}_{i,t}, \\mathbf{h}_{i,t}) = \\mathbf{v}^\\text{T} \\text{tanh} (\\mathbf{W_H} \\mathbf{h}^{p,q}_{i,t} + \\mathbf{W_X} \\mathbf{h}_{i,t} + \\mathbf{b_X}),$$ 其中，$\\mathbf{W_H}, \\mathbf{W_X}, \\mathbf{b_X}, \\mathbf{v}$ 是参数，$\\mathbf{v}^\\text{T}$ 是转置。对于前面的每一天 $p$，我们得到一个周期表示 $\\mathbf{h}^p_{i,t}$。然后我们使用另一个 LSTM 用这些周期表示作为输入，保存序列信息，即 $$\\tag{9}\\hat{\\mathbf{h}}^p_{i,t} = \\text{LSTM}(\\mathbf{h}^p_{i,t}, \\hat{\\mathbf{h}}^{p-1}_{i,t}).$$ 我们将最后一个时段的输出 $\\hat{\\mathbf{h}}^P_{i,t}$ 看作是时间动态相似度的表示（即长期周期信息）。 Joint Training我们拼接把短期表示 $\\mathbf{h}_{i,t}$ 和 长期表示 $\\hat{\\mathbf{h}}^P_{i,t}$ 拼接得到 $\\mathbf{h}^c_{i,t}$，对于预测区域和时间来说既保留了短期依赖又保留了长期依赖。我们将 $\\mathbf{h}^c_{i,t}$ 输入到全连接中，获得每个区域 $i$ 流入和流出流量的最终预测值，分别表示为 $y^i_{s,t+1}$ 和 $y^i_{e,t+1}$。最终预测函数定义为： $$\\tag{10}[y^i_{s,t+1}, y^i_{e,t+1}] = \\text{tanh}(\\mathbf{W}_{fa} \\mathbf{h}^c_{i,t} + \\mathbf{b}_{fa}),$$ 因为我们做了归一化，所以输出的范围在 $(-1, 1)$，输出值会映射回需求值。 我们同时预测出发和到达流量，损失函数定义为： $$\\tag{11}\\mathcal{L} = \\sum^n_{i=1} \\lambda (y^s_{i,t+1} - \\hat{y}^s_{i, t+1})^2 + (1 - \\lambda) (y^e_{i,t+1} - \\hat{y}^e_{i, t+1})^2,$$ $\\lambda$ 用来平衡流入和流出的影响。区域 $i$ 在时间 $t+1$ 实际的流入和流出流量表示为 $\\hat{y}^s_{i, t+1}, \\hat{y}^e_{i, t+1}$。 ExperimentExperiment SettingsDatasets 我们在两个 NYC 的大型数据集上评价了模型。每个数据集包含旅行记录，详情如下： NYC-Taxi：2015 年 22349490 个出租车的旅行记录，从 2015年1月1日到2015年3月1日。实验中，我们使用1月1日到2月10日作为训练，剩下20天测试。 NYC-Bike：2016 年 NYC 自行车轨迹数据，7月1日到8月29日，包含了 2605648 条记录。前 40 天用来训练，后 20 天做测试。 Preprocessing 我们将整个城市分成 $10 \\times 20$ 个区域。每个区域大小是 $1km \\times 1km$。时段长度设为 30min。使用最大最小归一化 volume 和 flow 到 $[0, 1]$。预测后，使用逆变换后的值评价。我们使用滑动窗采样。测试模型的时候，滤掉 volumn 小于 10 的样本，这是工业界和学界常用的技巧 (Yao et al. 2018)。因为真实数据集中，关注较小的交通数据没有太大的意义。我们选择 80% 的数据训练，20% 验证。 Evaluation Metric &amp; Baselines 两个指标：MAPE, RMSE。baselines：HA, ARIMA, Ridge, Lin-UOTD (Tong el al. 2017), XGBoost, MLP, ConvLSTM, DeepSD, ST-ResNet, DMVST-Net。 Hyperparameter Settings 我们基于验证集设定超参数。对于空间信息，64 个卷积核，$3 \\times 3$ 大小。每个邻居的大小设定为 $7 \\times 7$。层数 $K = 3$，对于 flow 考虑的时间跨度 $l = 2$。时间信息，短期 LSTM 长度为 7，长期周期信息 $\\vert P \\vert = 3$，周期平移注意力机制 $\\vert Q \\vert = 3$，LSTM 中隐藏表示的维度是 128。STDN 通过 Adam 优化，batch size 64，学习率 0.001。LSTM 中的 dropout 0.5。$\\lambda$ 取 0.5。 ResultsPerformanceComparison 表 1 展示了我们的方法对比其他方法在两个数据集上的结果。我们跑每个 baseline 10次，取了平均和标准差。此外，我们也做了 t 检验。我们的方法在两个数据集上指标都很好。","link":"/blog/2019/03/21/revisiting-spatial-temporal-similarity-a-deep-learning-framework-for-traffic-prediction/"},{"title":"Self-Attention Graph Pooling","text":"ICML 2019，原文地址：Self-Attention Graph Pooling Abstract这些年有一些先进的方法将深度学习应用到了图数据上。研究专注于将卷积神经网络推广到图数据上，包括重新定义图上的卷积和下采样（池化）。推广的卷积方法已经被证明有性能提升且被广泛使用。但是，下采样的方法仍然是一个难题且有提升空间。我们提出了一个基于自注意力的图的池化方法。使用图卷积的自注意力使得我们的池化方法可以同时考虑顶点特征和图的拓扑结构。为了确保一个公平的对比，我们使用了相同的训练步骤和模型架构。实验结果显示我们的方法有更高的分类精度。 1. IntroductionCNN 成功利用了图像、语音、视频数据中的欧氏空间（网格结构）。CNN 由卷积层和下采样层（池化层）组成。卷积层和池化层挖掘了网格数据的平移不变性和compositionality（这个我不知道是什么。。。）。结果是，CNN 用少量的参数就可以表现的很好。 然而，很多数据是非欧空间上的。社交网络、生物蛋白质网络、分子网络可以表示成网络。将 CNN 应用在非欧空间上的尝试已经获得了成功。很多研究重新定义了图上的卷积和池化。 对于图卷积的池化操作现在比较少。之前图的池化的研究只考虑图的拓扑结构 (Defferrard et al., 2016; Rhee et al., 2018)。一些方法利用了结点的特征获得一个小图的表示。最近，Ying et al.; Gao &amp; Ji; Cangea et al. 提出了创新的池化方法，可以层级的表示图。这些方法使得图神经网络可以通过端到端的形式，在池化后获得尺寸缩减的图。 然而，上述的池化方法仍有提升空间。举个例子，Ying et al. 的可微层级池化方法有平方级别的空间复杂度，参数依赖于顶点数。Gao &amp; Ji; Cangea et al. 解决了复杂度的问题，但是没有考虑图的拓扑结构。 我们提出的 SAGPool 是一个层次的自注意力图池化方法。我们的方法可以通过端到端的方式使用相对较少的参数学习到层次表示。自注意力机制用来区分结点是否丢弃掉还是保留。由于自注意力机制使用图卷积计算注意力分数，结点特征和图的拓扑结构可以被考虑其中。一句话，SAGPool 有前面方法的优点，是第一个使用自注意力用于池化的方法，并且获得了很好的性能。代码已经在 Github 上开源了。 2. Related Work2.1. Graph Convolution图上的卷积要么是基于谱的，要么是非谱的。谱方法专注于在傅里叶域上定义卷积，利用使用图拉普拉斯矩阵的谱滤波器。Kipf &amp; Welling 提出了一个层级传播的规则，简化了使用切比雪夫展开来趋近拉普拉斯矩阵的方法。非谱方法的目标是定义一个卷积操作，可以直接应用在图上。通常来说，非谱方法，中心结点在特征传入下层之前聚合邻接结点的特征。Hamilton et al. 提出了 GraphSAGE，通过采样和聚合学习结点的嵌入。尽管 GraphSAGE 会采样固定数量的邻居，GAT 基于注意力机制，在所有的邻居上计算结点表示。两个方法在图相关的任务上都有提升。 2.2. Graph Pooling池化层通过缩减表示的大小，使得 CNN 能减少参数的数量，因此能避免过拟合。为了泛化 CNN，GNN 上的池化是必要的。图的池化方法可以归入三类：基于拓扑的，基于全局的，基于层次的。 Topology based pooling 早期的工作使用图的缩减算法，而不是神经网络。谱聚类算法使用特征值分解获得缩减的图。然而，特征值分解的时间复杂度高。Graclus (Dhillon et al., 2007) 不使用特征向量计算给定图的聚类结果，而是通过一个谱聚类的目标函数与一个带权的核 k-means 目标函数的等价性。即便在最近的 GNN 模型中，Graclus 也被使用作为一个池化单元。 Global pooling 不像之前的方法，全局池化方法考虑图的特征。全局池化方法在每层聚合表示的时候使用加和的方式而不是用神经网络。这个方法可以处理不同结构的图，因为它获得了所有的表示。Gilmer et al. 将 GNN 看作是一种信息的传递规则，提出了一个通用框架用于图分类，整个图的表示可以通过使用 Set2Set (Vinyals et al., 2015) 来获得。SortPool (Zhang et al., 2018b) 根据一个图的结构角色对结点嵌入排序，将排序后的表示传入下一层。 Hierarchical pooling 全局池化方法不学习层次表示，但是对于捕获图的结构信息来说，层次表示很关键。层次池化的动机在于在每层构建一个模型，这个模型可以学习基于特征的或基于拓扑的顶点分配。Ying et al. 提出了 DiffPool，这是一种可微的图的池化方法，可以以端到端的形式学习分配矩阵。在层 $l$ 学习到的分配矩阵 $S^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l+1}}$ 包含了层 $l$ 中的结点在 $l + 1$ 层被分配到类簇的概率。$n_l$ 表示层 $l$ 的结点数。结点通过下式来分配： $$\\tag{1}S^{(l)} = \\text{softmax}(\\text{GNN}_l (A^{(l)}, X^{(l)})) \\\\A^{(l+1)} = S^{(l)\\text{T}} A^{(l)} S^{(l)}$$ $X$ 表示矩阵的结点特征，$A$ 是邻接矩阵。 Cangea et al. 使用 gPool (Gao &amp; Ji, 2019) 获得了和 DiffPool 相当的性能。gPool 需要 $\\mathcal{O}(\\vert V \\vert + \\vert E \\vert)$ 的空间复杂度，DiffPool 需要 $\\mathcal{O}(k \\vert V \\vert^2)$ 的空间复杂度。$V$，$E$，$k$ 分别表示顶点、边、池化比例。gPool 使用一个可学习的向量 $p$ 计算投影分数，然后使用这个分数选择最高的结点。投影分数通过 $p$ 和所有结点的特征向量的内积获得。分数表示结点可以获得的信息量。下面的式子大体的描述了 gPool 中的池化步骤： $$\\tag{2}y = X^{(l)} \\mathbf{p}^{(l)} / \\Vert \\mathbf{p}^{(l)} \\Vert, \\text{idx=top-rank}(y, \\lceil kN \\rceil) \\\\A^{(l+1)} = A^{(l)}_{\\text{idx,idx}}$$ 如式 2，图的拓扑结构不影响投影分数。 为了进一步提高图的池化，我们提出了 SAGPool，可以在可观的时间和空间复杂度上利用特征和拓扑结构生成层次表示。 3. Proposed MethodSAGPool 的关键是它使用了 GNN 得到的注意力分数。SAGPool 层和模型架构分别是图 1 和图 2. 3.1 Self-Attention Graph PoolingSelf-attention mask 注意力机制广泛应用在最近的深度学习研究中。这样的机制使得模型可以更专注于重要的特征，不那么关注不重要的特征。自注意力一般称为内注意力，允许输入特征作为自身注意力的标准 (Vaswani et al., 2017)。我们使用图卷积获得自注意力分数。举个例子，如果图卷积的公式是 Kipf &amp; Welling 使用的，那么自注意力分数 $Z \\in \\mathbb{R}^{N \\times 1}$ 通过下式计算： $$\\tag{3}Z = \\sigma(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} X \\Theta_{att})$$ $\\sigma$ 是激活函数，如 $tanh$，$\\tilde{A} \\in \\mathbb{R}^{N \\times N}$ 是有自连接的邻接矩阵，$\\tilde{D} \\in \\mathbb{R}^{N \\times N}$ 是度矩阵，$X \\in \\mathbb{R}^{N \\times F}$ 是图的特征矩阵，$\\Theta_{att} \\in \\mathbb{R}^{F \\times 1}$ 是 SAGPool 层仅有的参数。通过利用图卷积获得自注意力分数，池化的结果是同时基于图的特征和拓扑结构的。我们利用 Gao &amp; Ji; Cangea et al. 的结点选择方法，保留了输入的图的一部分结点，甚至当图的尺寸和结构改变时。池化比例 $k \\in (0, 1]$ 是一个超参数决定了保留多少结点。基于 $Z$ 的值选择最高的 $\\lceil kN \\rceil$ 个结点。 $$\\tag{4}\\text{idx = top-rank}(Z, \\lceil kN \\rceil), Z_{mask} = Z_{\\text{idx}}$$ $\\text{top-rank}$ 返回最高的 $\\lceil kN \\rceil$ 个值的下标，$\\cdot_{\\text{idx}}$ 是下标操作，$Z_{mask}$ 是特征的注意力 mask。 Graph pooling 输入的图通过图 1 中的 masking 操作。 $$\\tag{5}X’ = X_{idx,:}, X_{out} = X’ \\odot Z_{mask}, A_{out} = A_{\\text{idx, idx}}$$ 其中 $X_{\\text{idx,:}}$ 是指定行下标的特征矩阵，每行表示一个结点，$\\odot$ 是 elementwise 乘积，$A_{\\text{idx, idx}}$ 是指定行下标和列下标的邻接矩阵。$X_{out}$ 和 $A_{out}$ 是新的特征矩阵和对应的邻接矩阵。 Variation of SAGPool 使用图卷积的主要原因是为了反映图的特征和拓扑结构。可以使用不同的图卷积来替换式 3 中的图卷积。计算注意力机制 $Z \\in \\mathbb{R}^{N \\times 1}$ 的泛化公式如下： $$\\tag{6}Z = \\sigma(\\text{GNN}(X, A))$$ $X$ 和 $A$ 是特征矩阵和邻接矩阵。 除了使用邻接结点还可以使用多跳结点来计算注意力分数。式 7 和式 8 分别使用了两跳连接和堆叠 GNN 层。增加邻接矩阵的平方增加了两条邻居： $$\\tag{7}Z = \\sigma(\\text{GNN}(X, A + A^2))$$ 堆叠 GNN 层可以间接的聚合两跳结点。这样的话，非线性层和参数的数量就增加了： $$\\tag{8}Z = \\sigma(\\text{GNN}_2 (\\sigma(\\text{GNN}_1 (X, A)), A))$$ 式 7 和式 8 可以利用多跳连接。 另一个变体是平均多个注意力分数。平均注意力分数通过 $M$ 个 GNN 获得： $$\\tag{9}Z = \\frac{1}{m} \\sum_m \\sigma(\\text{GNN}_m (X, A))$$ 在论文中，式 7，8，9 的模型分别记为 $\\rm {SAGPool}_{augmentation}$，$\\rm {SAGPool}_{serial}$，$\\rm {SAGPool}_{parallel}$。 3.2 Model Architecture根据 Lipton &amp; Steinhardt 的研究，如果对一个模型做很多修改，那很难知道是哪部分改进起的作用。为了一个公平的对比，我们使用了 Zhang et al. 和 Cangea et al. 的模型来对比我们的方法。 Convolution layer 如 2.1 节提到的，有很多图卷积的定义。其他类型的图卷积可能也能提升性能，但是我们利用的是 Kipf &amp; Welling 提出的广泛使用的图卷积。式 10 和式3 一样，除了 $\\Theta$ 的维度： $$\\tag{10}h^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} h^{(l)} \\Theta)$$ 其中 $h^{(l)}$ 是第 $l$ 层的节点表示，$\\Theta \\in \\mathbb{R}^{F \\times F’}$ 是卷积核。使用 ReLU 作为激活函数。 Readout layer 受 JK-net 的启发，Cangea et al. 提出了一个 readout 层，聚合结点的特征生成一个固定大小的表示。readout 层的聚合特征如下： $$\\tag{11}s = \\frac{1}{N} \\sum^N_{i=1} x_i \\mid \\mid \\mathop{max}\\limits^N_{i=1} x_i$$ $N$ 是结点数，$x_i$ 是第 $i$ 个结点的特征向量，$\\mid \\mid$ 表示拼接。 Global pooling architecture 我们实现了 Zhang et al. 提出的全局池化结构。如图 2 所示，全局池化结构由三层图卷积层组成，每层的输出拼接在一起。结点特征在 readout 层聚合，然后接一个池化层。图的特征表示传入线性层用来分类。 Hierarchical pooling architecture 在这部分设置中，我们实现了 Cangea et al. 的层次池化结构。如图 2 所示，结构包含了三个块，每个块由一个卷积层和一个池化层组成。每个块的输出通过一个 readout 层聚合。每个 readout 层的输出之和放入线性层做分类。 4. Experiments我们在图分类上评估了全局池化和层次池化。 4.1. Datasets5 个数据集。","link":"/blog/2019/06/25/self-attention-graph-pooling/"},{"title":"Semi-Supervised Classification With Graph Convolutional Networks","text":"ICLR 2017。图卷积中谱图领域理论上很重要的一篇论文，提升了图卷积的性能，使用切比雪夫多项式的1阶近似完成了高效的图卷积架构。原文链接：Semi-Supervised Classification with Graph Convolutional Networks. Kipf &amp; Welling 2017 摘要我们提出了一种在图结构数据上的半监督可扩展学习方法，基于高效的图卷积变体。契机是通过一个谱图卷积的局部一阶近似得到的我们的图卷积结构。我们的模型与图的边数呈线性关系，学习到的隐藏层可以对图的顶点和局部图结构同时进行编码。在引文网络和一个知识图谱数据集上的大量实验结果表明我们的方法比相关方法好很多。 引言我们考虑一个对图顶点进行分类的问题，只有一小部分的顶点有标签。这个问题可以通过基于图的半监督学习任务建模，通过某些明确的图正则化方法(Zhu et al., 2003; Zhou et al., 2004; Belkin et al., 2006; Weston et al., 2012)可以平滑标签信息，举个例子，通过在loss function使用一个图拉普拉斯正则项：$$\\tag{1} \\mathcal{L} = \\mathcal{L_0} + \\lambda \\mathcal{L_{reg}}, \\rm with \\ \\mathcal{L_{reg}} = \\sum_{i.j}A_{ij} \\Vert f(X_i) - f(X_j) \\Vert^2 = f(X)^T \\Delta f(X)$$其中，$\\mathcal{L_0}$表示对于图的标签部分的监督损失，$f(\\cdot)$可以是一个神经网络类的可微分函数，$\\lambda$是权重向量，$X$是定点特征向量$X_i$的矩阵。$N$个顶点$v_i \\in \\mathcal{V}$，边$(v_i, v_j) \\in \\varepsilon$，邻接矩阵$A \\in \\mathbb{R}^{N \\times N}$（二值的或者带权重的），还有一个度矩阵$D_{ii} = \\sum_jA_{ij}$。式1依赖于“图中相连的顶点更有可能具有相同的标记”这一假设。然而，这个假设，可能会限制模型的能力，因为图的边并不是必须要编码成相似的，而是要包含更多的信息。在我们的研究中，我们将图结构直接通过一个神经网络模型$f(X, A)$进行编码，并且在监督的目标$\\mathcal{L_0}$下对所有有标记的顶点进行训练，因此避免了损失函数中刻意的对图进行正则化。在图的邻接矩阵上使用$f(\\cdot)$可以使模型从监督损失$\\mathcal{L_0}$中分布梯度信息，并且能够从有标记和没有标记的顶点上学习到他们的表示。我们的贡献有两点，首先，我们引入了一个简单的，表现很好的针对神经网络的对层传播规则，其中，这个神经网络是直接应用到图上的，并且展示了这个规则是如何通过谱图卷积的一阶近似启发得到的。其次，我们展示了这种形式的基于图的神经网络可以用于对图中的顶点进行更快更可扩展的半监督分类任务。在大量数据集上的实验表明我们的模型在分类精度和效率上比当前在半监督学习中的先进算法要好。 图上的快速近似卷积在这部分，我们会讨论一个特殊的基于图的神经网络$f(X, A)$。考虑一个多层图卷积网络(GCN)，通过以下的传播规则：$$\\tag{2} H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{(l)} W^{(l)})$$其中，$\\tilde{A} = A + I_N$是无向图$\\mathcal{G}$加了自连接的邻接矩阵。$I_N$是单位阵，$\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$，$W^{(l)}$是一个针对层训练的权重矩阵。$\\sigma(\\cdot)$表示一个激活函数，比如$\\rm ReLU(\\cdot) = \\rm max(0, \\cdot)$，$H^{(l)} \\in \\mathbb{R}^{N \\times D}$是第$l$层的激活矩阵；$H^{(0)} = X$。接下来我们将展示通过图上的一阶近似局部谱滤波器(Hammond et al., 2011; Defferrard et al., 2016)的传播过程。 谱图卷积 spectral graph convolutions定义图上的谱图卷积为信号$x \\in \\mathbb{R}^N$和一个滤波器$g_\\theta = \\rm diag(\\theta)$，参数是傅里叶域中的$\\theta \\in \\mathbb{R}^N$，也就是：$$\\tag{3} g_\\theta \\ast x = U g_\\theta U^T x$$其中$U$是归一化的拉普拉斯矩阵$L = I_N - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} = U \\Lambda U^T$的特征向量组成的矩阵，$\\Lambda$是特征值组成的对角阵，$U^Tx$是$x$的图傅里叶变换。可以认为$g_\\theta$是关于$L$的特征值的函数，也就是说$g_\\theta(\\Lambda)$。式3的计算量很大，因为特征向量矩阵$U$的乘法的时间复杂度是$O(N^2)$。此外，对于大尺度的图来说，对$L$进行特征值分解是计算量非常大的一件事。为了避开这个问题，Hammond et al.(2001)建议使用$K$阶切比雪夫多项式$T_k(x)$来近似$g_\\theta(\\Lambda)$：$$\\tag{4} g_\\theta’ \\approx \\sum^K_{k=0} \\theta’_k T_k(\\tilde{\\Lambda})$$其中，$\\tilde{\\Lambda} = \\frac{2}{\\lambda_{max}} \\Lambda - I_N$。$\\lambda_{max}$表示$L$的最大特征值。$\\theta’ \\in \\mathbb{R}^K$是切比雪夫系数向量。切比雪夫多项式的定义是：$T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x)$，$T_0(x) = 1$，$T_1(x) = x$。回到我们对于一个信号$x$和一个滤波器$g_\\theta’$的卷积的定义：$$\\tag{5} g_\\theta’ \\ast x \\approx \\sum^K_{k=0} \\theta’_k T_k(\\tilde{L}) x$$其中，$\\tilde{L} = \\frac{2}{\\lambda x_{max}} L - I_N$；注意$(U \\Lambda U^T)^k = U \\Lambda^k U^T$。这个表达式目前是$K$阶局部的，因为这个表达式是拉普拉斯矩阵的$K$阶多项式，也就是说从中心节点向外最多走$K$步，$K$阶邻居。式5的时间复杂度是$O(\\vert \\varepsilon \\vert)$，也就是和边数呈线性关系。Defferrard et al.(2016)使用这个$K$阶局部卷积定义了在图上的卷积神经网络。 按层的线性模型 layer-wise linear model一个基于图卷积的神经网络模型可以通过堆叠式5这样的多个卷积层来实现，每层后面加一个非线性激活即可。现在假设$K=1$，也就是对$L$线性的一个函数，因此得到一个在图拉普拉斯谱(graph Laplacian spectrum)上的线性函数。这样，我们仍然能通过堆叠多个这样的层获得一个卷积函数，但是我们就不会再受限于明显的参数限制，比如切比雪夫多项式。我们直觉上期望这样一个模型可以减轻在度分布很广泛的图上局部图结构模型过拟合的问题，如社交网络、引文网络、知识图谱和其他很多真实数据集。此外，这个公式可以让我们搭建更深的网络，一个可以提升模型学习能力的实例是He et al., 2016。在GCN的线性公式中，我们让$\\lambda_{max}$近似等于2，因为我们期望神经网络参数可以在训练中适应这个变化。在这个近似下，式5可以简化为：$$\\tag{6} g_\\theta’ \\ast x \\approx \\theta’_0x + \\theta’_1 (L - I_N)x = \\theta’_0x - \\theta’_1 D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} x$$两个参数$\\theta’_0$和$\\theta’_1$。滤波器参数可以在整个图上共享。连续的使用这种形式的卷积可以有效的对一个顶点的$k$阶邻居进行卷积，$k$是连续的卷积操作或模型中卷积层的个数。实际上，通过限制参数的数量可以进一步的解决过拟合的问题，并且最小化每层的操作数量（比如矩阵乘法）。这时的我们得到了下面的式子：$$\\tag{7} g_\\theta \\ast x \\approx \\theta(I_N + D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}) x$$只有一个参数$\\theta = \\theta’_0 = - \\theta’_1$。注意，$I_N + D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$现在的特征值在$[0, 2]$之间。在深层模型中重复应用这个操作会导致数值不稳定和梯度爆炸、消失的现象。为了减轻这个问题，我们引入了如下的重新正则化技巧：$I_N + D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} \\to \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}$，$\\tilde{A} = A + I_N$，$\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$。我们可以将这个定义泛化到一个有着$C$个通道的信号$X \\in \\mathbb{R}^{N \\times C}$上，也就是每个顶点都有一个$C$维的特征向量，对于$F$个滤波器或$F$个feature map的卷积如下：$$\\tag{8} Z = \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} X \\Theta$$其中$\\Theta \\in \\mathbb{R}^{C \\times F}$是一个滤波器的参数矩阵，$Z \\in \\mathbb{R}^{N \\times F}$是卷积的信号矩阵。卷积操作的时间复杂度是$O(\\vert \\varepsilon \\vert F C)$，因为$\\tilde{A} X$可以被实现成一个稀疏矩阵和一个稠密矩阵的乘积。 半监督顶点分类介绍过这个简单、灵活的可以在图上传播信息的模型$f(X, A)$后，我们回到半监督顶点分类的问题上。如介绍里面所说的，我们可以减轻在基于图的半监督学习任务中的假设，通过在图结构上的数据$X$和邻接矩阵$A$上使用模型$f(X, A)$。我们期望这个设置可以在邻接矩阵表达出数据$X$没有的信息的这种情况时表现的很好，比如引文网络中，引用的关系或是知识图谱中的关系。整个模型是一个多层的GCN，如图1所示。 例子我们考虑一个两层GCN对图中的顶点进行半监督分类，邻接矩阵是对称的。我们首先在预处理中计算$\\hat{A} = \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}$。前向传播模型的形式如下：$$\\tag{9} Z = f(X, A) = \\rm softmax( \\hat{A} \\ \\rm ReLU( \\hat{A}XW^{(0)})W^{(1)})$$这里，$W^{(0)} \\in \\mathbb{R}^{C \\times H}$是输入到隐藏层的权重矩阵，有$H$个feature map。$W^{(1)} \\in \\mathbb{R}^{H \\times F}$是隐藏层到输出的权重矩阵。softmax激活函数定义为$\\rm softmax(x_i) = \\frac{1}{\\mathcal{Z}} \\exp(x_i)$，$\\mathcal{Z} = \\sum_i \\exp(x_i)$，按行使用。对于半监督多类别分类，我们使用交叉熵来衡量所有标记样本的误差：$$\\tag{10} \\mathcal{L} = - \\sum_{l \\in \\mathcal{Y}_L} \\sum^F_{f = 1} Y_{lf} \\ln(Z_{lf})$$其中，$\\mathcal{Y}_L$是有标签的顶点的下标集合。神经网络权重$W^{(0)}$和$W^{(1)}$使用梯度下降训练。我们每次训练的时候都是用全部的训练集来做梯度下降，只要数据集能放到内存中。对$A$进行稀疏矩阵的表示，内存的使用量是$O(\\vert \\varepsilon \\vert)$。训练过程中使用了dropout增加随机性。我们将在未来的工作使用mini-batch随机梯度下降。 实现我们使用Tensorflow实现了基于GPU的，稀疏稠密矩阵乘法形式。式9的时间复杂度是$O(\\vert \\varepsilon \\vert C H F)$。","link":"/blog/2018/07/02/semi-supervised-classification-with-graph-convolutional-networks/"},{"title":"Session-based Social Recommendation via Dynamic Graph Attention Networks","text":"WSDM 2019，原文链接：Session-based Social Recommendation via Dynamic Graph Attention Networks Abstract像 Facebook 和 Twitter 这样的在线社区很流行，已经成为很多用户生活中的重要部分。通过这些平台，用户可以发掘并创建信息，其他人会消费这些信息。在这种环境下，给用户推荐相关信息变得很重要。然而，在线社区的推荐是一个难题：1. 用户兴趣是动态的，2. 用户会受其朋友的影响。此外，影响者与环境相依。不同的朋友可能关注不同的话题。对两者建模对推荐来说是重要的。 我们提出了一个基于动态图注意力机制的在线社区推荐系统。我们用一个 RNN 对动态的用户行为建模，用图卷积对依赖环境的社交影响建模，可以动态地根据用户当前的兴趣推测影响者。整个模型可以高效地用于大规模的数据。几个真实数据集上的实验结果显示我们的方法很好，源码在：https://github.com/DeepGraphLearning/RecommenderSystems 1 Introduction在线社区已经成为今天在线体验的重要组成部分。Facebook, Twitter, 豆瓣可以让用户创建、分享、消费信息。因此这些平台的推荐系统对平台上的表层信息和维持用户活跃度来说很重要。然而，在线社区对推荐系统提出了一些挑战。 首先，用户兴趣本质上来说是动态的。一个用户可能一段时间对体育感兴趣，之后呢对音乐感兴趣。其次，因为在线社区里面的用户经常给朋友分享信息，用户也会被他们的朋友影响。举个例子，一个找电影的用户可能会被她的朋友喜欢的电影影响。此外，施加影响的一方组成的集合是动态的，因为这和环境有关。举个例子，一个用户在找一个搞笑电影的时候会听取一群喜欢喜剧的朋友的意见，在找动作电影的时候，会受到另一组朋友的影响。 Motivating Example. 图 1 展示了 Alice 和 她的朋友在一个在线社区的行为。行为通过一个动作（比如点击操作）序列描述。为了捕获用户的动态兴趣，她们的行为被分成了不同的子序列，表示 sessions。我们感兴趣的是基于 session 的推荐：我们根据当前情境下 Alice 已经消费过的东西给她推荐下一个她可能消费的东西。图 1 展示出两个情景，a 和 b。此外，Alice 朋友们的消费信息也是可获得的。我们会利用这些信息生成更好的推荐。因此我们在一个基于 session 的社交推荐情景下。 在 session a 中，Alice 浏览了体育的物品。她的两个朋友：Bob 和 Eva，是出了名的体育粉丝（长期兴趣），他们最近正好浏览了体育相关的物品（短期兴趣）。考虑到这个情况，Alice 可能被他们两个影响，比如说接下来她可能会学习乒乓球。在 session b 中，Alice 对文学艺术物品感兴趣。这个环境和刚才不一样了因为她没有最近正在消费这样物品的朋友。但是 David 一直对这个话题感兴趣（长期兴趣）。在这种情况下，对 Alice 来说可能会被 David 影响，可能会被推荐一本 David 喜欢的书。这些例子表明了一个用户当前的兴趣是如何与他不同的朋友的兴趣相融合来提供基于情景的推荐的。我们提出了一个推荐模型来处理这两种情况。 当前的推荐系统要么对用户的动态兴趣建模，要么对他们的社交影响建模，但是，据我们所知，现存的方法还没有融合过他们。最近的一个研究对 session 级别的用户行为使用 RNN 建模，忽略了社交影响。其他的研究仅考虑社交影响。举个例子，Ma et al. 探索了朋友的长期兴趣产生的社交影响。但是，不同用户的影响是静态的，没有描绘出每个用户当前的兴趣。 我们提出了一个方法对用户基于 session 的兴趣和动态社交影响同时建模。也就是说，考虑了基于当前用户的 session，他的朋友的哪个子集影响了他。我们的推荐模型基于动态注意力网络。我们的方法先用一个 RNN 对一个 session 内的用户行为建模。根据用户当前兴趣——通过 RNN 的隐藏表示捕获到的——我们使用 GAT 捕获了他的朋友的影响。为了提供 session 级别的推荐，我们区分了短期兴趣和长期兴趣。在给定用户当前兴趣的基础上，每个朋友的影响通过注意力机制自动地决定。 我们做了大量实验，效果比很多方法好。贡献如下： 提出了同时对动态用户兴趣和依赖环境的社交影响学习后对在线社区进行推荐的方法。 提出了基于动态图注意力网络的推荐方法。在大数据集上也有效。 实验结果比 state-of-the-art 好很多。 2 Related Work讨论三条路线：1. 对动态用户行为建模的推荐系统，2. 考虑社交影响的推荐系统，3. 图卷积网络。 2.1 Dynamic Recommendation2.2 Social Recommendation2.3 Graph Convolutional Networks3 Problem Definition推荐系统根据历史行为推荐相关的物品。传统的推荐模型，如矩阵分解，忽略了用户的消费顺序。在线社区中，用户兴趣是快速变化的，必须要考虑用户偏好顺序，以便对用户的动态兴趣建模。实际上，因为用户全部的历史记录可以很长（有些社区存在好多年了），用户兴趣切换的很快，一个常用的方法是将用户的偏好分成不同的 session，（使用时间戳，以一个星期为时间段考虑每个用户的行为）并以 session 为级别提供推荐。定义如下： DEFINITION 1. (Session-based Recommendation)，$U$ 表示用户的集合，$I$ 表示物品集。每个用户 $u$ 和一组带有时间步 $T$ 的 session 相关，$I^u_T = \\lbrace \\vec{S}^u_1, \\vec{S}^u_2, \\dots \\vec{S}^u_T \\rbrace$，其中 $\\vec{S}^u_t$ 是用户 $u$ 的第 $t$ 个 session。在每个 session 内，$\\vec{S}^u_t$ 由一个用户行为的序列 $\\lbrace i^u_{t,1}, i^u_{t,2}, \\dots, i^u_{t,N_{u,t}} \\rbrace$ 组成，其中 $i^u_{t,p}$ 是在第 $t$ 个 session 中用户消费的第 $p$ 个物品，$N_{u,t}$ 是 session 中物品的总数。对于每个用户 $u$，给定一个 session $\\vec{S}^u_{T+1} = \\lbrace i^u_{T+1,1}, \\dots i^u_{T+1,n} \\rbrace$，基于 session 的推荐系统的目标是从 $I$ 中推荐一组用户可能在下来的 $n+1$ 步时感兴趣的物品，即 $i^u_{T+1, n+1}$。 在在线社区中，用户的兴趣不仅与他们的历史行为相关，也受他们的朋友的影响。举个例子，一个朋友看电影，我也可能会感兴趣。这就叫社交影响。此外，从朋友那里来的影响是跟环境有关的。换句话说，从朋友那里来的影响是不一样的。如果一个用户想买个笔记本电脑，她可能更倾向于问问她喜欢高科技产品的朋友；如果她要买相机，她可能会被她的摄影师朋友影响。就像图 1，一个用户可能被她朋友的长期兴趣和短期兴趣影响。 为了提供一个有效的推荐结果，我们提出对动态的用户兴趣和依赖于环境的社交影响建模。我们定义了如下的问题： DEFINITION 2. (Session-based Social Recommendation) $U$ 表示用户集，$I$ 表示物品集合，$G=(U, E)$ 是社交网络，$E$ 是社交网络的边。给定用户 $u$ 的一个 session $\\vec{S}^u_{T+1} = \\lbrace i^u_{T+1,1}, \\dots i^u_{T+1,n} \\rbrace$，目标是利用她的动态兴趣（$\\cup^{T+1}_{t=1} \\vec{S}^u_t$）和社交影响（$\\cup^{N(u)}_{k=1} \\cup^T_{t=1} \\vec{S}^k_t$，其中 $N(u)$ 是用户 $u$ 的邻居），从 $I$ 中推荐一组用户 $u$ 可能在下来的 $n+1$ 步时感兴趣的物品，即 $i^u_{T+1, n+1}$。 4 Dynamic Social Recommender Systems我们提出的模型 Dynamic Graph Recommendation (DGREC) 是个动态图注意力模型，可以对用户近期的偏好和他的朋友的偏好建模。 DGREC 有 4 个模块（图 2）。首先，一个 RNN 对用户当前 session 中的物品序列建模。她朋友的偏好使用长期偏好和短期偏好融合来建模。短期偏好，或是最近一次 session 中的物品也使用 RNN 来编码。朋友的长期偏好通过一个独立的嵌入层编码。模型使用 GAT 融合当前用户的表示和她朋友的表示。这是我们模型的关键：我们提出了基于用户当前的兴趣学习每个朋友的权重的机制。最后一步，模型通过融合用户当前偏好和她的社交影响得到推荐结果。","link":"/blog/2019/05/29/session-based-social-recommendation-via-dynamic-graph-attention-networks/"},{"title":"Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic","text":"IJCAI 2018，大体思路：使用Kipf &amp; Welling 2017的近似谱图卷积得到的图卷积作为空间上的卷积操作，时间上使用一维卷积对所有顶点进行卷积，两者交替进行，组成了时空卷积块，在加州PeMS和北京市的两个数据集上做了验证。但是图的构建方法并不是基于实际路网，而是通过数学方法构建了一个基于距离关系的网络。原文链接：Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting 摘要实时精确的交通预测对城市交通管控和引导很重要。由于交通流的强非线性以及复杂性，传统方法并不能满足中长期预测的要求，而且传统方法经常忽略对时空数据的依赖。在这篇文章中，我们提出了一个新的深度学习框架，时空图卷积(Spatio-Temporal Graph Convolutional Networks)，来解决交通领域的时间序列预测问题。我们在图上将问题形式化，并且建立了完全卷积的结构，并不是直接应用传统的卷积以及循环神经单元，这可以让训练速度更快，参数更少。实验结果显示通过在多尺度的交通网络上建模，STGCN模型可以有效地捕获到很全面的时空相关性并且在各种真实数据集上表现的要比很多state-of-the-art算法好。 引言交通运输在每个人的生活中都扮演着重要的角色。根据2015年的调查，美国的司机们平均每天要在车上呆48分钟。这种情况下，精确的实时交通状况预测对于路上的用户，private sector和政府来说变得至关重要。广泛使用的交通服务，如交通流控制、路线规划和导航，也依赖于高质量的交通状况预测。总的来说，多尺度的交通预测的研究很有前景而且是城市交通流控制和引导的基础，也是智能交通系统的一个主要功能。 在交通研究中，交通流的基本变量，也就是速度、流量和密度，通常作为监控当前交通状态以及未来预测的指示指标。根据预测的长度，交通预测大体分为两个尺度：短期(5~30min)，中和长期预测(超过30min)。大多数流行的统计方法(比如，线性回归)可以在短期预测上表现的很好。然而，由于交通流的不确定性和复杂性，这些方法在相对长期的预测上不是那么的有效。 之前在中长期交通预测上的研究可以大体的分为两类：动态建模和数据驱动的方法。动态建模使用了数学工具（比如微分方程）和物理知识通过计算模拟来形式化交通问题[Vlahogiani, 2015]。为了达到一个稳定的状态，模拟进程不仅需要复杂的系统编程，还需要消耗大量的计算资源。模型中不切实际的假设和化简也会降低预测的精度。因此，随着交通数据收集和存储技术的快速发展，一大群研究者正在将他们的目光投向数据驱动的方法。典型的统计学和机器学习模型是数据驱动方法的两种体现。在时间序列分析上，自回归移动平均模型（ARIMA）和它的变形是众多统一的方法中基于传统统计学的方法[Ahmed and Cook, 1979; Williams and Hoel, 2003; Lippi $et al.$, 2013]。然而，这种类型的模型受限于时间序列的平稳分布，而且不能考虑时空相关性。因此，这些方法限制了高度非线性的交通流的表示能力。最近，传统的统计方法在交通预测上已经受到了机器学习方法的冲击。这些模型可以获得更高的精度，对更复杂的数据建模，比如k近邻（KNN），支持向量机（SVM）和神经网络（NN）。 深度学习方法 最近，深度学习已经被广泛且成功地应用于各式各样的交通任务中，在最近的工作中已经取得了很显著的成果，比如，深度置信网络（DBN）[Jia et al., 2016; Huang et al., 2014]和层叠自编码器(stacked autoencoder)(SAE)[Lv et al., 2015; Chen et al., 2016]。然而，这些全连接神经网络很难从输入中提取空间和时间特征。而且，空间属性的严格限制甚至完全缺失，这些网络的表示能力被限制的很严重。 为了充分利用空间特征，一些研究者使用了卷积神经网络来捕获交通网络中的临近信息，同时也在时间轴上部署了循环神经网络。通过组合长短期记忆网络[Hochreiter and Schmidhuber, 1997]和1维卷积，Wu和Tan[2016]首先提出一个特征层面融合的架构CLTFP来预测短期交通状况。尽管它采取了一个很简单的策略，CLTFP仍然是第一个尝试对时间和空间规律性对齐的方法。后来，Shi et al.[2015]提出了卷积LSTM，这是一个带有嵌入卷积层的全连接LSTM的扩展。然而，常规的卷积操作限制了模型只能处理常规的网格结构（如图像或视频），而不是其他的大部分领域（比如Graph）。与此同时，循环神经网络对于序列的学习需要迭代训练，这会导致误差的积累。更进一步地说，循环神经网络（包括基于LSTM的RNN）的难以训练和计算量大是众所周知的。 为了克服这些问题，我们引入了一些策略来有效的对交通流的时间动态和空间依赖进行建模。为了完全利用空间信息，我们通过一个广义图对交通网络建模，而不是将交通流看成各个离散的部分（比如网格或碎块）。为了处理循环神经网络的缺陷，我们在时间轴上部署了一个全卷积结构来阻止累积效应（cumulative effects）并且加速模型的训练过程。综上所述，我们提出了一个新的神经网络架构，时空图卷积网络，来预测交通情况。这个架构由多个时空图卷积块组成，这些都是图卷积层和卷积序列学习层（convolutional sequence learning layers）的组合，用来对时间和空间依赖关系进行建模。 我们的主要贡献可以归纳为以下三点： 我们研究了在交通领域时间与空间依赖结合的好处。为了充分利用我们的知识，这是在交通研究中第一次应用纯卷积层来同时从图结构的时间序列中提取时空信息。 我们提出了一个新的由时空块组成的神经网络结构。由于这个架构中是纯卷积操作，它比基于RNN的模型的训练速度快10倍以上，而且需要的参数更少。这个架构可以让我们更有效地处理更大的路网，这部分将在第四部分展示。 我们在两个真实交通数据集上验证了提出来的网络。这个实验显示出我们的框架比已经存在的在多长度预测和网络尺度上的模型表现的更好。 准备工作路网上的交通预测交通预测是一个典型的时间序列预测问题，也就是预测在给定前M个观测样本接下来H个时间戳后最可能的交通流指标（比如速度或交通流）， $$ \\tag{1} \\hat{v}_{t+1}, ..., \\hat{v}_{t+H} = \\mathop{\\arg\\min}_{v_{t+1},...,v_{t+H}}logP(v_{t+1},...,v_{t+H}\\vert v_{t-M+1},...v_t) $$ 这里$v_t \\in \\mathbb{R}^n$是$n$个路段在时间戳$t$观察到的一个向量，每个元素记录了一条路段的历史观测数据。 在我们的工作中，我们在一个图上定义了一个交通网络，并专注于结构化的交通时间序列。观测到的样本$v_t$间不是相互独立的，而是在图中两两相互连接的。因此，数据点$v_t$可以被视为定义在权重为$w_{ij}$，如图1展示的无向图（或有向图）$\\mathcal{G}$上的一个信号。在第$t$个时间戳，在图$\\mathcal{G_t}=(\\mathcal{V_t}, \\mathcal{\\varepsilon}, W)$, $\\mathcal{V_t}$是当顶点的有限集，对应在交通网络中$n$个监测站；$\\epsilon$是边集，表示观测站之间的连通性；$W \\in \\mathbb{R^{n \\times n}}$表示$\\mathcal{G_t}$的邻接矩阵。 图上的卷积传统网格上的标准卷积很明显是不能应用在广义图上的。现在有两个基本的方法正在探索如何泛化结构化数据上的CNN。一个是扩展卷积的空间定义[Niepert et al., 2016]，另一个是使用图傅里叶变换在谱域中进行操作[Bruna et al., 2013]。前一个方法重新将顶点安排至确定的表格形式内，然后就可以使用传统的卷积方法了。后者引入了谱框架，在谱域中应用图卷积，经常被称为谱图卷积。一些后续的研究通过将时间复杂度从$O(n^2)$降至线性[Defferrard et al., 2016;Kipf and Welling, 2016]使谱图卷积的效果更好。我们基于谱图卷积的定义引入图卷积操作“$\\ast_{\\mathcal{G}}$”的符号，也就是一个核$\\Theta$和信号$x \\in \\mathbb{R}^n$的乘法， $$\\tag{2} \\Theta \\ast_{\\mathcal{G}}x=\\Theta(L)x=\\Theta(U \\Lambda U^T)x=U\\Theta(\\Lambda)U^Tx$$ 这里图的傅里叶基$U \\in \\mathbb{R}^{n \\times n}$是归一化的拉普拉斯矩阵$L=I_n-D^{-1/2}WD^{-1/2}= U \\Lambda U^T \\in \\mathbb{R}^{n \\times n}$的特征向量组成的矩阵，其中$I_n$是单位阵，$D \\in \\mathbb{R}^{n \\times n}$是对角的度矩阵$D_{ii}=\\sum_j{W_{ij}}$；$\\Lambda \\in \\mathbb{R}^{n \\times n}$是$L$的特征值组成的矩阵，卷积核$\\Theta(\\Lambda)$是一个对角矩阵。通过这个定义，一个图信号$x$是被一个核$\\Theta$通过$\\Theta$和图傅里叶变换$U^Tx$[Shuman et al., 2013]过滤的。 提出的模型网络架构在这部分，我们详细说明了时空图卷积网络的框架。如图二所示，STGCN有多个时空卷积块组成，每一个都是像一个“三明治”结构的组成，有两个门序列卷积层和一个空间图卷积层在中间。每个模块的细节如下。 图二：时空图卷积网络的架构图。STGCN的架构有两个时空卷积块和一个全连接的在末尾的输出层组成。每个ST-Conv块包含了两个时间门卷积层，中间有一个空间图卷积层。每个块中都使用了残差连接和bottleneck策略。输入$v_{t-M+1},…v_t$被ST-Conv块均匀的（uniformly）处理，来获取时空依赖关系。全部特征由一个输出层来整合，生成最后的预测$\\hat{v}$。 提取空间特征的图卷积神经网络交通网络大体上是一个图结构。由数学上的图来构成路网是很自然也很合理的。然而，之前的研究忽视了交通网络的空间属性：因为交通网络被分成了块或网格状，所以网络的全局性和连通性被过分的关注了。即使是在网格上的二维卷积，由于数据建模的折中，也只能捕捉到大体的空间局部性。根据以上情况，在我们的模型中，图卷积被直接的应用在了图结构数据上为了在空间中抽取很有意义的模式和特征。集是在图卷积中由式2可以看出核$\\Theta$的计算的时间复杂度由于傅里叶基的乘法可以达到$O(n^2)$，两个近似的策略可以解决这个问题。 切比雪夫多项式趋近 为了局部化过滤器并且减少参数，核$\\Theta$可以被一个关于$\\Lambda$的多项式限制起来，也就是$\\Theta(\\Lambda)=\\sum_{k=0}^{K-1} \\theta_k \\Lambda^k$，其中$\\theta \\in \\mathbb{R}^K$是一个多项式系数的向量。$K$是图卷积核的大小，它决定了卷积从中心节点开始的最大半径。一般来说，切比雪夫多项式$T_k(x)$被用于近似核，作为$K-1$阶展开的一部分，也就是$\\Theta(\\Lambda) \\approx \\sum_{k=0}^{K-1} \\theta_k T_k(\\widetilde{\\Lambda})$，其中$\\widetilde{\\Lambda}=2\\Lambda/\\lambda_{max}-I_n$（$\\lambda_{max}$表示$L$的最大特征值）[Hammond et al., 2011]。图卷积因此可以被写成 $$\\tag{3} \\Theta \\ast_{\\mathcal{G}} x = \\Theta(L)x \\approx \\sum_{k=0}^{K-1}\\theta_k T_k(\\widetilde{L})x$$ 其中$T_k(\\widetilde{L}) \\in \\mathbb{R}^{n \\times n}$是k阶切比雪夫多项式对缩放后（scaled）的拉普拉斯矩阵$\\widetilde{L}=2L/\\lambda_{max}-I_n$。通过递归地使用趋近后的切比雪夫多项式计算K阶卷积操作，式2的复杂度可以被降低至$O(K\\vert \\varepsilon \\vert)$，如式3所示[Defferrard et al., 2016]。 1阶近似 一个针对层的线性公式可以由堆叠多个使用拉普拉斯矩阵的一阶近似的局部图卷积层[Kipf and Welling, 2016]。结果就是，这样可以构建出一个深的网络，这个网络可以深入地恢复空间信息并且不需要指定多项式中的参数。由于在神经网络中要缩放和归一化，我们可以进一步假设$\\lambda_{max} \\approx 2$。因此，式3可以简写为 $$\\begin{aligned}\\Theta \\ast_{\\mathcal{G}}x \\approx &amp; \\theta_0x+\\theta_1(\\frac{2}{\\lambda_{max}}L-I_n)x\\\\ \\approx &amp; \\theta_0 x- \\theta_1(D^{-\\frac{1}{2}} W D^{-\\frac{1}{2}}) x\\end{aligned}$$ 其中，$\\theta_0$，$\\theta_1$是核的两个共享参数。为了约束参数并为稳定数值计算，$\\theta_0$和$\\theta_1$用一个参数$\\theta$来替换，$\\theta=\\theta_0=-\\theta_1$；$W$和$D$是通过$\\widetilde{W}=W+I_n$和$\\widetilde{D}_{ii}=\\sum_j\\widetilde{W}_{ij}$重新归一化得到的。之后，图卷积就可以表达为 $$\\begin{aligned}\\Theta \\ast_{\\mathcal{G}} x = &amp; \\theta(I_n + D^{-\\frac{1}{2}} W D^{\\frac{1}{2}})x\\\\= &amp; \\theta (\\widetilde{D}^{-\\frac{1}{2}} \\widetilde{W} \\widetilde{D}^{-\\frac{1}{2}})x\\end{aligned}$$ 竖直地堆叠一阶近似的图卷积可以获得和平行的K阶卷积相同的效果，所有的卷积可以从一个顶点的$K-1$阶邻居中获取到信息。在这里，$K$是连续卷积操作的次数或是模型中的卷积层数。进一步说，针对层的线性结构是节省参数的，并且对大型的图来说是效率很高的，因为多项式趋近的阶数为1。 图卷积的泛化 图卷积操作$\\ast_{\\mathcal{G}}$也可以被扩展到多维张量上。对于一个有着$C_i$个通道的信号$X \\in \\mathbb{R}^{n \\times C_i}$，图卷积操作可以扩展为 $$y_j = \\sum_{i=1}^{C_i} \\Theta_{i,j}(L) x_i \\in \\mathbb{R}^n, 1 \\leq j \\leq C_o$$ 其中，$C_i \\times C_o$个向量是切比雪夫系数$\\Theta_{i,j} \\in \\mathbb{R}^K$（$C_i$，$C_o$分别是feature map的输入和输出大小）。针对二维变量的图卷积表示为$\\Theta \\ast_{\\mathcal{G}} X$，其中$\\Theta \\in \\mathbb{R}^{K \\times C_i \\times C_o}$。需要注意的是，输入的交通预测是由$M$帧路网组成的，如图1所示。每帧$v_t$可以被视为一个矩阵，它的第$i$列是图$\\mathcal{G_t}$中第$i$个顶点的一个为$C_i$维的值，也就是$X \\in \\mathbb{R}^{n \\times C_i}$（在这个例子中，$C_i=1$）。对于$M$中的每个时间步$t$，相同的核与相同的图卷积操作是在$X_t \\in \\mathbb{R}^{n \\times C_i}$中并行进行的。因此，图卷积操作也可以泛化至三维，记为$\\Theta \\ast_{\\mathcal{G}} \\mathcal{X}$，其中$\\mathcal{X} \\in \\mathbb{R}^{M \\times n \\times C_i}$ 抽取时间特征的门控卷积神经网络尽管基于RNN的模型可以广泛的应用于时间序列分析，用于交通预测的循环神经网络仍然会遇到费时的迭代，复杂的门控机制，对动态变化的响应慢。相反，CNN训练快，结构简单，而且不依赖于前一步。受到[Gehring et al., 2017]的启发，我们在时间轴上部署了整块的卷积结构，用来捕获交通流的动态时间特征。这个特殊的设计可以让并行而且可控的训练过程通过多层卷积结构形成层次表示。 如图2右侧所示，时间卷积层包含了一个一维卷积，核的宽度为$K_t$，之后接了一个门控线性单元(GLU)作为激活。对于图$\\mathcal{G}$中的每个顶点，时间卷积对输入元素的$K_t$个邻居进行操作，导致每次将序列长度缩短$K_t-1$。因此，每个顶点的时间卷积的输入可以被看做是一个长度为$M$的序列，有着$C_i$个通道，记作$Y \\in \\mathbb{R}^{M \\times C_i}$。卷积核$\\Gamma \\in \\mathbf{R}^{K_t \\times C_i \\times 2C_o}$是被设计为映射$Y$到一个单个的输出$[P Q] \\in \\mathbb{R}^{(M-K_t+1) \\times (2C_o)}$($P$, $Q$是通道数的一半)。作为结果，时间门控卷积可以定义为： $$\\Gamma \\ast_ \\tau Y = P \\otimes \\sigma (Q) \\in \\mathbb{R}^{(M-K_t+1) \\times C_o}$$ 其中，$P$, $Q$分别是GLU的输入门，$\\otimes$表示哈达玛积，sigmoid门$\\sigma(Q)$控制当前状态的哪个输入$P$对于发现时间序列中的组成结构和动态方差是相关的。非线性门通过堆叠时间层对挖掘输入也有贡献。除此以外，在堆叠时间卷积层时，实现了残差连接。相似地，通过在每个节点$\\mathcal{Y_i} \\in \\mathbb{R}^{M \\times C_i}$(比如监测站)上都使用同样的卷积核$\\Gamma$，时间卷积就可以泛化至3D变量上，记作$\\Gamma \\ast_\\tau \\mathcal{Y}$，其中$\\mathcal{Y} \\in \\mathbb{R}^{M \\times n \\times C_i}$。 这里我之前认为残差是用了 padding 的，其实不是，看了作者的代码后发现作者是用了一半数量的卷积核完成卷积，这样就和 P 的维度一致了，然后直接和 P 相加，然后与 sigmoid 激活后的值进行点对点的相乘。 时空卷积块为了同时从空间和时间领域融合特征，时空卷积块(ST-Conv block)的构建是为了同时处理图结构的时间序列的。如图2（中）所示，bottleneck策略的应用形成了三明治的结构，其中含有两个时间门控卷积层，分别在上下两层，一个空间图卷积层填充中间的部分。空间卷积层导致的通道数$C$的减小促使了参数的减少，并且减少了训练的时间开销。除此以外，每个时空块都使用了层归一化来抑制过拟合。 ST-Conv块的输入和输出都是3D张量。对于块$l$的输入$v^l \\in \\mathbb{R}^{M \\times n \\times C^l}$，输出$v^{l+1} \\in \\mathbb{R}^{(M-2(K_t-1)) \\times n \\times C^{l+1}}$通过以下式子计算得到： $$v^{l+1} = \\Gamma^l_1 \\ast_\\tau \\rm ReLU(\\Theta^l \\ast_{\\mathcal{G}}(\\Gamma^l_0 \\ast_\\tau v^l))$$ 其中$\\Gamma^l_0$，$\\Gamma^l_1$是块$l$的上下两个时间层；$\\Theta^l$是图卷积谱核；$\\rm ReLU(·)$表示ReLU激活函数。我们在堆叠两个ST-Conv块后，加了一个额外的时间卷积和全连接层作为最后的输出层（图2左侧）。时间卷积层将最后一个ST-Conv块的输出映射到一个最终的单步预测上。之后，我们可以从模型获得一个最后的输出$Z \\in \\mathbb{R}^{n \\times c}$，通过一个跨$c$个通道的线性变换$\\hat{v} = Zw+b$来预测$n$个节点的速度，其中$w \\in \\mathbb{R}^c$是权重向量,$b$是偏置。对交通预测的STGCN的损失函数可以写成： $$L(\\hat{v}; W_\\theta) = \\sum_t \\Vert \\hat{v}(v_{t-M+1, …, v_t, W_\\theta}) - v_{t+1} \\Vert^2$$ 其中，$W_\\theta$是模型中所有的训练参数; $v_{t+1}$是ground truth，$\\hat{v}(·)$表示模型的预测。 我们来总结一下我们的STGCN的主要特征： STGCN是处理结构化的时间序列的通用框架，不仅可以解决交通网络建模，还可以应用到其他的时空序列学习的挑战中，比如社交网络和推荐系统。 时空块融合了图卷积和门控时间卷积，可以同时抽取有用的空间信息，捕获本质上的时间特征。 模型完全由卷积层组成，因此可以在输入序列上并行运算，空间域中参数少易于训练。更重要的是，这个经济的架构可以使模型更高效的处理大规模的网络。 实验数据集描述我们在两个真实的数据集上验证了模型，分别是BJER4和PeMSD7，由北京市交委和加利福尼亚运输部提供。每个数据集包含了交通观测数据的关键属性和对应时间的地图信息。 BJER4是通过double-loop detector获取的东四环周边的数据。我们的实验中有12条道路。交通数据每五分钟聚合一次。时间是从2014年的7月1日到8月31日，不含周末。我们选取了第一个月的车速速度记录作为训练集，剩下的分别做验证和测试。 PeMSD7是Caltrans Performance Measurement System(PeMS)通过超过39000个监测站实时获取的数据，这些监测站分布在加州高速公路系统主要的都市部分[Chen et al., 2001]。数据是30秒的数据样本聚合成5分钟一次的数据。我们在加州的District 7随机选取了一个小的和一个大的范围作为数据源，分别有228和1026个监测站，分别命名为PeMSD7(S)和PeMSD7(L)（如图3左侧所示）。PeMSD7数据集的时间范围是2012年五月和六月的周末。我们使用同样的原则对数据进行了训练集和测试集的划分。 数据预处理两个数据集的间隔设定为5分钟。因此，路网中的每个顶点每天就有288个数据点。数据清理后使用了线性插值的方法来填补缺失值。通过核对相关性，每条路的方向和OD(origin-destination)点，环路系统可以被数值化成一个有向图。 在PeMSD7，路网的邻接矩阵通过交通网络中的监测站的距离来计算。带权邻接矩阵$W$通过以下公式计算： $$w_{ij} = \\begin{cases}\\exp{(-\\frac{d^2_{ij}}{\\sigma^2})}&amp;,i \\neq j \\ \\rm and \\exp{(-\\frac{d^2_{ij}}{\\sigma^2}) \\geq \\epsilon} \\\\0&amp;, \\rm otherwise\\end{cases}$$ 其中$w_{ij}$是边的权重，通过$d_{ij}$得到，也就是$i$和$j$之间的距离。$\\sigma^2$和$\\epsilon$是来控制矩阵$W$的分布和稀疏性的阈值，我们用了10和0.5。$W$的可视化在图3的右侧。 代码作者代码，这个是作者提供的代码。 仓库地址，我按照论文结合作者的代码进行了复现与修正。","link":"/blog/2018/05/10/spatio-temporal-graph-convolutional-networks-a-deep-learning-framework-for-traffic/"},{"title":"Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition","text":"AAAI 2018，以人体关节为图的顶点，构建空间上的图，然后通过时间上的关系，连接连续帧上相同的关节，构成一个三维的时空图。针对每个顶点，对其邻居进行子集划分，每个子集乘以对应的权重向量，得到时空图上的卷积定义。实现时使用Kipf &amp; Welling 2017的方法实现。原文链接：Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition 摘要人体骨骼的动态传递了用于人体动作识别的很多信息。传统方法需要手工和遍历规则，导致表现力的限制和泛化的困难。我们提出了动态骨骼识别的新模型，STGCN，可以从数据中自动学习时空模式。这套理论有很强的表达能力与泛化能力。在两个大型数据集Kinetics和NTU-RGBD上比主流方法表现的更好。 1 引言动作识别在视频理解中很有用。一般，从多个角度识别人体动作，如外表、景深、光源、骨骼。对骨骼建模受到的关注较外表和光源较少，我们系统的研究了这个模态，目的是研发出一个有效的对动态骨骼建模的方法，服务于动作识别。 动态骨骼模态很自然地表示成人体关节的时间序列，以2D或3D坐标的形式。人体动作可以通过分析移动规律来识别。早期的工作只用每帧的关节坐标生成特征向量，然后使用空间分析（Wang et al., 2012; Fernando et al., 2015）。这些方法能力受限的原因是他们没有挖掘关节之间的空间信息，但是这些信息对于理解人体动作来说很关键。最近，新的方法尝试利用关节间的自然连接关系(Shahroudy et al., 2016; Du, Wang, and Wang 2015)。这些方法都有提升，表明了连接的重要性。然而，很多显存的方法依赖手工的部分或是分析空间模式的规则。结果导致针对特定问题设计的模型不能泛化。 为了跨越这些限制，我们需要一个新的方法能自动捕获关节的空间配置与时间动态性中嵌入的模式。这就是深度神经网络的优势了。由于骨骼是图结构，不是2D或3D网格，因此传统的CNN不行，最近GCN已经成功的应用在了一些应用上，如图像分类(Bruna et al., 2014)，文档分类(Defferrard, Bresson, and Vandergheynst 2016)，还有半监督学习(Kipf and Welling 2017)。然而，这些工作都假设一个固定的图作为输入。GCN的应用在大尺度的数据集上对动态图建模，如人体骨骼序列还没有被挖掘过。 我们将图网络扩展到一个时空图模型来对骨骼序列进行表示后识别动作。图1所示，这个模型基于一个骨骼图序列，每个顶点表示人体的一个关节。有两种类型的边，空间边，连接关节，时间边连接连续时间的同一关节。构建在上面的时空卷积可以同时集成时间和空间上的信息。 ST-GCN的层次本质消除了手工和遍历部分。不仅有更强的表达能力和更好的表现，也更简单的泛化到其他环境中。在基础的GCN公式基础上，受到图像模型的启发，我们还提出了设计图卷积核新的策略。 我们的工作有三点贡献： 提出了ST-GCN，对动态骨骼建模的基于图结构的模型，第一个应用基于图的神经网络到这个任务上。 设计ST-GCN的卷积核时提出了几个原则使得在骨骼建模时满足特定的需求。 在两个大尺度数据集上，我们提出的模型效果比之前的手工和遍历规则的方法强。代码和模型：https://github.com/yysijie/st-gcn 2 相关工作两类方法 谱方法，谱分析中考虑图卷积的局部形式(Henaff, Bruna, and LeCun 2015; Duvenaud et al., 2015; Li et al., 2016; Kipf and Welling., 2017) 空间方法，卷积核直接在顶点和他们的邻居上做卷积(Bruna et al., 2014; Niepert, Ahmed, and Kutzkov 2016)。我们的工作follow了第二种方法。我们在空间领域构建CNN滤波器，通过限制滤波器到每个顶点的一阶邻居上。 骨骼动作识别 方法可分为手工方法和深度学习方法。第一类是手工设计特征捕获关节移动的动态性。可以是关节轨迹的协方差矩阵(Hussein et al., 2013)，关节的相对位置(Wang et al., 2012)，身体部分的旋转和变换(Vemulapalli, Arrate, and Chellappa 2014)。深度学习的工作使用RNN(Shahroudy et al., 2016; Zhu et al. 2016; Liu et al. 2016; Zhang, Liu and Xiao 2017)和时间CNN(Li et al. 2017; Ke et al. 2017; Kim and Reiter 2017)用端到端的方式学习动作识别模型。这些方法中很多强调了将关节与身体部分建模的重要性。但这些都需要领域知识。我们的ST-GCN是第一个将图卷积用在骨骼动作识别上的。不同于之前的方法，我们的方法可以利用图卷积的局部性和时间动态性学习身体部分的信息。通过消除手工部分标注的需要，模型更容易去设计，而且能学到更好的动作表示。 3 Spatial Temporal Graph ConvNet人们在活动的时候，关节只在一个范围内活动，这个部分称为body parts。已有的方法已经证明了将body parts融入到模型中是很有效的(Shahroudy et al., 2016; Liu et al., 2016; Zhang, Liu and Xiao 2017)。我们认为提升很有可能是因为parts将关节轨迹限制在了局部区域中。像物体识别这样的任务，层次表示和局部性通常是卷积神经网络潜在就可以获得的(Krizhevsky, Sutskever, and Hinton 2012)，而不是手动分配的。这使得我们在基于骨骼的动作识别中引入CNN的性质。结果就是ST-GCN模型的尝试。 3.1 Pipeline Overview骨骼数据通过动作捕捉设备和动作估计算法即可从视频中获得。通常数据是一系列的帧，每帧有一组关节坐标。给定身体关节2D或3D的坐标序列，我们构建了一个时空图，关节作图的顶点，身体结构或时间作边。ST-GCN的输入因此就是关节坐标向量。可以认为这是基于图片的CNN的近似，后者的输入是2D网格中的像素向量。多层时空图卷积操作加到输入上会生成更高等级的特征。然后使用softmax做费雷。整个模型以端到端的方式进行训练。 3.2 骨骼图构建骨骼序列通常表示成每帧都是人体关节的2D或3D坐标。之前使用卷积来做骨骼动作识别的工作(Kim and Reiter 2017)拼接了在每帧拼接了所有关节的坐标向量来生成一个特征向量。我们的工作中，我们利用时空图来生成骨骼序列的层次表示。特别地，我们构建了无向时空图$G = (V, E)$，$N$个关节，$T$帧描述身体内和帧与帧之间的连接。 顶点集$V = \\lbrace v_{ti} \\mid t = 1, …, T, i = 1, …, N \\rbrace$包含了骨骼序列中所有的关节。ST-GCN的输入，每个顶点的特征向量$F(v_{ti})$由第$t$帧的第$i$个关节的坐标向量组成，还有estimation confidence。构建时空图分为两步，第一步，一帧内的关节通过人体结构连接，如图1所示。然后每个关节在连续的帧之间连接起来。这里不需要人工干预。举个例子，Kinetics数据集，我们使用OpenPose toolbox(Cao et al., 2017b)2D动作估计生成了18个关节，而NTU-RGB+D(Shahroudy et al., 2016)数据集上使用3D关节追踪产生了25个关节。ST-GCN可以在这两种情况下工作，并且提供一致的优越性能。图1就是时空图的例子。严格来说，边集$E$由两个子集组成，第一个子集描述了每帧骨骼内的连接，表示为$E_S = \\lbrace v_{ti}v_{tj} \\mid (i, j) \\in H \\rbrace$，$H$是自然连接的关节的结合。第二个子集是连续帧的相同关节$E_F = \\lbrace v_{ti} v_{(t+1)i} \\rbrace$，因此$E_F$中所有的边对于关节$i$来说表示的是它随时间变化的轨迹。 3.3 空间图卷积神经网络时间$\\tau$上，$N$个关节顶点$V_t$，骨骼边集$E_S(\\tau) = \\lbrace v_{ti} v_{tj} \\mid t = \\tau, (i, j) \\in H \\rbrace$。图像上的2D卷积的输入和输出都上2D网格，stride设为1时，加上适当的padding，输出的size就可以不变。给定一个$K \\times K$的卷积操作，输入特征$f_{in}$的channels数是$c$。在空间位置$\\mathbf{x}$的单个通道的输出值可以写成： $$\\tag{1}f_{out}(\\mathbf{x}) = \\sum^K_{h=1} \\sum^K_{w=1} f_{in}(\\mathbf{p}(\\mathbf{x}, h, w)) \\cdot \\mathbf{w}(h, w)$$ 采样函数$\\mathbf{p} : Z^2 \\times Z^2 \\rightarrow Z^2$对$\\mathbf{x}$的邻居遍历。在图像卷积中，也可表示成$\\mathbf{p}(\\mathbf{x}, h, w) = \\mathbf{x} + \\mathbf{p}’(h, w)$。 权重函数$\\mathbf{w}: Z^2 \\rightarrow \\mathbb{R}^c$提供了一个$c$维的权重向量，与采出的$c$维输入特征向量做内积。需要注意的是权重函数与输入位置$\\mathbf{x}$无关。因此滤波器权重在输入图像上是共享的。图像领域标准的卷积通过对$\\mathbf{p}(x)$中的举行进行编码得到。更多解释和应用可以看(Dai et al., 2017)。 图上的卷积是对上式的扩展，输入是空间图$V_t$。feature map $f^t_{in}: V_t \\rightarrow R^c$在图上的每个顶点有一个向量。下一步扩展是重新定义采样函数$\\mathbf{p}$，权重函数是$\\mathbf{w}$。 采样函数. 图像中，采样函数$\\mathbf{p}(h, w)$定义为中心位置$\\mathbf{x}$的邻居像素。图中，我们可以定义相似的采样函数在顶点$v_{ti}$的邻居集合上$B(v_{ti}) = \\lbrace v_{tj} \\mid d(v_{tj}, v_{ti} \\leq D \\rbrace)$。这里$d(v_{tj}, t_{ti})$表示从$v_{tj}$到$v_{ti}$的任意一条路径中最短的。因此采样函数$\\mathbf{p}: B(v_{ti}) \\rightarrow V$可以写成：$$\\tag{2}\\mathbf{p}(v_{ti}, v_{tj}) = v_{tj}.$$我们令$D = 1$，也就是关节的一阶邻居。更高阶的邻居会在未来的工作中实现。 权重函数. 对比采样函数，权重函数在定义时更巧妙。在2D卷积，网格型自然就围在了中心位置周围。所以像素与其邻居有个固定的顺序。权重函数根据空间顺序通过对维度为$(c, K, K)$的tensor添加索引来实现。对于像我们构造的这种图，没有这种暗含的关系。解决方法由(Niepert, Ahmed, and Kuzkov 2016)提出，顺序是通过根节点周围的邻居节点的标记顺序确定。我们根据这个思路构建我们的权重函数。不再给每个顶点一个标签，我们通过将顶点$v_{ti}$的邻居集合$B(v_{ti})$划分为$K$个子集来简化过程，其中每个子集都有一个数值型标签。因此我们可以得到一个映射$l_{ti}:B(v_{ti}) \\rightarrow \\lbrace 0,…,K-1 \\rbrace$，这个映射将顶点映射到它的邻居子集的标签上。权重函数$\\mathbf{w}(v_{ti}, v_{tj}):B(v_{ti}) \\rightarrow R^c$可以通过对维度为$(c, K)$的tensor标记索引或$$\\tag{3}\\mathbf{w}(v_{ti}, v_{tj}) = \\mathbf{w}’(l_{ti}(v_{tj})).$$我们会在3.4节讨论分区策略。 空间图卷积. 我们可以将式1重写为：$$\\tag{4}f_{out}(v_{ti}) = \\sum_{v_{tj} \\in B(v_{ti})} \\frac{1}{Z_{ti}(v_{tj})} f_{in}(\\mathbf{p}(v_{ti}, v_{tj})) \\cdot \\mathbf{w}(v_{ti}, v_{tj}),$$其中归一化项$Z_{ti}(v_{tj}) = \\vert \\lbrace v_{tk} \\mid l_{ti}(v_{tk}) = l_{ti}(t_{tj}) \\rbrace \\vert$等于对应子集的基数。这项被加入是来平衡不同子集对输出的贡献。替换式2和式3，我们可以得到$$\\tag{5}f_{out}(v_{ti}) = \\sum_{v_{tj} \\in B(v_{ti})} \\frac{1}{Z_{ti}(v_{tj})} f_{in}(v_{tj}) \\cdot \\mathbf{w}(l_{ti}(v_{tj})).$$这个公式与标准2D卷积相似如果我们将图片看作2D网格。比如，$3 \\times 3$卷积核的中心像素周围有9个像素。邻居集合应被分为9个子集，每个子集有一个像素。 时空建模. 通过对空间图CNN的构建，我们现在可以对骨骼序列的时空动态性进行建模。回想图的构建，图的时间方面是通过在连续帧上连接相同的关节进行构建的。这可以让我们定义一个很简单的策略来扩展空间图CNN到时空领域。我们扩展邻居的概念到包含空间连接的关节：$$\\tag{6}B(v_{ti}) = \\lbrace v_{qj} \\mid d(v_{tj}, v_{ti}) \\leq K, \\vert q - t \\vert \\leq \\lfloor \\Gamma / 2 \\rfloor \\rbrace.$$参数$\\Gamma$控制被包含到邻居图的时间范围，因此被称为空间核的大小。我们需要采样函数来完成时空图上的卷积操作，与只有空间卷积一样，我们还需要权重函数，具体来说就是映射$l_{ST}$。因为空间轴是有序的，我们直接修改根节点为$v_{ti}$的时空邻居的标签映射$l_{ST}$为：$$\\tag{7}l_{ST}(v_{qj}) = l_{ti}(v_{tj}) + (q - t + \\lfloor \\Gamma / 2 \\rfloor) \\times K,$$其中$l_{ti}(v_{tj})$是$v_{ti}$的单帧的标签映射。这样，我们就有了一个定义在时空图上的卷积操作。 3.4 分区策略设计一个实现标记映射的分区策略很重要。我们探索了几种分区策略。简单来说，我们只讨论单帧情况下，因为使用式7就可以很自然的扩展到时空领域。 Uni-labeling. 最简单的分区策略，所有的邻居都是一个集合。每个邻居顶点的特征向量会和同一个权重向量做内积。事实上，这个策略和Kipf and Welling 2017提出的传播规则很像。但是有个很明显的缺陷，在单帧的时候使用这种分区策略就是将邻居的特征向量取平均后和权重向量做内积。在骨骼序列分析中不能达到最优，因为丢失了局部性质。$K = 1$，$l_{ti}(v_{tj}) = 0, \\forall{i, j} \\in V$。 Distance partitioning. 另一个自然的分区策略是根据顶点到根节点$v_{ti}$的距离$d(\\cdot, v_{ti})$来划分。我们设置$D = 1$，邻居集合会被分成两个子集，$d = 0$表示根节点子集，其他的顶点是在$d = 1$的子集中。因此我们有两个不同的权重向量，他们能对局部性质进行建模，比如关节间的相对变换。$K = 2$，$l_{ti}(v_{tj}) = d(v_{tj}, v_{ti})$ Spatial configuration partitioning. 因为骨骼是空间局部化的，我们仍然可以利用这个特殊的空间配置来分区。我们将邻居集合分为三部分：1. 根节点自己；2. 中心组：相比根节点更接近骨骼重心的邻居顶点；3. 其他的顶点。其中，中心定义为一帧中骨骼所有的关节的坐标的平均值。这是受到人体的运动大体分为同心运动和偏心运动两类。$$\\tag{8}l_{ti}(v_tj) = \\begin{cases}0 &amp; if r_j = r_i \\\\1 &amp; if r_j &lt; r_i \\\\2 &amp; if r_j &gt; r_i \\end{cases}$$其中，$r_i$是训练集中所有帧的重心到关节$i$的平均距离。分区策略如图3所示。我们通过实验检验提出的分区策略在骨骼动作识别上的表现。分区策略越高级，效果应该是越好的。 3.5 Learnable edge importance weighting尽管人在做动作时关节是以组的形式移动的，但一个关节可以出现在身体的多个部分。然而，这些表现在建模时应该有不同的重要性。我们在每个时空图卷积层上添加了一个可学习的mask$M$。这个mask会基于$E_S$中每个空间图边上可学习的重要性权重来调整一个顶点的特征对它的邻居顶点的贡献。通过实验我们发现增加这个mask可以提升ST-GCN的性能。使用注意力映射应该也是可行的，这个留到以后再做。 3.6 Implementation ST-GCN实现这个图卷积不像实现2D或3D卷积那样简单。我们提供了实现ST-GCN的具体细节。我们采用了Kipf &amp; Welling 2017的相似的实现方式。单帧内身体内关节的连接表示为一个邻接矩阵$\\rm{A}$，单位阵$\\rm{I}$表示自连接。在单帧情况下，ST-GCN使用第一种分区策略时可以实现为：$$\\tag{9}\\rmf_{out} = \\Lambda^{-\\frac{1}{2}}(A + I) \\Lambda^{-\\frac{1}{2}} f_{in}W,$$其中，$\\Lambda^{ii} = \\sum_j (A^{ij} + I^{ij})$。多个输出的权重向量叠在一起形成了权重矩阵$\\mathrm{W}$。实际上，在时空情况下，我们可以将输入的feature map表示为维度为$(C, V, T)$的tensor。图卷积通过一个$1 \\times \\Gamma$实现一个标准的2D卷积，将结果与归一化的邻接矩阵$\\rm \\Lambda^{-\\frac{1}{2}}(A + I)\\Lambda^{-\\frac{1}{2}}$在第二个维度上相乘。 对于多个子集的分区策略，我们可以再次利用这种实现。但注意现在邻接矩阵已经分解成了几个矩阵$A_j$，其中$\\rm A + I = \\sum_j A_j$。举个例子，在距离分区策略中，$\\rm A_0 = I$，$\\rm A_1 = A$。式9变形为$$\\tag{10}\\rm f_{out} = \\sum_j \\Lambda^{-\\frac{1}{2}}_j A_j \\Lambda^{\\frac{1}{2}}_j f_{in} W_j$$其中，$\\rm \\Lambda^{ii}_j = \\sum_k (A^{ik}_j) + \\alpha$。这里我们设$\\alpha = 0.001$避免$\\rm A_j$中有空行。 实现可学习的边重要性权重很简单。对于每个邻接矩阵，我们添加一个可学习的权重矩阵$M$，替换式9中的$\\rm A + I$和式10中的$\\rm A_j$中的$A_j$为$\\rm (A + I) \\otimes M$和$\\rm A_j \\otimes M$。这里$\\otimes$表示两个矩阵间的element-wise product。mask$M$初始化为一个全一的矩阵。","link":"/blog/2018/04/18/spatial-temporal-graph-convolutional-networks-for-skeleton-based-action-recognition/"},{"title":"Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting","text":"AAAI 2019，滴滴的网约车需求预测，5个点预测1个点。空间依赖建模上：以图的形式表示数据，从空间地理关系、区域功能相似度、区域交通连通性三个角度构造了三个不同的图，提出了多图卷积，分别用 k 阶 ChebNet 对每个图做图卷积，然后将多个图的卷积结果进行聚合(sum, average 等)成一个图；时间依赖建模上：提出了融合背景信息的 Contextual Gated RNN (CGRNN)，用 ChebNet 对每个结点卷积后，得到他们的邻居表示，即每个结点的背景信息表示，与原结点特征拼接，用一个两层全连接神经网络计算出 T 个权重，将权重乘到历史 T 个时刻的图上，对历史值进行缩放，然后用一个共享的 RNN，针对每个结点形成的长度为 T 的时间序列建模，得到每个结点新的时间表示。最后预测每个点的网约车需求。原文地址：Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting Abstract区域级别的预测是网约车服务的关键任务。精确地对网约车需求预测可以指导车辆调度、提高车辆利用率，减少用户的等待时间，减轻交通拥堵。这个任务的关键在于区域间复杂的时空依赖关系。现存的方法主要关注临近区域的欧式关系建模，但是在距离较远的区域间组成的非欧式关系对精确预测也很关键。我们提出了 spatiotemporal multi-graph convolution network (ST-MGCN)。我们首先将非欧的关系对编码到多个图中，然后使用 multi-graph convolution 对他们建模。为了在时间建模上利用全局的背景信息，我们提出了 contextual gated recurrent neural network，用一个注意背景的门机制对不同的历史观测值重新分配权重。在两个数据集上比当前的 state-of-the-art 强 10%。 Introduction我们研究的问题是区域级别网约车需求预测，是智能运输系统的重要部分。目标是通过历史观测值，预测一个城市里面各区域未来的需求。任务的挑战是复杂的时空关系。一方面，不同区域有着复杂的依赖关系。举个例子，一个区域的需求通常受其空间上临近的区域所影响，同时与有着相同背景的较远的区域有联系。另一方面，非线性的依赖关系也存在于不同的时间观测值之间。预测一个时刻通常和多个历史的观测值相关，比如一小时前、一天前、甚至一周前。 最近在深度学习的进步使得对基于区域级别的时空关系预测有了很好的结果。使用卷积神经网络和循环神经网络，得到了很多非常好的效果(Shi et al. 2015; Yu et al. 2017; Shi et al. 2017; Zhang, Zheng, and Qi 2017; Zhang et al. 2018a; Ma et al. 2017; Yao et al. 2018b; 2018a)。尽管有了很好的效果，但是我们认为在对时空关系建模上有两点被忽略了。其一，这些方法主要对不同区域的欧式关系建模，但是我们发现非欧关系很重要。图 1 是一个例子，对于区域 1，以及邻居区域 2，可能和很远的区域 3 有相似的功能，也就是他们都靠近学校和医院。此外，区域 1 还可能被区域 4 影响，区域 4 是通过高速公路直接与区域 1 相连的。其二：这些方法中，在使用 RNN 对时间关系建模时，每个区域是独立处理的，或者只基于局部信息。然而，我们认为全局和背景信息也很重要。举个例子，网约车需求的一个全局性的增长/减小通常表明一些可能会影响未来需求的活动发生了。 我们提出了 ST-MGCN 解决这些问题。在 ST-MGCN 中，我们提出了将区域间非欧关系编码进多个图的方法。不同于 Yao et al. 2018b 给每个区域使用图嵌入作为额外的不变特征，我们用图卷积对区域间的关系对直接建模。图卷积在预测的时候可以聚合邻居特征，传统的图嵌入难以做到这一点。此外，在对时间关系建模时，为了聚合全局的背景信息，我们提出了 contextual gated recurrent neural network (CGRNN)。通过一个基于全局信息计算的门机制增强 RNN，对不同时间步的观测值重新赋权重。我们在两个大型的真实数据集上做了测试，ST-MGCN 比 baselines 好了一大截。我们主要的贡献是： 识别了网约车需求预测中的非欧关系，将他们编码进多个图。利用多图卷积对这些关系建模。 对时间依赖，提出了 Contextual Gated RNN (CGRNN) 来集成全局背景信息。 在两个大型真实数据集上做了实验，提出的方法比 state-of-the-art 在相对误差上小了 10%. Related workSpatiotemporal prediction in urban computing时空预测是数据驱动的城市管理的基础问题。有很多关于这方面的工作，自行车流量预测(Zhang, Zheng, and Qi 2017)，出租车需求(Ke et al. 2017b; Yao et al. 2018b)，到达时间(Li et al. 2018b)，降雨量(Shi et al. 2015; 2017)，对矩形区域的聚合值进行预测，区域关系通过地理距离建模。具体来讲，城市数据的空间结构通过矩阵形式表示，每个元素表示一个矩形区域。在之前的工作中，区域和他们的关系对一般表示成欧式结构，使得卷积神经网络可以有效地利用这个结构来预测。 非欧结构的数据也存在于城市计算。通常，基于站点或点的预测任务，像流量预测(Li et al. 2018c; Yu, Yin, and Zhu 2018; Yao et al. 2018a)，基于点的出租车需求预测(Tong et al. 2017)以及基于站点的自行车流量预测(Chai, Wang, and Yang 2018)是很自然的非欧结构，数据不再是矩阵形式，卷积神经网络也不那么有效了。人工定制的特征工程或图卷积网络是处理非欧结构数据目前最好的方法。不同于之前的工作，ST-MGCN 将区域间的关系对编码进语义图中。尽管 ST-MGCN 是对基于区域的预测设计的，但是区域关系的非规整性使得它实际是对非欧数据进行预测。 在 (Yao et al. 2018b)，作者提出 DMVST-Net，将区域间关系编码进图中来预测出租车需求。DMVST-Net 主要使用图嵌入作为额外特征来预测，没有使用相关区域的需求值（目标值）。在 (Yao et al. 2018a) 的工作中，作者通过注意力机制对周期性的平移问题建模提升了性能。但是，这些方法都没有直接对区域间的非欧关系建模。我们的工作中，ST-MGCN 使用提出的多图卷积从相关区域聚合特征，从不同角度的相关区域的预测值中做预测。 最近在对帕金森的神经图像分析 (Zhang et al. 2018b) 的研究中，图卷积在空间特征提取上很有效。他们使用 GCN 从最相似的区域中学习特征，提出了多视图结构融合了不同的 MRI。然而，上述工作没有考虑时间依赖。ST-GCN 用于基于骨骼的动作识别(Li et al. 2018a; Yan, Xiong, and Lin 2018)。ST-GCN 的变换是一个空间依赖和局部时间循环的组合。然而，我们认为这些模型，在时间依赖建模上，背景信息或全局信息被忽略了。 Graph convolution network图卷积网络定义在图 $\\mathcal{G} = (V, \\boldsymbol{A})$ 上，$V$ 是顶点集，$\\boldsymbol{A} \\in \\mathbb{R}^{\\vert V \\vert \\times \\vert V \\vert}$ 是邻接矩阵，元素表示顶点间是否相连。GCN 可以用不同的感受野从不同的非欧结构中提取局部特征(Hammond et al. 2011)。令 $\\boldsymbol{L} = \\boldsymbol{I} - \\boldsymbol{D}^{-1/2} \\boldsymbol{A} \\boldsymbol{D}^{-1/2}$ 表示图拉普拉斯矩阵，$\\boldsymbol{D}$ 是度矩阵，图卷积操作 (Defferrard, Bresson, and Vandergheynst 2016) 定义为： $$\\boldsymbol{X}_{l+1} = \\sigma (\\sum^{K-1}_{k=0} \\alpha_k \\boldsymbol{L}^k \\boldsymbol{X}_l),$$ $\\boldsymbol{X}_l$ 表示第 $l$ 层的特征，$\\alpha_k$ 表示可学习的参数，$\\boldsymbol{L}^k$ 是图拉普拉斯矩阵的 $k$ 次幂，$\\sigma$ 是激活函数。 Channel-wise attentionChannel-wise attention (Hu, Shen, and Sun 2018; Chen et al. 2017) 在 cv 的论文中提出。本质是给每个通道学习一个权重，为了找到最重要的帧，然后基于他们更高的权重。$\\boldsymbol{X} \\in \\mathbb{R}^{W \\times H \\times C}$ 表示输入，$W$ 和 $H$ 是输入图像的维度，$C$ 表示通道数，channel-wise attention 计算方式如下： $$\\tag{1}z_c = F_{pool}(\\boldsymbol{X}_{:,:,c}) = \\frac{1}{WH} \\sum^W_{i=0} \\sum^H_{j=0} X_{i,j,c} \\quad \\text{for} c=1,2,\\dots,C \\\\\\boldsymbol{s} = \\sigma(\\boldsymbol{W}_2 \\delta (\\boldsymbol{W}_1 \\boldsymbol{z})) \\\\\\tilde{\\boldsymbol{X}}_{:,:,c} = \\boldsymbol{X}_{:,:,c} \\circ s_c \\quad \\text{for} c=1,2,\\dots,C$$ $F_{pool}$ 是全局池化操作，把每个通道聚合成一个标量 $\\boldsymbol{z}_c$，$c$ 是通道的下标。用一个注意力机制对聚合的向量 $\\boldsymbol{z}$ 使用非线性变换生成自适应的通道权重 $\\boldsymbol{s}$，$\\boldsymbol{W}_1, \\boldsymbol{W}_2$ 是对应的权重，$\\delta, \\sigma$ 是 ReLU 和 sigmoid 激活函数。$\\boldsymbol{s}$ 通过矩阵乘法乘到输入上。最后，输入通道基于学习到的权重得到了缩放。我们使用这个方法，针对一系列的图生成了时间依赖的注意力分数。 MethodologyRegion-level ride-hailing demand forecasting我们将城市分为相同大小的网格，每个格子定义为一个区域 $v \\in V$，$V$ 表示城市内所有不相交的区域。$\\boldsymbol{X}^{(t)}$ 表示第 $t$ 个时段所有区域的订单。区域级别的网约车需求预测 问题定义为：给定一个定长的输入，对单个时间步进行时空预测，也就是学习一个函数 $f: \\mathbb{R}^{\\vert V \\vert \\times T} \\rightarrow \\mathbb{R}^{\\vert V \\vert}$，将所有区域的历史需求映射到下一个时间步上。 $$[\\boldsymbol{X}^{(t-T+1)}, \\dots, \\boldsymbol{X}^{(t)}]$$ Framework overview ST-MGCN 的系统架构如图2。我们从不同的角度表示区域间的关系，顶点表示区域，边对区域间的关系编码。首先，我们使用提出的 CGRNN，考虑全局背景信息对不同时间的观测值进行聚合。然后，使用多图卷积捕获区域间不同类型的关系。最后，使用全连接神经网络将特征映射到预测上。 Spatial dependency modeling我们用图将区域间关系建模成三种类型，（1）邻居图 $\\mathcal{G}_N = (V, \\boldsymbol{A}_N)$，编码了空间相近程度，（2）功能相似度图 $\\mathcal{G}_F = (V, \\boldsymbol{A}_F)$，编码了区域的 POI 的相似度，（3）连接图 $\\mathcal{G}_T = (V, \\boldsymbol{A}_T)$，编码了距离较远的区域的连通性。我们的方法可以轻易地扩展到其他的图上。 Neighborhood 区域的邻居基于空间近邻程度定义。我们将 $3 \\times 3$ 区域中的最中间的那个区域与他邻接的 8 个区域相连。 $$\\tag{3}A_{N, ij} = \\begin{cases}1, \\quad v_i \\quad \\text{and} \\quad v_j \\quad \\text{are} \\quad \\text{adjacent}\\\\0, \\quad \\text{otherwise}\\end{cases}$$ Functional similarity 对一个区域做预测的时候，很自然的会想到和这个区域在功能上相似的区域会有帮助。区域功能可以由 POI 刻画，两个顶点间的边定义为 POI 的相似度： $$\\tag{3}A_{S,i,j} = \\text{sim}(P_{v_i}, P_{v_j}) \\in [0, 1]$$ 其中 $P_{v_i}, P_{v_j}$ 是区域 $v_i$ 和 $v_j$ 的 POI 向量，维度等于 POI 种类的个数，每个分量表示这个区域内这个 POI 类型的数量。 Transportation connectivity 运输系统也是一个重要因素。一般来说，这些空间距离上相距较远但是可以很方便到达的区域可以关联起来。这种连接包含高速公路、公路、地铁这样的公共运输。我们定义：如果两个区域间通过这些路直接相连，那么他们之间有边： $$\\tag{4}A_{C,i,j} = max(0, \\text{conn}(v_i, v_j) - A_{N,i,j}) \\in \\lbrace 0, 1\\rbrace$$ $\\text{conn}(u, v)$ 表示 $v_i$ 和 $v_j$ 之间的连通性。邻居的边在这个图中移除掉了，减少冗余的关系，所以这个图最后是一个稀疏图。 Multi-graph convolution for spatial dependency modeling 有了这些图，我们提出了多图卷积对空间关系建模： $$\\tag{5}\\boldsymbol{X}_{l+1} = \\sigma(\\bigsqcup_{\\mathbf{A} \\in \\mathbb{A}} f(\\mathbf{A; \\theta_i}) \\boldsymbol{X}_l \\mathbf{W}_l)$$ 其中 $\\boldsymbol{X}_l \\in \\mathbb{R}^{\\vert V \\vert \\times P_l}, \\boldsymbol{X}_{l+1} \\in \\mathbb{R}^{\\vert V \\vert \\times P_{l+1}}$ 是第 $l$ 和 $l+1$ 层的特征向量，$\\sigma$ 是激活函数，$\\bigsqcup$ 表示聚合函数，如 sum, max, average etc. $\\mathbb{A}$ 表示图的集合，$f(\\mathbf{A}; \\theta_i) \\in \\mathbb{R}^{\\vert V \\vert \\times \\vert V \\vert}$ 表示参数为 $\\theta_i$ 的基于图 $\\mathbf{A} \\in \\mathbb{A}$ 的不同样本组成的矩阵的聚合值，$\\mathbf{W}_l \\in \\mathbb{R}^{P_l \\times P_{l+1}}$ 表示特征变换矩阵，举个例子，如果 $f(\\mathbf{A}, \\theta_i)$ 是拉普拉斯矩阵 $\\mathbf{L}$ 的多项式，那么这就是多图上的 ChebNet。如果是 $\\mathbf{I}$，那就是全连接神经网络。 我们实现的是 $K$ 阶 拉普拉斯 $\\mathbf{L}$ 多项式，图 3 是一个中心区域通过图卷积层变换后的例子。假设邻接矩阵中的值不是 0 就是 1，$L^k_{ij} \\not = 0$ 表示 $v_i$ 在 $k$ 步内可达 $v_j$。根据卷积操作，$k$ 是空间特征提取时的感受野范围。使用图 1 的道路连通性图 $\\mathcal{G}_C = (V, \\boldsymbol{A}_C)$ 来说明。在邻接矩阵 $\\boldsymbol{A}_C$ 中，我们有： $$A_{C,1,4} = 1; A_{C,1,6} = 0; A_{C,4,6} = 1,$$ 在 1 度拉普拉斯矩阵中对应的分量是： $$L^1_{C,1,4} \\not = 0; L^1_{C,1,6} = 0; L^1_{C,4,6} \\not = 0$$ 如果拉普拉斯矩阵的最大度数 $K$ 设为 $1$，那么区域 1 变换的特征向量，即 $\\boldsymbol{X}_{l+1, 1,:}$ 不会包含区域 6: $\\boldsymbol{X}_{l,6,:}$，因为 $L^1_{C,1,6}=0$。当 $K$ 增大到 2 的时候，对应的元素 $L^2_{C,1,6}$ 变成非零，$\\boldsymbol{X}_{l+1,1,:}$ 就可以利用 $\\boldsymbol{X}_{l,6,:}$ 的信息了。 基于多图卷积的空间依赖建模不限于上述三种图，可以轻易地扩展到其他的图上，适用于其他的时空预测问题上。多图卷积对区域间的关系进行特征提取。感受野小的时候，专注于近邻的区域。增大拉普拉斯阶数，或者堆叠卷积层可以增加感受野的范围，鼓励模型捕获全局依赖关系。 图嵌入是另一种对区域间关系建模的方法。在 DMVST-Net (Yao et al. 2018b)，作者使用图嵌入表示区域间关系，然后将嵌入作为额外特征加到每个区域上。我们认为 ST-MGCN 中的空间依赖建模方法比之前的方法好，因为：ST-MGCN 将区域间关系编码到图中，通过图卷积从相关区域聚合需求值。但是在 DMVST-Net 中区域关系是嵌入到一个基于区域的不随时间变化的特征中，作为的模型的输入， 尽管 DMVST-Net 也捕获了拓扑结构信息，但是它很难从相关的区域中通过区域关系聚合需求值。而且不变的特征对模型训练的贡献有限。 Temporal correlation modeling我们提出 Contextual Gated Recurrent Neural Network (CGRNN) 对不同时间步上的样本建模。CGRNN 通过使用一个上下文注意的门机制增强 RNN 将背景信息集成到时间建模中，结构如图 4。假设我们有 $T$ 个观测样本，$\\boldsymbol{X}^{(t)} \\in \\mathbb{R}^{\\vert V \\vert \\times P}$ 表示第 $t$ 个样本，$P$ 是特征数，如果特征只包含订单数，那就是 1。上下文门控机制如下： $$tag{6}\\hat{\\boldsymbol{X}}^{(t)} = [\\boldsymbol{X}^{(t)}, F^{K’}_\\mathcal{G}(\\boldsymbol{X}^{(t)})] \\quad \\text{for} \\quad t = 1,2,\\dots,T$$ 首先，上下文门控机制通过将临近区域的历史信息和当前区域拼接，得到了区域的描述信息。从相邻区域来的信息看作是环境信息，通过图卷积 $F^{K’}_\\mathcal{G}$ 使用最大阶数为 $K’$ 的拉普拉斯矩阵提取。上下文门控机制用来用图卷积操作集成临近区域的信息，然后使用一个池化： $$\\tag{7}z^{(t)} = F_{pool}(\\hat{\\boldsymbol{X}}^{(t)}) = \\frac{1}{\\vert V \\vert} \\sum^{\\vert V \\vert}_{i=1} \\hat{X}^{(t)}_{i,:} \\quad \\text{for} \\quad t=1,2,\\dots,T$$ 然后，我们在所有的区域上使用全局平均池化 $F_{pool}$ 生成每个时间步观测值的平均值。 $$tag{8}\\boldsymbol{s} = \\sigma(\\boldsymbol{W}_2 \\delta(\\boldsymbol{W}_1) \\boldsymbol{z})$$ 然后使用一个注意力机制，$\\boldsymbol{W}_1, \\boldsymbol{W}_2$ 是参数，$\\delta, \\sigma$ 分别是 ReLU 和 sigmoid 激活。 $$\\tag{9}\\tilde{\\boldsymbol{X}^{(t)}} = \\boldsymbol{X}^{(t)} \\circ s^{(t)} \\quad \\text{for} \\quad t=1,2,\\dots,T$$ 最后，$\\boldsymbol{s}$ 用来对每个时间样本进行缩放： $$tag{10}\\boldsymbol{H}_{i,:} = \\text{RNN}(\\tilde{\\boldsymbol{X}}^{(1)}_{i,:}, \\dots, \\tilde{\\boldsymbol{X}}^{(T)}_{i,:}; \\boldsymbol{W}_3) \\quad \\text{for} \\quad i=1,\\dots,\\vert V \\vert$$ 在上下文门控之后，使用一个共享的 RNN 对所有的区域进行计算，将每个区域聚合成单独的向量 $\\boldsymbol{H}_{i,:}$。使用共享 RNN 的原因是我们想找到一个对所有区域通用的聚合规则，这个规则鼓励模型泛化且减少模型的复杂度。 ExperimentsDataset 北京和上海。时间是从2017年5月1日到2017年12月31日。5月1日到7月31日训练、8月1日到9月30日验证，剩下的测试。POI 数据是2017年的，包含13个类别。每个区域和一个 POI 向量相关，分量是这个 POI 类型在这个区域的个数。用来评估运输可达性的路网使用的是 OpenStreetMap (Haklay and Weber 2008)。 Experimental Settings学习任务是：$f: \\mathbb{R}^{\\vert V \\vert \\times T} \\rightarrow \\mathbb{R}^{\\vert V \\vert}$。实验中，我们将区域以 $1km \\times 1km$ 的大小划分成网格。北京和上海分别 1296 和 896 个区域。就像 Zhang, Zheng, and Qi 2017 做的那样，网络的输入包含 5 个历史观测值，三个最近邻的部分，1个周期部分，一个最新的趋势部分。在构建运输可达性网络的时候，我们考虑了高速公路、公路、地铁。两个区域间只要有这样的路直接相连就认为是连通的。 $f(\\mathbf{A}; \\theta_i)$ 选择的是 $K = 2$ 时的切比雪夫多项式，$\\bigsqcup$ 是 sum 函数。隐藏层为3，每层 64 个隐藏单元，L2 正则，weight decay 是 $1e-4$。CGRNN 中的图卷积 $K’$ 是 1。 我们使用 ReLU 作为图卷积的激活函数。ST-MGCN 的学习率是 $2e-3$，使用验证集上的早停。所有的算法都用 tf 实现，adam 优化 RMSE。ST-MGCN 训练时用了 10G 内存，9G GPU 显存。在 Tesla P40 单卡上训练了一个半小时。 Methods for evaluation HA, LASSO, Ridge, Auto-regressive model(VAR, STAR), Gradient boosted machine (GBM), ST-ResNet (Zhang, Zheng and Qi 2017), DMVST-Net (Yao et al. 2018b), DCRNN, ST-GCN。 Performance comparison我们在验证集上用网格搜索调整了所有模型的参数，在测试集上跑了多次得到的最后的结果。我们使用 RMSE 和 MAPE 作为评价指标。表 1 展示了不同方法在 10 次以上的预测中的对比结果。 我们在两个数据集上观测到了几个现象：（1）基于深度学习的方法能够对非线性的时空依赖关系建模，比其他的方法好；（2）ST-MGCN 在两个数据集上比其他的方法都好，比第二好的高出 10%；（3）对比其他的深度学习方法，ST-MGCN 的方差更小。 Effect of spatial dependency modeling为了研究空间和时间依赖建模的效果，我们通过减少模型中的组成部分评估了 ST-MGCN 的几个变体，包括：（1）邻居图，（2）功能相似性图，（3）运输连通性图。结果如表 2 所示。移除任何一个图都会造成性能损失，证明了每种关系的重要性。这些图编码了重要的先验知识，也就是区域间的相关性。 为了评估集成多个区域关系的效果，我们扩展了基于单个图的模型，包括 DCRNN 和 STGCN，分别记为 DCRNN+ 和 ST-GCN+。结果如图 3，两个算法都得到了提升。 Effect of temporal dependency modeling我们使用不同的方法对时间建模，评估 ST-GCN 对时间关系建模的效果。（1）平均池化：通过平均池化对历史观测值进行聚合，（2）RNN：使用 RNN 对历史观测值聚合，（3）CG：使用上下文门对不同的历史观测值赋权，不适用 RNN，（4）GRNN：不用图卷积的 CGRNN。结果如表 4。我们观察到了以下现象： 平均池化会盲目地平均不同的样本，导致性能下降，能做上下文依赖非线性时间聚合的 RNN 能显著地提升性能。 CGRNN 增强了 RNN。移除 RNN 和 图卷积都导致性能下降，证明了每个部件的有效性。 Effect of model parameters我们调整了两个最重要的参数来看不同参数对模型的影响，$K$ 和图卷积层数。图 5 展示了测试集上的结果。可以观察到随着层数的增加，错误先降后增。但是随着 $K$ 的增加，错误是先减小，后不变。越大的 $K$ 或层数使得模型能捕获全局关联性，代价是模型的复杂度会增加，更易过拟合。 Conclusion and Future work我们研究的是网约车需求预测，要找寻这个问题唯一的时空依赖关系。我们提出的深度学习模型使用多个图对区域间的非欧关系建模，使用多图卷积明显的捕获了这个关系。然后用上下文门控机制增强了 RNN，在时间建模上集成了全局背景信息。在两个大型真实数据集上评估了模型，比 state-of-the-art好。未来的工作是：（1）在其他的时空预测任务上评估模型；（2）将提出的模型扩展到多步预测上。","link":"/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/"},{"title":"Hadoop HA安装一：在Linux中安装和配置ntp，ssh和jdk","text":"Hadoop HA安装一：安装和配置ntp，ssh和jdk 记录一下安装5节点的高可用Hadoop集群的安装过程。High availability表示高可用，在Hadoop集群中，表示两个主节点。HDFS中是两个Namenode，Yarn中是两个ResourceManager。本教程中的Hadoop和HBase均为HA，MySQL为双机热备。 操作系统：CentOS 7. 软件服务如下： 软件 版本 路径 JDK 1.7.80 /usr/local/jdk1.7.0_80 MySQL 5.6.37 /usr/local/mysql Zookeeper 3.4.6 /usr/local/zookeeper-3.4.6 Kafka 0.8.2.1 /usr/local/kafka_2.10.-0.8.2.1 Hadoop 2.6.5 /usr/local/hadoop-2.6.5 HBase 1.2.6 /usr/local/hbase-1.2.6 Hive 1.1.0 /usr/local/apache-hive-1.1.0-bin MySQL JDBC 5.1.43 Scala 2.10.6 /usr/local/scala-2.10.6 Spark 1.6.3 /usr/local/spark-1.6.3-bin-hadoop2.6 Storm 1.1.1 /usr/local/apache-storm-1.1.1 Sqoop 1.4.6 /usr/local/sqoop-1.4.6 Pig 0.16.0 /usr/local/pig-0.16.0 各节点搭载的服务为： Hostname Services cluster1 NameNode, DataNode, ZKFC, ResourceManager, NodeManager, HMaster, HRegionServer, Master, Worker, MySQL cluster2 QuorumPeerMain, NameNode, DataNode, ZKFC, ResourceManager, NodeManager, HMaster, HRegionServer, Worker, MySQL cluster3 QuorumPeerMain, DataNode, NodeManager, HRegionServer, Worker, Kafka, nimbus, core, logviewer cluster4 QuorumPeerMain, DataNode, NodeManager, HRegionServer, Worker, Kafka, Supervisor, logviewer cluster5 DataNode, NodeManager, HRegionServer, Worker, Kafka, Supervisor, logviewer 本系列教程中，命令以#开头的是需要使用root用户执行，$开头的使用普通用户（一般为hadoop用户）。而且本教程的操作系统是CentOS7，有些配置文件内的内容可能与Ubuntu等系统不符。 关闭防火墙和Selinux// 关闭防火墙和selinux# systemctl stop firewalld.service // 禁止firewall 开机启动# systemctl disable firewalld.service // 开机关闭Selinux，编辑Selinux配置文件# vi /etc/selinux/config将SELINUX设置为disabled如下:SELINUX=disabled千万别把SELINUXTYPE改了！ // 重启# reboot // 重启机器后root用户查看Selinux状态# getenforce 修改hosts文件假设5台机器的IP地址分别为192.168.1.211-192.168.1.215每台机器都要做如下修改：// 修改hosts# vi /etc/hosts// 在最下面添加以下几行内容12345192.168.1.211 cluster1192.168.1.212 cluster2192.168.1.213 cluster3192.168.1.214 cluster4192.168.1.215 cluster5 修改成这个样子后，对于这台机器来说，cluster1就代表了192.168.1.211，其他的也同理。 ntp的安装与配置一个集群内需要有一个机器运行ntp server，其他机器用ntpdate向它同步时间。Hbase和Spark都要求时间是严格同步的，所以ntp是必需的。我们将ntp server设置在cluster1上，所以只在cluster1上面安装ntpserver，在其他机器上安装ntpdate。 ubuntu下使用如下命令安装12# apt-get install ntp# apt-get install ntpdate CentOS使用12# yum install ntp# yum install ntpdate 配置时间服务器： // cluster1上执行以下操作# vi /etc/ntp.conf 注释掉以下4行，也就是在这4行前面加#1234server 0.centos.pool.ntp.org iburstserver 1.centos.pool.ntp.org iburstserver 2.centos.pool.ntp.org iburstserver 3.centos.pool.ntp.org iburst 最下面加入以下内容，192.168.1.1和255.255.255.0分别为网关和掩码，127.127.1.0表示以本机时间为标准。123restrict default ignorerestrict 192.168.1.1 mask 255.255.255.0 nomodify notrapserver 127.127.1.0 保存后ubuntu使用# /etc/init.d/ntp restart重启ntp服务，CentOS使用# service ntpd restart除了搭载ntp server的主机，其他所有机器，设定每天00:00向ntp server同步时间，并写入日志# crontab –e添加以下内容10 0 * * * /usr/sbin/ntpdate cluster1&gt;&gt; /home/hadoop/ntpd.log 这样就完成了ntp的配置 // 手动同步时间，需要在每台机器上（除ntp server），使用ntpdate cluster1同步时间# ntpdate cluster1 新建hadoop用户每台机器上都要新建hadoop用户，这个用户专门用来维护集群，因为实际中使用root用户的机会很少，而且不安全。// 新建hadoop组# groupadd hadoop // 新建hadoop用户# useradd -s /bin/bash -g hadoop -d /home/hadoop -m hadoop // 修改hadoop这个用户的密码# passwd hadoop ssh密钥的生成与分发ssh是Linux自带的服务，不需要安装。这里的目的是让节点间实现无密码登陆。其实就是当前机器生成密钥，然后用ssh-copy-id复制到其他机器上，这样这台机器就可以无密码直接登陆那台机器了。Hadoop主节点需要能无密码连接到其他的机器上。 在cluster1上，使用hadoop用户123456789101112131415161718192021222324252627282930313233// 使用hadoop用户# su hadoop// 切到home目录$ cd ~/// 生成密钥ssh-keygen -t rsa// 一路回车//复制密钥$ ssh-copy-id cluster1yes输入cluster的密码$ ssh-copy-id cluster2同上$ ssh-copy-id cluster3同上$ ssh-copy-id cluster4同上$ ssh-copy-id cluster5同上// 然后测试能否无密码登陆$ ssh cluster1$ ssh cluster2$ ssh cluster3$ ssh cluster4$ ssh cluster5 查看登陆时是否有密码，若无密码，则配置成功。以上步骤需要在cluster2上也执行一遍，为了让cluster2也可以无密码登陆到其他机器上，因为cluster2也是主节点。 jdk的安装与配置安装hadoop集群，jdk是必须要装的，1.7和1.8都可以，不过从Hadoop3开始，好像只支持1.8+了，但是换成1.9和1.10会出问题，所以推荐用1.8，我这里用的是1.7。 将下载好后的jdk解压到/usr/local/下# vi /etc/profile将下面4行添加到环境变量中1234export JAVA_HOME=/usr/local/jdk1.7.0_80 export JRE_HOME=/usr/local/jdk1.7.0_80/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$JAVA_HOME:$PATH 使用# source /etc/profile刷新环境变量使用# java -version查看java版本验证是否安装成功，如果能看到Java的版本，说明安装成功，没有问题。","link":"/blog/2017/08/21/ssh-ntp-jdk的安装/"},{"title":"STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting","text":"IJCAI 2019. 原文链接：STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting Abstract多步乘客需求预测对于按需车辆共享服务来说是个重要的任务。然而，预测多个时刻的乘客需求由于时空依赖的非线性和动态性很有挑战。我们提出了基于图的城市范围的旅客需求预测模型，使用一个层次图卷积同时捕获空间和时间关联性。我们的模型有三部分：1) 长期编码器对历史旅客需求编码；2) 短期编码器推导下一步预测结果来生成多步预测；3) 使用一个基于注意力的输出模块对动态的时间和各通道信息建模。实验在三个数据集上表明我们的方法比很多方法好。 1. Introduction2. Notations and Problem Statement假设一个城市分成 $N$ 个小的区域，不考虑是分成网格还是路网。我们将区域的集合表示为 $\\lbrace r_1, r_2, \\dots, r_i, \\dots r_N \\rbrace$。在每个时间步 $t$，一个二维矩阵 $\\boldsymbol{D_t} \\in \\mathbb{R}^{N \\times d_{in}}$ 表示所有区域在时间 $t$ 的旅客需求。另一个向量 $\\boldsymbol{E_t} \\in \\mathbb{R}^{d_e}$ 表示时间步 $t$ 的时间特征，包含了几点、星期几以及节假日的信息。 给定城市范围的历史旅客需求序列 $\\lbrace \\bm{D_0}, \\bm{D_1}, \\dots, \\bm{D_t} \\rbrace$ 和时间特征 $\\lbrace \\bm{E_0}, \\bm{E_1}, \\dots, \\bm{E_{t+\\tau}} \\rbrace$，目标是学习一个预测函数 $\\Gamma(\\cdot)$ 来预测接下来的 $\\tau$ 个时间步上城市范围的旅客需求序列。我们只使用历史 $h$ 个时间步的需求序列作为输入 $\\lbrace \\bm{D_{t-h+1}, \\bm{D_{t-h+2}}, \\dots, \\bm{D_t}} \\rbrace$。我们的任务描述为： $$\\tag{1}(\\bm{D_{t+1}}, \\bm{D_{t+2}}, \\dots, \\bm{D_{t+\\tau}}) = \\Gamma(\\bm{D_{t-h+1}}, \\bm{D_{t-h+2}}, \\dots, \\bm{D_t}; \\bm{E_0}, \\bm{E_1}, \\dots, \\bm{E_{t+\\tau}})$$ 3. MethodologySTG2Seq 的架构有三个组件：1. 长期编码器，2. 短期编码器，3.基于注意力的输出模块。长期和短期编码器由多个序列时空门控图卷积模块 (GGCM) 组成，通过在时间维度使用 GCN 可以同时捕获时间和空间相关性。 3.1 Passenger Demand on Graph我们先介绍如何将城市范围的旅客需求在图上描述出来。之前的工作假设一个区域的旅客需求会被近邻的区域影响。然而，我们认为空间关系并不是仅依赖空间位置。如果遥远的区域和当前区域有相似的地方，比如具有相似的 POI，那么也可能拥有相同的旅客需求模式。因此，我们将城市看作一个图 $G = (v, \\xi, A)$，$v$ 是区域的集合 $v = \\lbrace r_i \\mid i=1,2,\\dots,N \\rbrace$，$\\xi$ 表示边的集合，$A$ 是邻接矩阵。我们根据区域间旅客需求模式的相似性定义图的边。 $$\\tag{2}A_{i, j} = \\begin{cases}1, \\quad \\text{if} \\quad Similarity_{r_i, r_j} &gt; \\epsilon\\\\0, \\quad \\text{otherwise}\\end{cases}$$ 其中 $\\epsilon$ 是阈值，控制 $A$ 的稀疏程度。为了定量区域间的旅客需求模式的相似性，我们使用皮尔逊相关系数。$D_{0\\text{\\textasciitilde}t}(r_i)$ 表示时间从 0 到 $t$ 的区域 $r_i$ 历史旅客需求序列。$r_i$ 和 $r_j$ 之间的相似度可以定义为： $$\\tag{3}Similarity_{r_i, r_j} = Pearson(D_{0\\text{\\textasciitilde}t}(r_i), D_{0\\text{\\textasciitilde}t}(r_j))$$ 3.2 Long-term and Short-term Encoders很多之前的工作只考虑下一步预测，即预测下一时间步的旅客需求。在训练过程中通过减少下一时间步预测值的误差而不考虑后续时间步的误差来优化模型。因此，这些方法在多步预测的问题上会退化。仅有一些工作考虑了多步预测的问题 [Xingjian et al., 2015; Li et al., 2018]。这些工作采用了基于 RNN 的编码解码器的架构，或是它的变体，比如 ConvLSTM 这样的作为编码解码器。这些方法有两个劣势：1. 链状结构的 RNN 在编码的时候需要遍历输入的时间步。因此他们需要与输入序列等长的 RNN 单元个数（序列多长，RNN单元就有多少个）。在目标需求和前一个需求上的长距离计算会导致一些信息的遗忘。2. 在解码部分，为了预测时间步 $T$ 的需求，RNN 将隐藏状态和前一时间步 $T-1$ 作为输入。因此，前一时间步带来的误差会直接影响到预测，导致未来时间步误差的累积。 不同于之前所有的工作，我们引入了一个依赖于同时使用长期和短期编码器的架构，不用 RNN 做多步预测。长期编码器取最近的 $h$ 个时间步的城市历史旅客需求序列 $\\lbrace \\bm{D_{t-h+1}}, \\bm{D_{t-h+2}}, \\dots, \\bm{D_t} \\rbrace$ 作为输入来学习历史的时空模式。这 $h$ 步需求合并后组织成三维矩阵，$h \\times N \\times d_{in}$。长期编码器由一些 GGCM 组成，每个 GCGGM 捕获在所有的 $N$ 个区域上捕获空间关联性，在 $k$ 个时间步上捕获时间关联性。$k$ 是超参数，我们会在 3.3 节讨论。因此，只需要 $\\frac{h-1}{k-1}$ 个迭代的步数就可以捕获 $h$ 个时间步上的时间关联性。对比 RNN 结构，我们的基于 GGCM 的长期编码器显著的降低了遍历长度，进一步减少了信息的损失。长期编码器的输出 $Y_h$ 的维数是 $h \\times N \\times d_{out}$，是输入的编码表示。 短期编码器用来集成已经预测的需求，用于多步预测。它使用一个长度为 $q$ 的滑动窗来捕获近期的时空关联性。当预测在 $T(T \\in [t+1,t+\\tau])$ 步的旅客需求时，它取最近的 $q$ 个时间步的旅客需求，即 $\\lbrace \\bm{D_{T-q}}, \\bm{D_{T-q+1}}, \\dots, \\bm{D_{T-1}} \\rbrace$ 作为输入。除了时间步的长度以外，短期编码器和长期编码器一样。短期编码器生成一个维数为 $q \\times N \\times d_{out}$ 的矩阵 $Y^T_q$ 作为近期趋势表示。和基于 RNN 的解码器不同的是，RNN的解码器只将最后一个时间步的预测结果输入回去。因此，预测误差会被长期编码器小柔，减轻基于 RNN 的解码器会导致误差累积的问题。 3.3 Gated Graph Convolutional Module门控图卷积模块是长期编码器和短期编码器的核心。每个 GGCM 由几个 GCN 层组成，沿着时间轴并行。为了捕获时空关联性，每个 GCN 在一定长度的时间窗内操作($k$)。它可以提取 $k$ 个时间步内所有区域的空间关联性。通过堆叠多个 GGCM，我们的模型形成了一个层次结构，可以捕获整个输入的时空关联性。图 3 展示了只使用 GCN 捕获时空关联性，为了简化我们忽略了通道维。Yu et al., 2018 的工作和我们的 GGCM 模块很像。他们的工作首先使用 CNN 捕获时间关联性，然后使用 GCN 捕获空间关联性。我们的方法对比他们的方法极大的简化了，因为我们可以同时捕获时空关联性。 GGCM 模块的详细设计如图 4。第 $l$ 个 GGCM 的输入是一个矩阵，维数为 $h \\times N \\times C^l$。在第一个 GGCM 模块，$C^l$ 是 $d_{in}$ 维的。第 $l$ 个 GGCM 的输出是 $h \\times N \\times C^{l+1}$。我们先拼接一个 zero padding，维数为 $(k-1) \\times N \\times C^l$，得到新的输入 $(h+k-1) \\times N \\times C^l$，确保变换不会减少序列的长度。接下来，GGCM 中的每个 GCN 取 $k$ 个时间步的数据 $k \\times N \\times C^l$ 作为输入来提取时空关联性，然后 reshape 成一个二维矩阵 $N \\times (k \\cdot C^l)$。根据 Kipf &amp; Welling 的 GCN，GCN 层可以描述如下： $$\\tag{4}X^{l+1} = (\\tilde{P}^{-\\frac{1}{2}} \\tilde{A} \\tilde{P}^{-\\frac{1}{2}}) X^l W$$ $\\tilde{A} = A + I_n$，$\\tilde{P}_{ii} = \\sum_j \\tilde{A}_{ij}$，$X \\in \\mathbb{R}^{N \\times (k \\cdot C^l)}$，$W \\in \\mathbb{R}^{(k \\cdot C^l) \\times C^{l+1}}$，$X^{l+1} \\in \\mathbb{R}^{N \\times C^{l+1}}$。 除此以外，我们使用了门控机制对旅客需求预测的复杂非线性建模。式 4 重新描述如下： $$\\tag{5}X^{l+1} = ((\\tilde{P}^{-\\frac{1}{2}} \\tilde{A} \\tilde{P}^{-\\frac{1}{2}}) X^l W_1 + X^l) \\otimes \\sigma((\\tilde{P}^{-\\frac{1}{2}} \\tilde{A} \\tilde{P}^{-\\frac{1}{2}}) X^l W_2)$$ $\\otimes$ 是对应元素相乘，$\\sigma$是 sigmoid 激活函数。因此输出是一个非线性门 $\\sigma((\\tilde{P}^{-\\frac{1}{2}} \\tilde{A} \\tilde{P}^{-\\frac{1}{2}}) X^l W_2)$ 控制的线性变换 $((\\tilde{P}^{-\\frac{1}{2}} \\tilde{A} \\tilde{P}^{-\\frac{1}{2}}) X^l W_1 + X^l)$。非线性门控制线性变换的哪个部分可以通过门影响预测。此外，我们使用残差连接来避免式 5 中的网络退化。 最后，门控机制产生的 $h$ 个输出沿时间轴合并，生成 GGCM 模块的输出 $h \\times N \\times C^{l+1}$。 3.4 Attention-based Output Module如 3.2 描述的那样，长期时空依赖和 $T$ 时间步的近期时空依赖通过两个矩阵描述 $Y_h$ 和 $Y^T_q$。我们拼接。我们拼接他们形成联合表示 $Y_{h+q} \\in \\mathbb{R}^{(h+q) \\times N \\times d_{out}}$，通过一个基于注意力机制的模块解码获得预测值。这里为了简便忽略 $T$。$Y_{h+q}$ 的三个轴分别是时间、空间、通道。 我们先引入一个时间注意力机制来解码 $Y_{h+q}$。旅客需求是一个典型的时间序列，前一时刻的需求对后一时刻有影响。然而，之前的每一步对预测目标的影响是不同的，影响随时间变化。我们设计了一个时间注意力机制对每个历史时间步增加注意力分数衡量其影响。分数通过 $Y_{h+q} = y_1, y_2, \\dots, y_{h+q}$ 和目标时间步的时间特征 $\\bm{E}_T$ 生成，这个分数可以自适应地学习之前的时间步随时间的动态影响。我们定义时间注意力分数如下： $$\\tag{6}\\bm{\\alpha} = softmax(tanh(Y_{h+q} W^Y_3 + E_T W^E_4 + b_1))$$ $W^Y_3 \\in \\mathbb{R}^{(h+q) \\times (N \\times d_{out}) \\times 1}$，$W^E_4 \\in \\mathbb{R}^{d_e \\times (h+q)}$，$b_1 \\in \\mathbb{R}^{(h+q)}$。联合表示 $Y_{h+q}$ 通过注意力分数 $\\bm{\\alpha}$ 转换： $$\\tag{7}Y_{\\alpha} = \\sum^{h+q}_{i=1} \\alpha^i y_i \\quad \\in \\mathbb{R}^{N \\times d_{out}}$$ 受到 [Chen et al., 2017] 的启发，每个通道的重要性是不同的，我们在时间注意力后面加了一个通道注意力模块来找到 $Y_\\alpha = [y_1, y_2, \\dots, y_{d_{out}}]$ 中最重要的那个。计算如下： $$\\tag{8}\\bm\\beta = softmax(tanh(Y_\\alpha W^Y_5 + E_T W^E_6 + b_2))$$ $$\\tag{9}Y_{\\beta} = \\sum^{d_{out}}_{i=1} \\beta^i y_i \\quad \\mathbb{R}^N$$ 其中，$W^Y_5 \\in \\mathbb{R}^{d_{out} \\times N \\times 1}$，$W^E_6 \\in \\mathbb{R}^{d_e \\times d_{out}}$；$\\bm\\beta \\in \\mathbb{R}^{d_{out}}$ 是每个通道的注意力分数。当预测的维度是1时，$Y_\\beta$ 就是我们预测的旅客需求 $\\bm{D’_T}$。当预测维度是 2 时（预测起止需求），我们给每个通道计算注意力分数，将他们拼接起来得到最后的预测值。","link":"/blog/2019/07/12/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/"},{"title":"Structured Sequence Modeling With Graph Convolutional Recurrent Networks","text":"ICLR 2017(reject)，两个模型，第一个是将数据扔到Defferrard的图卷积里面，然后将输出扔到LSTM里面。第二个模型是将RNN中的矩阵乘法换成了图卷积操作，最后对动态的mnist进行了识别。原文链接：Structured Sequence Modeling With Graph Convolutional Recurrent Networks 摘要GCRN(Graph Convolutional Recurrent Network)，一个可以预测结构化序列数据的深度学习模型。GCRN是传统的循环神经网络的在任意的图结构上的一种泛化形式。这样的结构化数据可以表示成视频中的一系列帧，检测器组成的网络监测到的时空监测值，或是用于自然语言建模的词网中的随机游走。我们提出的模型合并了图上的CNN来辨识空间结构，RNN寻找动态模型。我们研究了两种GCRN，对Penn Treebank数据集进行建模。实验显示同时挖掘图的空间与动态信息可以同时提升precision和学习速度。 1 Introduction很多工作，Donahue et al. 2015; Karpathy &amp; Fei-Fei 2015; Vinyals et al. 2015，利用CNN和RNN的组合来挖掘时空规律性。这些模型那个处理时间变化的视觉输入来做变长的预测。这些网络架构由视觉特征提取的CNN，和一个在CNN后面，用于序列学习的RNN组成。这样的架构成功地用于视频活动识别，图像注释生成以及视频描述。 最近，大家开始对时空序列建模时融合CNN和RNN感兴趣。受到语言模型的启发，Ranzato et al. 2014提出了通过发现时空相关性的能表示复杂变形和动作模式的模型。他们的实验表明在通过quantizing the image patches获得到的visual words上，使用RNN建模，可以很好的预测视频的下一帧以及中间帧。他们的表现最好的模型是recursive CNN(rCNN)，对输入和状态同时使用卷积。Shi et al. 2015之后提出了卷积LSTM(convLSTM)，一个使用2D卷积利用输入数据的空间相关性，用于时空序列建模的RNN模型。他们成功的对降雨临近预报的雷达回波图的演化进行了预测。 很多重要问题中，空间结构不是简单的网格状。气象站就不是网格状。而且空间结构不一定是空间上的，如社交网络或生物网络。最后，Mikolov et al. 2013等人认为，句子可以解释成在词网上的随机游走，使得我们转向了分析图结构的句子建模问题。 我们的工作利用了近期的模型——Defferrard et al. 2016; Ranzato et al. 2014; Shi et al. 2015——来设计GCRN模型对时间变化的图结构数据建模和预测。核心思想是融合图结构上的CNN和RNN来同时辨识空间结构和动态模式。图1给出了GCRN的架构。 2 Preliminaries2.1 Structured Sequence Modeling序列建模是给定前$J$个观测值，对未来最可能的长度为$K$的序列进行预测：$$\\tag{1}\\hat{x}_{t+1},…,\\hat{x}_{t+K} = \\mathop{\\mathrm{argmax}}\\limits_{x_{t+1},…,x_{t+K}}P(x_{t+1},…,x_{t+K} \\mid x_{t-J+1},…,x_t),$$$x_t \\in \\mathbf{D}$是时间$t$的观测值，$\\mathbf{D}$表示观测到的特征的域。原型应用是$n-\\mathrm{gram}$模型$(n = J + 1)$，$P(x_{t+1} \\mid x_{t-J+1},…,x_t)$对在句子中给定过去$J$个词时$x_{t+1}$出现的概率进行建模。 我们感兴趣的是特别的结构化的句子，也就是句子中$x_t$的特征不是相互独立的，而是有着两两相连的关系。这样的关系广义上通过带权图建模。 $x_t$可以看作是一个图信号，也就是一个定义在无向带权图$\\mathcal{G} = ( \\mathcal{V}, \\Large{\\varepsilon}, \\normalsize{A )}$，其中$\\mathcal{V}$是$\\vert \\mathcal{V} \\vert = n$个顶点的有限集，$\\Large{\\varepsilon}$是边集，$A \\in \\mathbb{R}^{n \\times n}$是带权邻接矩阵，编码了两个顶点之间的连接权重。定义在图的顶点上的信号$x_t: \\mathcal{V} \\rightarrow \\mathbb{R}^{d_x}$可以当作是一个矩阵$x_t \\in \\mathbb{R}^{n \\times d_x}$，列$i$是$d_x$维向量，表示$x_t$在第$i$个顶点的值。尽管自由变量的数量在长度为$K$的结构化序列中本质上是$\\mathcal{O}(n^K{d_x}^K)$，我们仍然试图去挖掘可能的预测结果的空间结构以减少维度，来使这些问题变得容易解决。 2.2 Long Short-Term Memory防止梯度过快消失，由Hochreiter &amp; Schmidhuber 1997发明的一种RNN，LSTM。这个模型已经被证明在各种序列建模任务中，对长期依赖关系是稳定且强劲的模型(Graves, 2013; Srivastava et al., 2015; Sutskever et al., 2014)。全连接LSTM(FC-LSTM)可以看作是一个多变量版本的LSTM，其中$x_t \\in \\mathbb{R}^d_x$是输入，$h_t \\in [-1, 1]^{d_h}$是细胞状态，$c_t \\in \\mathbb{R}^{d_h}$是隐藏状态，他们都是向量。我们使用Graves 2013的FC-LSTM：$$\\tag{2}i = \\sigma(W_{xi} x_t + W_{hi}h_{t-1} + w_{ci} \\odot c_{t-1} + b_i),\\\\f = \\sigma(W_{xf} x_t + W_{hf} h_{t-1} + w_{cf} \\odot c_{t-1} + b_f),\\\\c_t = f_t \\odot c_{t-1} + i_t \\odot \\mathrm{tanh}(W_{xc} x_t + W_{hc} h_{t-1} + b_c),\\\\o = \\sigma(W_{xo} x_t + W_{ho} h_{t-1} + w_{co} \\odot c_t + b_o),\\\\h_t = o \\odot \\mathrm{tanh}(c_t),$$其中$\\odot$表示Hadamard product，$\\sigma(\\cdot)$表示sigmoid function $\\sigma(x) = 1/(1+e^{-x})$，$i,f,o \\in [0, 1]^{d_h}$是输入门，遗忘门，输出门。权重$W_{x\\cdot} \\in \\mathbb{R}^{d_h \\times d_x}$，$W_{h\\cdot} \\in \\mathbb{R}^{d_h \\times d_h}$，$w_{c\\cdot} \\in \\mathbb{R}^{d_h}$，偏置$b_i,b_f,b_c,b_o \\in \\mathbb{R}^{d_h}$是模型参数。这个模型之所以称为全连接是因为$W_{x\\cdot}$和$W_{h\\cdot}$与$x$和$h$所有分量进行线性组合。由Gers &amp; Schmidhuber 2000引入可选的peephole connections $w_{c\\cdot} \\odot c_t$，在某些特定任务上可以提升性能。 2.3 Convolutional Neural Networks On GraphsDefferrard et al., 2016选择了谱上的卷积操作：$$\\tag{3}y = g_\\theta \\ast_\\mathcal{G} x = g_\\theta (L)x = g_\\theta (U \\Lambda U^T)x = U g_\\theta (\\Lambda) U^T x \\in \\mathbb{R}^{n \\times d_x},$$对于归一化的拉普拉斯矩阵$L = I_n - D^{-1/2} A D^{-1/2} = U \\Lambda U^T \\in \\mathbb{R}^{n \\times n}$来说，$U \\in \\mathbb{R}^{n \\times n}$是矩阵的特征向量，$\\Lambda \\in \\mathbb{R}^{n \\times n}$是特征值的对角矩阵。式3的时间复杂度很高，因为$U$的乘法的时间复杂度是$\\mathcal{O}(n^2)$。此外，计算$L$的特征值分解对于大的图来说很慢。Defferrard et al., 2016使用切比雪夫多项式：$$\\tag{4}g_\\theta(\\Lambda) = \\sum^{K-1}_{k=0} \\theta_k T_k(\\tilde{\\Lambda}),$$参数$\\theta \\in \\mathbb{R}^K$是切比雪夫系数的向量，$T_k(\\tilde{\\Lambda}) \\in \\mathbb{R}^{n \\times n}$是切比雪夫多项式的k阶项在$\\tilde{\\Lambda} = 2\\Lambda/\\lambda_{max} - I_n$的值。图卷积操作可以写为：$$\\tag{5}y = g_\\theta \\ast_{\\mathcal{G}} x = g_\\theta (L) x = \\sum^{K-1}_{k=0} \\theta_k T_k (\\tilde{L})x,$$$T_0 = 1$，$T_1 = x$，$T_k(x) = 2xT_{k-1}(x)-T_{k-2}(x)$，时间复杂度是$\\mathcal{O}(K \\vert \\Large{\\varepsilon} \\normalsize \\vert)$，也就是和边数相关。这个图卷积是$K$阶局部化的。 3 Related WorksShi et al. 2015提出了针对常规网格结构的序列的模型，可以看作是图是图像网格且顶点有序的特殊情况。他们的模型本质上是FC-LSTM，$W$的乘法替换为卷积核$W$：$$\\tag{6}i = \\sigma(W_{xi} \\ast x_t + W_{hi} \\ast h_{t-1} + w_{ci} \\odot c_{t-1} + b_i),\\\\f = \\sigma(W_{xf} \\ast x_t + W_{hf} \\ast h_{t-1} + w_{cf} \\odot c_{t-1} + b_f),\\\\c_t = f_t \\odot c_{t-1} + i_t \\odot \\mathrm{tanh}(W_{xc} \\ast x_t + W_{hc} \\ast h_{t-1} + b_c),\\\\o = \\sigma(W_{xo} \\ast x_t + W_{ho} \\ast h_{t-1} + w_{co} \\odot c_t + b_o),\\\\h_t = o \\odot \\mathrm{tanh}(c_t),$$$\\ast$表示一组卷积核的2D卷积。在他们的设定中$x_t \\in \\mathbb{R}^{n_r \\times n_c \\times d_x}$是一个动态系统中，时间$t$的$d_x$的观测值，这个动态系统建立在一个表示为$n_r$行$n_c$列的空间区域上。模型有着空间分布的隐藏核细胞状态，大小是$d_h$，由张量$c_t$体现，$h_t \\in \\mathbb{R}^{n_r \\times n_c \\times d_h}$。卷积核$W_{h\\cdot} \\in \\mathbb{R}^{m \\times m \\times d_h \\times d_h}$和$W_{x\\cdot} \\in \\mathbb{R}^{m \\times m \\times d_h \\times d_x}$的尺寸$m$决定了参数的数量，与网格大小$n_r \\times n_c$无关。更早一点，Ranzato et al. 2014提出了相似的RNN变体，使用卷积层而不是全连接层。时间$t$的隐藏状态：$$\\tag{7}h_t = \\mathrm{tanh}(\\sigma(W_{x2} \\ast \\sigma(W_{x1} \\ast x_t)) + \\sigma(W_h \\ast h_{t-1})),$$卷积核$W_h \\in \\mathbb{R}^{d_h \\times d_h}$受限到$1 \\times 1$的大小。 观察到自然语言表示出语法性质，自然的将词融入短语中，Tai et al. 2015提出了一个处理树结构的模型，每个LSTM可以获取他们的孩子的状态。他们在semantic relatedness and sentiment classification上获得了state-of-the-art的结果。Liang et al. 2016在之后提出了在图上的变体。他们复杂的网络结构在4个数据集上获得了semantic object parsing的state-of-the-art结果。这些模型中，状态通过一个可训练的权重矩阵的带权加和从邻居上聚集。然而这些权重并不在图上共享，否则需要对顶点排序，就像其他图卷积的空间定义一样。此外，他们的公式受限于当前顶点的一阶邻居，给其他的邻居相同的权重。 受到如人体动作和物体交互等时空任务的启发，Jain et al 2016提出了一个方法将时空图看作是一个富RNN的混合，本质上是将一个RNN连接到每个顶点与每条边上。同样的，通信受限于直接连接的顶点与边。 和我们的工作最相关的模型可能是Li et al 2015提出的模型，在program verification上表现出了最好的结果。尽管他们使用Scarselli et al. 2009提出的GNN，以迭代的步骤传播顶点的表示，直到收敛，我们使用的是Defferrard et al. 2016提出的GCN在顶点间扩散信息。尽管他们的动机和我们很不一样，这些模型的关联是使用$K$阶多项式定义的谱滤波器可以实现成一个$K$层的GNN。 4 Proposed GCRN Models我们提出了两种GCRN架构 Model 1.$$\\tag{8}x^{\\mathrm{CNN}}_t = \\mathrm{CNN}_\\mathcal{G}(x_t)\\\\i = \\sigma(W_{xi} x^{\\mathrm{CNN}}_t + W_{hi}h_{t-1} + w_{ci} \\odot c_{t-1} + b_i),\\\\f = \\sigma(W_{xf} x^{\\mathrm{CNN}}_t + W_{hf} h_{t-1} + w_{cf} \\odot c_{t-1} + b_f),\\\\c_t = f_t \\odot c_{t-1} + i_t \\odot \\mathrm{tanh}(W_{xc} x^{\\mathrm{CNN}}_t + W_{hc} h_{t-1} + b_c),\\\\o = \\sigma(W_{xo} x^{\\mathrm{CNN}}_t + W_{ho} h_{t-1} + w_{co} \\odot c_t + b_o),\\\\h_t = o \\odot \\mathrm{tanh}(c_t).$$我们简单地写成$x^{\\mathrm{CNN}}_t = W^{\\mathrm{CNN}} \\ast_\\mathcal{G} x_t$，其中$W^{\\mathrm{CNN}} \\in \\mathbb{R}^{K \\times d_x \\times d_x}$是切比雪夫系数。Peepholes由$w_{c\\cdot} \\in \\mathbb{R}^{n \\times d_h}$控制。这样的架构可能足以捕获数据的分布，通过挖掘局部静止性以及性质的组合性，还有动态属性。 Model 2.$$\\tag{9}i = \\sigma(W_{xi} \\ast_\\mathcal{G} x_t + W_{hi} \\ast_\\mathcal{G} h_{t-1} + w_{ci} \\odot c_{t-1} + b_i),\\\\f = \\sigma(W_{xf} \\ast_\\mathcal{G} x_t + W_{hf} \\ast_\\mathcal{G} h_{t-1} + w_{cf} \\odot c_{t-1} + b_f),\\\\c_t = f_t \\odot c_{t-1} + i_t \\odot \\mathrm{tanh}(W_{xc} \\ast_\\mathcal{G} x_t + W_{hc} \\ast_\\mathcal{G} h_{t-1} + b_c),\\\\o = \\sigma(W_{xo} \\ast_\\mathcal{G} x_t + W_{ho} \\ast_\\mathcal{G} h_{t-1} + w_{co} \\odot c_t + b_o),\\\\h_t = o \\odot \\mathrm{tanh}(c_t),$$图卷积核是切比雪夫系数$W_{h\\cdot} \\in \\mathbb{R}^{K \\times d_h \\times d_h}$，$W_{x\\cdot} \\in \\mathbb{R}^{K \\times d_h \\times d_x}$决定了参数的数目，与顶点数$n$无关。这种RNN和CNN的混合，不限于LSTM。普通的RNN $h_t = \\mathrm{tanh}(W_x x_t + W_h h_{t-1})$可以写为：$$\\tag{10}h_t = \\mathrm{tanh}(W_x \\ast_\\mathcal{G} x_t + W_h \\ast_\\mathcal{G} h_{t-1}),$$GRU的版本可以写为：$$\\tag{11}z = \\sigma(W_{xz} \\ast_\\mathcal{G} x_t + W_{hz} \\ast_\\mathcal{G} h_{t-1}),\\\\r = \\sigma(W_{xr} \\ast_\\mathcal{G} x_t + W_{hr} \\ast_\\mathcal{G} h_{t-1}),\\\\\\tilde{h} = \\mathrm{tanh}(W_{xh} \\ast_\\mathcal{G} x_t + W_{hh} \\ast_\\mathcal{G} (r \\odot h_{t-1})),\\\\h_t = z \\odot h_{t-1} + (1 - z) \\odot \\tilde{h}.$$ 5 Experiments 数据集是moving-MNIST(Shi et al., 2015)。","link":"/blog/2018/07/23/structured-sequence-modeling-with-graph-convolutional-recurrent-networks/"},{"title":"T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction","text":"T-GCN，arxiv上面的一篇文章，用 GCN 对空间建模，GRU 对时间建模，很简单的模型。没有对比近几年的图卷积在时空数据挖掘中的模型。原文地址：T-GCN: A Temporal Graph ConvolutionalNetwork for Traffic Prediction Abstract精确和实时的交通预测在智能交通系统中扮演着重要的角色，对城市交通规划、交通管理、交通控制起着重要的作用。然而，交通预测由于其受限于城市路网且随时间动态变化，即有着空间依赖与时间依赖，早已成为一个公开的科学研究问题。为了同时捕获空间和时间依赖，我们提出了一个新的神经网络方法，时间图卷积网络模型 （T-GCN），将图卷积和门控循环单元融合起来。GCN 用来学习复杂的拓扑结构来捕获空间依赖，门控循环单元学习交通数据的动态变化来捕获时间依赖。实验表明我们的 T-GCN 模型比之前的方法要好。我们的 tf 实现：代码仓库地址。 1 Introduction随着智能交通系统的发展，交通预测受到了越来越多的关注。交通预测是高级交通管理系统中的关键部分，是实现交通规划、交通管理、交通控制的重要部分。交通预测是分析城市路网上交通状况、包括流量、车速、密度，挖掘交通模式，对路网上交通进行预测的一个过程。交通预测不仅能给管理者提供科学依据来预测交通拥挤并提前限制出行，还可以给旅客提供适当的出行路线并提高交通效率。然而，交通由于其空间和时间的依赖至今还是一个有难度的挑战： （1）空间依赖。流量的改变主要受路网的拓扑结构控制。上游道路的交通状态通过转移影响下游的道路，下游的交通状态会通过反馈影响上游的状态。如图 1 所示，由于邻近道路的强烈影响，短期相似性从状态 1 （上游与中游相似）转移到 状态 2（上游与下游相似）。 （2）时间依赖。流量随时间动态改变，主要会出现周期性和趋势。如图 2（a）所示，路 1 的流量在一周内展示出了周期性变化。图 2（b）中，一天的流量也发生变换；举个例子，流量会被其前一时刻或更前的时刻的交通状况所影响。 有很多交通预测方法，一些考虑时间依赖，包括 ARIMA，Kalman filtering model, SVR, knn, Beyesian model, partial neural network model.上述方法考虑交通状况在时间上的动态变化，忽略了空间依赖，导致不能精确预测。为了更好地刻画空间特征，一些研究引入了卷积神经网络对空间建模；然而，卷积适用于欧氏空间的数据，如图像、网格等。这样的模型不能在城市路网这样有着复杂拓扑结构的环境下工作，所以他们不能描述空间依赖。 为了解决这个问题，我们提出了新的交通预测方法，时间图卷积网络 （T-GCN），用于对基于城市路网的交通预测任务。我们的贡献有三点： （1） 我们提出的模型结合了 GCN 和 GRU，图卷积捕获路网的拓扑结构做空间建模，GRU 捕获路网上交通数据的时间依赖。T-GCN 模型可以用于其他时空预测任务上。 （2） T-GCN 的预测结果比其他的方法好，表明我们的 T-GCN 模型不仅可以做短期预测，也可以做长期预测。 （3）我们使用深圳市罗湖区的出租车速度数据和洛杉矶线圈数据。结果表明我们的预测误差比所有的 baseline 小了 1.5%到57.8%，表明 T-GCN 在交通预测上的优越性。 2 Related work智能交通系统交通预测是现在的一个重要研究问题。现存的方法分两类：模型驱动的方法和数据驱动的方法。首先，模型驱动的方法主要解释交通流量、速度、密度的瞬时性和平稳性。这样的方法需要基于先验知识的系统建模。代表方法包括排队论模型，细胞传递模型，交通速度模型，microscopic fundamental diagram model 等等。实际中，交通数据受多种因素影响，很难获得一个精准的交通模型。现存的模型不能精确地描述复杂的现实环境中的交通数据的变化。此外，这些模型的构建需要很强的计算能力，而且很容易收到交通扰乱和采样点空间等问题的影响。 数据驱动的方法基于数据的规律性，从统计学推测变化局势，然后用于预测。这类方法不分析物理性质和交通系统的动态行为，有很高的灵活性。早期的方法包括历史均值模型，使用历史周期的交通流量均值作为预测值。这个方法不需要假设，计算简单而且还快，但是不能有效地拟合时间特征，预测的精准度低。随着研究的深入，很多高精度的方法涌现出来，主要分为参数模型和非参数模型。 参数模型提前假设回归函数，参数通过对原始数据处理得到，基于回归函数对交通流预测。时间序列模型，线性回归模型，Kalman filtering model 是常用的方法。时间序列模型将观测到的时间序列拟合进一个模型，然后用来预测。早在 1976 年，Box and Jenkins 提出了 ARIMA，Hamed 等人使用 ARIMA 预测城市内的交通流量。为了提高模型的精度，不同的变体相继被提出，Kohonen ARIMA，subset ARIMA，seasonal ARIMA 等等。Lippi 等人对比支持向量回归和 seasonal ARIM，发现 SARIMA 模型在交通拥堵上的预测有更好的结果。线性回归模型基于历史数据构建模型来预测。2004 年，Sun 等人使用 local linear model 解决了区间预测，在真实数据集上获得了较好的效果。Kalman filtering model 基于前一时刻和当前时刻的交通状态预测未来的状态。1984 年，Okutani 等人使用 Kalman filtering 理论建立了交通流状态预测模型。后续，一些研究使用 Kalman filtering 模型解决交通预测任务。 传统的参数模型算法简单，计算方便。然而，这些方法依赖平稳假设，不能反映交通数据的非线性和不确定性，也不能克服交通事件这种随机性事件。非参数模型很好地解决这些问题，只需要足够的历史信息能自动地从中学到统计规律即可。常见的非参数模型包括：k近邻，支持向量回归，Fuzzy Logic 模型等。 近些年，随着深度学习的快速发展，深度神经网络可以捕获交通数据的动态特征，获得很好的效果。根据是否考虑空间依赖，模型可以划分成两类。一些方法只考虑时间依赖，如 Park 等人使用 FNN 预测交通流。Huang 等人使用深度置信网络 DBN 和回归模型在多个数据集上证明可以捕获交通数据中的随机特征，提升预测精度。此外，RNN 及其变体 LSTM, GRU 可以有效地使用自循环机制，他们可以很好地学习到时间依赖并获得更好的预测结果。 这些模型考虑时间特征但是忽略空间依赖，所以交通数据的变化不受城市路网的限制，因此他们不能精确的预测路上的交通状态。解决交通预测问题的关键是充分利用空间和时间依赖。为了更好的刻画空间特征，很多研究已经在这个基础上进行了提升。Lv 等人提出了一个 SAE 模型从交通数据中捕获时空特征，实现短期交通流的预测。Zhang 等人提出了一个叫 ST-ResNet 的模型，基于人口流动的时间近邻、周期和趋势这些特征设计了残差卷积网络，然后三个网络和外部因素动态地聚合起来，预测城市内每个区域人口的流入和流出。Wu 等人设计了一个特征融合架构通过融合 CNN 和 LSTM 进行短期预测。一个一维的 CNN 用于捕获空间依赖，两个 LSTM 用来挖掘交通流的短期变化和周期性。Cao 等人提出一个叫 ITRCN 的端到端模型，将交互的网络交通转换为图像，使用 CNN 捕获交通的交互式功能，用 GRU 提取时间特征，预测误差比 GRU 和 CNN 分别高了 14.3% 和 13.0%。Ke 等人提出一个新的深度学习方法叫融合卷积长短时记忆网络（FCL-Net），考虑空间依赖、时间依赖，以及异质依赖，用于短期乘客需求预测。Yu 等人用深度卷积神经网络捕获空间依赖，用 LSTM 捕获时间动态性，在北京交通网络数据上展示出了 SRCN 的优越性。 尽管上述方法引入了 CNN 对空间依赖建模，在交通预测任务上有很大的进步，但 CNN 本质上只适用于欧氏空间，在有着复杂拓扑结构的交通网络上不能刻画空间依赖。因此，这类方法有缺陷。近些年，图卷积网络的发展，可以用来捕获图网络的结构特征，提供更好的解决方案。Li 等人提出了 DCRNN 模型，通过图上的随机游走捕获空间特征，通过编码解码结构捕获时间特征。 基于这个背景，我们提出了新的神经网络方法捕获复杂的时空特征，可以用于基于城市路网的交通预测任务上。 3 Methodology3.1 Problem Definition目标是基于历史信息预测未来。我们的方法中，交通信息是一个通用的概念，可以是速度、流量、密度。我们在实验的时候将交通信息看作是速度。 定义1：路网 $G$。我们用图 $G = (V, E)$ 描述路网的拓扑结构，每条路是一个顶点，$V$ 顶点集，$V = \\lbrace v_1, v_2, \\dots, v_N \\rbrace$，$N$ 是顶点数，$E$ 是边集。邻接矩阵 $A$ 表示路的关系，$A \\in R^{N \\times N}$。邻接矩阵只有 0 和 1。如果路之间有连接就为 1， 否则为 0。 定义2：特征矩阵 $X^{N \\times P}$。我们将交通信息看作是顶点的特征。$P$ 表示特征数，$X_t \\in R^{N \\times i}$ 用来表示时刻 $i$ 每条路上的速度。 时空交通预测的问题可以看作学习一个映射函数： $$\\tag{1}[X_{t+1}, \\dots, X_{t+T}] = f(G; (X_{t-n}, \\dots, X_{t-1}, X_t))$$ $n$ 是历史时间序列的长度，$T$ 是需要预测的长度。 3.2 OverviewT-GCN 模型有两个部分：GCN 和 GRU。图 3 所示，我们使用历史 $n$ 个时刻的时间序列数据作为输入，图卷积网络捕获路网拓扑结构获取空间依赖。然后将带有空间特征的时间序列放入 GRU 中，通过信息在单元间的传递捕获动态变化，获得时间特征。最后，将结果送入全连接层。 3.3 Methodology3.3.1 Spatial Dependence Modeling获取复杂的空间依赖在交通预测中是一个关键问题。传统的 CNN 只能用于欧氏空间。城市路网不是网格，CNN 不能反映复杂的拓扑结构。GCN 可以处理图结构，已经广泛应用到文档分类、半监督学习、图像分类中。GCN 在傅里叶域中构建滤波器，作用在顶点及其一阶邻居上，捕获顶点间的空间特征，可以通过堆叠构建 GCN 模型。如图 4 所示，假设顶点 1 是中心道路，GCN 模型可以获取中心道路和它周围道路的拓扑关系，将这个结构和道路属性编码，获得空间依赖。总之，我们用 GCN 模型从交通数据中学习空间特征。两层 GCN 表示为： $$\\tag{2}f(X, A) = \\sigma(\\hat{A} Relu(\\hat{A} X W_0) W_1)$$ $\\hat{A} = \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}$ 表示预处理，$\\tilde{A} = A + I_N$ 表示加了自连接的邻接矩阵。 3.3.2 Temporal Dependence Modeling因为 GRU 比 LSTM 参数少，训练快，我们使用 GRU 获取交通数据的时间依赖。如图 5 所示，$h_{t-1}$ 表示时刻 $t-1$ 的隐藏状态；$x_t$ 表示时刻 $t$ 的交通信息；$r_t$ 表示重置门，用来控制忽略前一时刻信息的程度；$u_t$ 是更新门，用来控制将信息从上一时刻拿到这个时刻的程度；$c_t$ 是时刻 $t$ 的记忆内容；$h_t$ 是时刻 $t$ 的输出状态。GRU 通过将时刻 $t-1$ 的隐藏状态和当前时刻的交通信息作为输入，获取时刻 $t$ 的交通状态。在捕获当前时刻的交通信息的时候，模型仍保留着历史信息，且有能力捕获时间依赖。 3.3.3 Temporal Graph Convolutional Network为了同时从交通数据中捕获时空依赖，我们提出了时间图卷极网络（T-GCN）。如图6所示，左侧是时空交通预测的过程，右侧是一个 T-GCN 细胞的结构，$h_{t-1}$ 表示 $t-1$ 时刻的输出，GC 是图卷积过程，$u_t, r_t$ 是时刻 $t$ 的更新门和重置门，$h_t$ 表示时刻 $t$ 的输出。计算过程如下。$f(A, X_t)$ 表示图卷积过程，如式 2 定义。$W$ 和 $b$ 表示训练过程的权重与偏置。 $$\\tag{3}u_t = \\sigma(W_u[f(A, X_t), h_{t-1}] + b_u)$$ $$\\tag{4}r_t = \\sigma(W_r[f(A, X_t), h_{t-1}] + b_r)$$ $$\\tag{5}c_t = tanh(W_c[f(A, X_t), (r_t \\ast h_{t-1})] + b_c)$$ $$\\tag{6}h_t = u_t \\ast h_{t-1} + (1 - u_t) \\ast c_t$$ 总之，T-GCN 能处理复杂的空间依赖和时间动态性。 3.3.4 Loss Function损失函数如式 7。第一项用来减小速度的误差。第二项 $L_{reg}$ 是一个 $L2$ 正则项，避免过拟合，$\\lambda$ 是超参。 $$\\tag{7}loss = \\Vert Y_t - \\hat{Y}_t \\Vert + \\lambda L_{reg}$$ 4 Experiments4.1 Data Description两个数据集，深圳出租车和洛杉矶线圈。两个数据集都和车速有关。 （1）SZ-taxi。数据是2015年1月1日到1月31日的深圳出租车轨迹数据。我们选了罗湖区 156 个主要路段作为研究区域。实验数据主要有两部分。一个是 156 * 156 的邻接矩阵，另一个是特征矩阵，描述了速度随时间的变化。我们将速度以 15 分钟为单位聚合。 （2）Los-loop。数据集是洛杉矶县高速公路线圈的实时数据。我们选了 207 个监测器，数据是 2012年5月1日到5月7日的数据。我们以5分钟为单位聚合车速。数据也是一个邻接矩阵和一个特征矩阵。我们用线性插值填补了缺失值。 我们将输入数据归一化到 $[0, 1]$。此外，80% 的数据用来训练，20% 用来测试。我们预测未来15、30、45、60分钟的车速。 4.2 Evaluation Metrics（1）RMSE: $$\\tag{8}RMSE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1} (Y_t - \\hat{Y}_t)^2}$$ （2）MAE: $$\\tag{9}MAE = \\frac{1}{n} \\sum^n_{i=1} \\vert Y_t - \\hat{Y}_t \\vert$$ （3）Accuracy: $$\\tag{10}Accuracy = 1 - \\frac{\\Vert Y - \\hat{Y} \\Vert}{\\Vert Y \\Vert_F}$$ （4）Coefficient of Determination (R2): $$\\tag{11}R^2 = 1 - \\frac{\\sum_{i=1} (Y_t - \\hat{Y}_t)^2}{\\sum_{i=1}(Y_t - \\bar{Y})^2}$$ （5）Explained Variance Score(Var): $$var = 1 - \\frac{Var\\lbrace Y - \\hat{Y}\\rbrace}{Var\\lbrace Y\\rbrace}$$ RMSE 和 MAE 用来评估预测误差：越小越好。精度衡量预测的精度：越大越好。$R^2$ 和 Var 计算相关系数，评估预测结果表达真实数据的能力，越大越好。 4.3 Model Parameters Designing(1) Hyperparameter 学习率、batch size、训练论述，隐藏层数。我们设定的是学习率0.001，batch size 64，轮数 3000 轮。 隐层单元数对 T-GCN 来说是个重要的参数，因为不同的单元数可能会影响预测精度。我们通过实验选取了最优的隐藏单元数。 看不下去了。。。","link":"/blog/2019/03/07/t-gcn-a-temporal-graph-convolutional-network-for-traffic-prediction/"},{"title":"ubuntu /boot 满了怎么办","text":"ubuntu /boot 满了怎么办，解决方案 1uname -a 看一下现在用的是什么内核 1cd /boot 把老的内核挪走，挪的时候按版本号挪，从老的开始挪，具有同一个版本号的文件同时挪走，挪几个老的就行。 然后 1apt-get install -f 1dpkg --get-selections |grep linux- 把老的内核都卸载掉： 1apt-get purge linux-headers-4.4.0-137-generic linux-image-4.4.0-137-generic linux-image-extra-4.4.0-137-generic 然后把刚才挪走的文件再挪回去，再用上面的命令卸载掉。","link":"/blog/2019/04/19/ubuntu-boot-满了怎么办/"},{"title":"The Emerging Field of Signal Processing on Graphs","text":"IEEE Signal Processing Magazine 2013, 原文链接：The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and Other Irregular Domains Abstract社交、能源、运输、传感器、神经网络、高维数据很多都很自然地依赖于带权图的顶点。新兴的图信号处理领域融合了代数、谱图理论与计算谐波分析来处理图上的信号。在这篇教程中，我们列出了这个领域的主要挑战，讨论了定义图谱域的不同方法，点明了融合图数据域中不规则的结构在处理图信号时的重要性。然后回顾了将基础的操作，如filtering, translation, modulation, dilation, downsampling等技术泛化到图上的方法，对已经提出的高效地从图中的高维数据提取信息的localized, multisacle transforms进行了总结。最后对一些问题以及未来的扩展做了一些讨论。 1. Introduction图是很多数据的表示形式，在描述很多应用的几何结构时很有用，如社交、能源、运输、传感器、神经网络等。图中每条边的权重，经常表示为两个顶点之间的相似度。连接性和边权重要么由问题的物理性质指明，要么从数据中推断出来。举个例子，边权重可能与网络中两个顶点之间的距离成反比。这些图的数据可以看作是一个样本的有限集合，每个顶点一个样本。我们称这些样本为一个图信号。一个图信号的例子如图1所示。 在运输网络中，我们关心分析描述疾病传播的传染病数据，描述用户迁移的人口数据，或是描述货物仓库的后勤数据。现在，在大脑图像中，推断大脑皮层上独特的功能区结构上的连接性变为可能，这种连接可以表示为一个带权图，顶点代表了功能区。因此，noisy fMRI图像可以看作是带权图上的信号。带权图一般用来表示统计学习问题中数据点之间的相似性，如计算机视觉和文本分类问题。事实上，很多研究图数据分析的论文是从统计学习社区中发表出来的，因为基于图的方法在半监督学习问题中变得非常流行，这些问题的目标是用一些标记样本对未知的样本进行分类。在图像处理，对图像的像素构造非局部和半局部连接的图的这种，基于图的filtering methods突然流行了起来，这些方法不仅基于像素间的物理相似性，还有要处理的图像的nosiy versions。这些方法经常能更好地识别并考虑图像的边和材质。 这些应用中常见的数据处理任务有filtering, denoising, inpainting, compressing graph signals。如何在不规则的域中处理他们，比如在任意结构的图上面？对数据的存储，通信，分析最有效的从高维数据中提取信息的方法是什么，统计与可视化？传统的信号处理的操作或算法可以使用吗？在图上的信号处理领域还有一些这样的问题。 A. The Main Challenges of Signal Processing on Graphs小波，时频，曲波和其他局部变化来稀疏地表示不同类别的高维数据，如欧氏空间中的音频和图像信号，这种表示能力在之前提到的信号处理任务中取得了很多的成功。 $N$个顶点的图信号和一个传统的$N$个样本的离散时域信号可以看作是$\\mathbb{R}^N$中的向量。然而，传统信号处理方法应用到图数据上的一个主要障碍是用离散时域信号的处理方式处理图信号时忽略了不规整的数据域内的关键依赖。此外，传统信号处理技术中的很多很简单的基础概念在图信号中变得很有挑战性：·为了让一个模拟信号$f(t)$向右移动3，我们只要简单的改变变量，考虑$f(t-3)$即可。然而，把一个图信号向右移动3的意义就不是很清晰了。改变变量的方法不会有效因为$f(\\circ - 3)$没有意义。一个朴素的方法是将顶点从$1$标到$N$，定义$f(\\circ - 3) := f(\\mathrm{mod}(\\circ - 3, N))$，但是如果这个变换依赖于顶点的顺序的话，这个方法就不是很有用了。不可避免的是，带权图是不规则的结构，这种结构缺少一种变换的平移不变性的性质。·通过乘以一个复杂的指数项在实数线上对信号建模对应了傅里叶域中的变换。然而，图问题中的模拟谱是离散且不规则的，因此没有好的方法定义一种对应图谱域中的变换。·举个例子，我们凭直觉每隔一个数据点删除一个数据点，对离散时域信号做下采样。但是在图1中的图信号中这意味着什么？带权图中的“每隔一个顶点”没有明确的含义。·甚至我们做一个固定的下采样，为了在图上做一个多分辨率，我们需要一个生成粗糙版本的图的方法，这个方法可以捕获原始图中嵌入的结构属性。 此外，处理数据域的不规则性，图结构在之前提到的应用中，可以表示很多顶点的特征。为了能很好地对数据的尺度进行缩放，对于图信号的处理技术应该使用局部操作，通过对每个顶点，计算顶点的邻居，或是和它很近的顶点的信息得到。 因此，图信号处理的主要挑战是：1. 有些任务中图没有直接给出，需要决定如何构建可以捕获数据几何结构的带权图；2. 将图结构整合到局部变换操作中；3. 同时利用这些年来信号处理在欧氏空间发展出的理论成果；4. 研究局部变换的高效实现，从高维的图结构数据或其他不规则数据域中提取信息。 为了解决这些问题，新兴的图信号处理领域将代数和谱图理论的概念与计算谐波分析融合了起来。这是在代数图理论和谱图理论中的扩展；但是，早于十年前的研究主要是聚焦于分析图，而不是分析图的信号。 2. The Graph Spectral Domains谱图理论是聚焦于构建、分析、操作图的，不是图上的信号。在构建扩展图、图的可视化、谱聚类、着色问题、还有许多如化学、物理、计算科学领域的问题上都很有效。 图信号处理领域，谱图理论被用作一个定义频谱和图傅里叶变换的基的扩展的工具。这部分我们会回顾一些谱图理论基本的定义与符号，研究它如何使得从传统的傅里叶分析扩展出很多重要的数学理论到图论上。 A. Weighted Graphs and Graph Signals我们分析无向、连通图$\\mathcal{G} = \\lbrace \\mathcal{V}, \\mathcal{E}, \\mathbf{W} \\rbrace$上的信号。边$e = (i, j)$连接了顶点$i$和$j$，$W_{i,j}$表示边的权重，否则$w_{i,j} = 0$。如果$\\mathcal{G}$有$M$个连通分量，我们可以将信号分为$M$份，然后将每份看作是一个子图进行处理。 当边的权重没有给出的时候，一种常用的方法是使用一个带阈值的高斯核权重函数：$$\\tag{1}W_{i,j} = \\begin{cases}\\exp{(-\\frac{[dist(i,j)]^2}{2\\theta^2})} \\quad &amp;\\text{if } dist(i, j) \\leq \\kappa \\\\0 \\quad &amp;\\text{otherwise}\\end{cases},$$参数是$\\theta$和$\\kappa$。式1中，$dist(i, j)$表示顶点$i$和$j$之间的物理距离，或是两个顶点的特征向量的欧氏空间中的距离，后者在半监督学习任务中很常用。另一个常用的方法是基于物理距离或特征空间距离，将顶点与它的$k$最近邻顶点相连。其他构建图的方法，见第四章，14。 一个定义在图的顶点上的信号或函数$f: \\mathcal{V} \\rightarrow \\mathbb{R}$可能表示成一个向量$\\mathbf{f} \\in \\mathbb{R}^N$，第$i$个分量表示顶点集$\\mathcal{V}$中的第$i$个顶点。 B. The Non-Normalized Graph Laplacian非归一化的拉普拉斯矩阵，也称为组合拉普拉斯矩阵(combinatorial graph Laplacian)，定义为$\\bf L := D - W$，$\\bf{D}$是对角矩阵，对角线上的第$i$个元素等于与顶点$i$相关的边的权重之和。拉普拉斯矩阵是一个差操作，因为对于任意一个信号$\\mathbf{f} \\in \\mathbb{R}^N$，它满足：$$(\\mathbf{L}f)(i) = \\sum_{j \\in \\mathcal{N}_i} W_{i,j}[f(i) - f(j)],$$邻居$\\mathcal{N}_i$是与顶点$i$通过一条边相连的顶点集合。我们用$\\mathcal{N}(i, k)$表示通过$k$步或小于$k$步连接到顶点$i$的顶点集合。因为图的拉普拉斯矩阵$L$是实对称矩阵，它的特征向量相互正交，我们表示为$\\lbrace \\mathbf{u}_l \\rbrace_{l=0,1,…,N-1}$。这些特征向量对应非负的特征值$\\lbrace \\lambda_l\\rbrace_{l=0,1,…,N-1}$，满足$L \\mathbf{u}_l = \\lambda_l \\mathbf{u}_l$，$l = 0,1,…,N-1$。零作为特征值,其多重性等于图的连通分量数，因为我们考虑的是连通图，我们假设拉普拉斯矩阵的特征值的顺序为：$0 = \\lambda_0 &lt; \\lambda_1 \\leq \\lambda_2 … \\leq \\lambda_{N-1} := \\lambda_{\\text{max}}$。我们将整个谱表示为$\\sigma(L) = \\lbrace \\lambda_0, \\lambda_1, …, \\lambda_{N-1}\\rbrace$。 C. A Graph Fourier Transform and Notion of Frequency传统的傅里叶变换$$\\hat{f}(\\xi) := \\langle f, e^{2\\pi i \\xi t} \\rangle = \\int_\\mathbb{R} f(t) e^{-2\\pi i \\xi t}dt$$是函数$f$根据复指数的扩展，是一维拉普拉斯算子的特征函数：$$\\tag{2}-\\Delta(e^{2\\pi i \\xi t}) = -\\frac{\\partial^2}{\\partial t^2} e^{2\\pi i \\xi t} = (2 \\pi \\xi)^2 e^{2\\pi i \\xi t}.$$ 类比这个，我们可以定义任何一个在图$\\mathcal{G}$的顶点上的函数$\\mathbf{f} \\in \\mathbb{R}^N$的图傅里叶变换$\\hat{\\mathbf{f}}$，根据图拉普拉斯矩阵的特征向量对$\\mathbf{f}$的扩展：$$\\tag{3}\\hat{f}(\\lambda_l) := \\langle \\mathbf{f}, \\mathbf{u}_l \\rangle = \\sum^N_{i = 1} f(i) u^*_l (i).$$逆图傅里叶变换为：$$\\tag{4}f(i) = \\sum^{N - 1}_{l = 0} \\hat{f}(\\lambda_l) u_l(i).$$ 传统的傅里叶分析中，式2中的特征值$\\lbrace (2 \\pi \\xi )^2 \\rbrace_{\\xi \\in \\mathbb{R}}$对频率有特殊性：对于$\\xi$接近0（低频），对应的复指数特征函数是平滑的，震荡慢的函数，而$\\xi$远离0（高频）的对应的复指数特征函数震荡的很快。在图任务中，图拉普拉斯矩阵的特征值和特征向量在频率上提供了相似的特点。对于连通图，拉普拉斯矩阵的对应特征值为0的特征向量$\\mathbf{u}_0$是不变的，且每个顶点的值为$\\frac{1}{\\sqrt{N}}$。图拉普拉斯矩阵的特征向量中对应低频的$\\lambda_l$在图上变化的慢；也就是，如果两个顶点通过一条权重很大的边连接，这些地方的特征向量的值就会变得比较相似。对应大的特征值的特征向量在图上变化的更快，且边的权重越高，这些顶点上的值越不相似。图2给出了不同的随机的sensor网络的拉普拉斯矩阵的特征向量，图3展示了每个特征向量zero crossing的数量$\\vert Z_\\mathcal{G}(\\cdot) \\vert$。一个信号$\\bf{f}$在图$\\mathcal{G}$的zero crossing的集合定义为：$$Z_\\mathcal{G}(\\mathbf{f}) := \\lbrace e = (i, j) \\in \\Large\\varepsilon \\normalsize : f(i)f(j) &lt; 0 \\rbrace;$$也就是，连接一个正信号和一个负信号的边的集合。 D. Graph Signal Representations in Two Domains图傅里叶变换(3)和它的逆(4)给了我们一种方式在两个不同的域中等价的表示一个信号：顶点域和图谱域。尽管我们经常从顶点域的一个信号$\\bf{g}$开始，直接在图谱域中定义一个信号$\\hat{\\bf{g}}$可能仍然是有用的。我们称这样的信号为核(kernels)。图4a和图4b中，一个这样的核，一个heat kernel，分别展示了在两个域中的效果。类比传统的模拟情况，图4中展示的一个平缓的信号图傅里叶系数衰减的很快。这样的信号是可压缩的(compressible)，因为可以通过调整一些图傅里叶系数来趋近他们。 E. Discrete Calculus and Signal Smoothness with Respect to the Intrinsic Structure of the Graph分析信号时，需要强调一点是，属性（如smoothness）与数据域的内在结构相对应，在我们讨论的环境中，就是带权图。尽管微分几何提供了方法将潜在流形的几何结构整合进可微分流形上连续信号的分析中，离散微积分(discrete calculus)提供了一组可以在有限离散空间中操作的多变量微积分的定义与可微分操作器。 为了增加smoothness对应图的内在结构的问题，我们简单的提一些离散可微分操作。一个信号$\\bf{f}$在顶点$i$，对于边$e = (i, j)$的边导数(edge derivative)定义为：$$\\left. \\frac{\\partial \\mathbf{f}}{\\partial e} \\right|_i := \\sqrt{W_{i,j}}[f(j) - f(i)],$$顶点$i$处$\\bf{f}$的图梯度是：$$\\nabla_i \\mathbf{f} := [\\lbrace \\left. \\frac{\\partial f}{\\partial b} \\right|_i \\rbrace_{e \\in \\varepsilon \\ \\text{s.t.} \\ e=(i,j) \\ \\text{for some} \\ j \\in \\mathcal{V}}].$$ 顶点$i$的local variation$$\\begin{aligned}\\Vert \\nabla_i \\mathbf{f} \\Vert_2 : &amp; = [\\sum_{e \\in \\varepsilon \\ \\text{s.t.} \\ e =(i, j) \\ \\text{for some} \\ j \\in \\mathcal{V}} (\\left. \\frac{\\partial \\mathbf{f}}{\\partial e} \\right|_i)^2]^{\\frac{1}{2}} \\\\&amp; = [\\sum_{j \\in \\mathcal{N}_i} W_{i, j} [f(j) - f(i)]^2]^{\\frac{1}{2}}\\end{aligned}$$可以度量顶点$i$周围的$\\bf{f}$的local smootheness，当顶点$i$和它的邻居$j$的$\\bf{f}$有相近的值时这个值较小。 对于global smoothness，$\\bf{f}$的discrete p-Dirichlet form定义为：$$\\tag{5}S_p(\\mathbf{f}) := \\frac{1}{p} \\sum_{i \\in V} \\Vert \\nabla_i \\mathbf{f} \\Vert^p_2 = \\frac{1}{p} \\sum_{i \\in V}\\LARGE[ \\normalsize \\sum_{j \\in \\mathcal{N}_i} W_{i,j} [f(j) - f(i)]^2 \\LARGE]^{\\normalsize \\frac{p}{2}}.$$ 当$p=1$时，$S_1(\\mathbf{f})$是信号对图的total variation。当$p = 2$时：$$\\tag{6}\\begin{aligned}S_2(\\mathbf{f}) &amp;= \\frac{1}{2}\\sum_{i \\in V} \\sum_{j \\in \\mathcal{N}_i} W_{i,j} [f(j) - f(i)]^2 \\\\&amp;= \\sum_{(i,j) \\in \\varepsilon} W_{i,j} [f(j) - f(i)]^2 = \\mathbf{f^TLf}.\\end{aligned}$$ $S_2(\\mathbf{f})$被称为图拉普拉斯矩阵的二次型，semi-norm $\\bf \\Vert f \\Vert_L$定义为：$$\\Vert \\mathbf{f} \\Vert_\\mathbf{L} := \\Vert \\mathbf{L}^{\\frac{1}{2}} \\mathbf{f} \\Vert_2 = \\sqrt{\\mathbf{f^TLf}} = \\sqrt{S_2(\\mathbf{f})}.$$ 注意式6，二次型$S_2(\\mathbf{f})$等于0当且仅当$\\bf{f}$在所有顶点上都为常数（which is why $\\Vert \\mathbf{f} \\Vert_L$ is only a semi-form），而且，更一般地，当信号$\\bf{f}$在那些通过大权重的边连接的邻居顶点上有相似值时，$S_2(\\mathbf{f})$的值较小；也就是当它平滑的时候。 回到拉普拉斯矩阵的特征值和特征向量上，Courant-Fischer Theorem指出，他们也可以通过Rayleigh quotient定义为：$$\\tag{7}\\lambda_0 = \\min_{ \\mathbf{f} \\in \\mathbb{R}^N, \\Vert \\mathbf{f} \\Vert_2 = 1} \\lbrace \\mathbf{f^TLf} \\rbrace,$$$$\\tag{8}\\text{and} \\ \\lambda_l = \\min_{ \\mathbf{f} \\in \\mathbb{R}^N, \\Vert \\mathbf{f} \\Vert_2 = 1, \\mathbf{f} \\perp span\\lbrace \\mathbf{u}_0, …, \\mathbf{u}_{l-1} \\rbrace} \\lbrace \\mathbf{f^TLf} \\rbrace, \\ l = 1, 2, …, N-1.$$其中，特征向量$\\mathbf{u}_l$是第$l$个问题的最小化问题的解。从式6和式7中，我们可以再次看出为什么$\\mathbf{u}_0$对于连通图来说是常数。式8解释了为什么拉普拉斯矩阵中对应小的特征值的特征向量更平滑，也提供了另一个对为什么拉普拉斯矩阵的谱反映了频率的解释。 总结一下，图的连通性编码进了拉普拉斯矩阵，拉普拉斯矩阵通常用于定义图傅里叶变换（通过特征向量），平滑性的不同表示。Example 1展示了smoothness和一个图信号的谱内容是如何依赖于图的。 F. Other Graph Matrices图拉普拉斯矩阵的基$\\lbrace \\mathbf{u}_l \\rbrace_{l = 0, 1, …, N - 1}$只是在正向(3)和逆向(4)图傅里叶变换中使用的一组可能的基。第二个常用的normalize每个权重$W_{i,j}$的方法是乘以$\\frac{1}{\\sqrt{d_i d_j}}$。这样可以对图的拉普拉斯矩阵归一化，定义为$\\bf\\tilde{L} := D^{-\\frac{1}{2}} L D^{-\\frac{1}{2}}$，等价于：$$(\\tilde{L}f)(i) = \\frac{1}{\\sqrt{d_i}} \\sum_{j \\in \\mathcal{N}_i} W_{i,j} \\LARGE[\\normalsize \\frac{f(i)}{\\sqrt{d_i}} - \\frac{f(j)}{\\sqrt{d_j}} \\LARGE].$$ 连通图$\\mathcal{G}$的归一化的拉普拉斯矩阵的特征值$\\lbrace \\tilde{\\lambda}_l \\rbrace_{l=0,1,…,N-1}$满足：$$0 = \\tilde{\\lambda}_0 &lt; \\tilde{\\lambda}_1 \\leq … \\leq \\tilde{\\lambda}_{\\text{max}} \\leq 2,$$当且仅当$\\mathcal{G}$是二分图时，$\\tilde{\\lambda}_{\\text{max}} = 2$。我们将归一化的拉普拉斯矩阵表示为$\\lbrace \\mathbf{\\tilde{u}}_l \\rbrace_{l = 0,1,…N-1}$。图3b中，$\\tilde{L}$的谱和频率也有关系，对应大的特征值的特征向量一般有着更多的zero crossing。然而，不像$\\mathbf{u}_0$，归一化的拉普拉斯矩阵中对应特征值为0的$\\tilde{\\mathbf{u}}_0$不是一个常向量。 归一化和非归一化的拉普拉斯矩阵都是generalized graph Laplacians的例子，也称为discrete Schrödinger operators。一个图$\\mathcal{G}$的泛化拉普拉斯矩阵是任意的对阵矩阵，如果这个矩阵中有边连接顶点$i$和顶点$j$，那么这个矩阵的$(i, j)$是负的，如果$i \\not = j$，而且$i$与$j$不相连，那么为$0$，如果$i = j$，那么有可能是任何值。 第三个常用的矩阵，经常在图信号的降维技术中使用，是random walk matrix，$\\bf{P := D^{-1}W}$。每个值$P_{i,j}$表示在图$\\mathcal{G}$上从顶点$i$到顶点$j$通过一步马尔可夫随机游走的概率。对于连通的、非周期的图，$\\mathbf{P}^t$在$t$趋近于无穷时，收敛至平稳分布。与随机游走矩阵密切相关的是非对称拉普拉斯矩阵，定义为 $\\mathbf{L}_a := \\mathbf{I}_N - \\mathbf{P}$，其中$\\mathbf{I}_N$表示$N \\times N$的单位阵。注意$\\mathbf{L}_a$有着和$\\tilde{\\mathbf{L}}$同样的特征值集合，如果$\\tilde{\\mathbf{u}}_l$是对应$\\tilde{L}$的特征值$\\tilde{\\lambda}_l$的特征向量，则$\\bf{D}^{-\\frac{1}{2}} \\tilde{\\mathbf{u}}_l$是对应$\\mathbf{L}_a$的特征值$\\tilde{\\lambda}_l$的特征向量。 正如下一节要讨论的，归一化和非归一化的拉普拉斯矩阵都能用于filtering。没有明确的规定要求什么时候必须使用归一化的，什么时候使用非归一化的拉普拉斯矩阵的特征向量，什么时候使用其他的基。归一化的拉普拉斯矩阵有很好的性质，它的谱总时在$[0, 2]$区间内，而且对于二分图，spectral folding phenomenon可以研究。然而，非归一化的拉普拉斯矩阵中对应特征值为0的特征向量是常向量，这在从传统filtering理论扩展关于信号的DC components上是一个有用的性质。 3. Generalized Operators For Signals on Graphs在这部分，我们会回顾不同的方式来泛化基本操作到图上，如filtering, translation, modulation, dilation, downsampling。这些泛化的操作是第四部分要讨论的localized, multiscale transforms的基础。 A. Filtering第一个泛化的操作是filtering。我们从扩展频率滤波的概念到图上开始，然后讨论顶点域上的局部滤波。 1. Frequency Filtering:在传统的信号处理中，频率滤波是将输入信号表示成一个复指数的线性组合，扩大或缩小一些复指数贡献的过程$$\\tag{9}\\hat{f}_{out}(\\xi) = \\hat{f}_{in}(\\xi) \\hat{h}(\\xi),$$其中，$\\hat{h}(\\cdot)$是滤波器的传递函数。取式9的逆傅里叶变换，傅里叶域中的乘法对应了时域中的卷积：$$\\tag{10}f_{out}(t) = \\int_\\mathbb{R} \\hat{f}_{in}(\\xi) \\hat{h}(\\xi)e^{2 \\pi i \\xi t} d\\xi$$$$\\tag{11}=\\intop_\\mathbb{R} f_{in}(\\tau) h(t-\\tau)d\\tau =: (f_in h)(t).$$一旦我们fix一个图谱表示，我们的图傅里叶变换的概念，我们可以直接将式9泛化到定义频率滤波上，或图谱滤波*(graph spectral filtering)上：$$\\tag{12}\\hat{f}_{out}(\\lambda_l) = \\hat{f}_{in}(\\lambda_l) \\hat{h}(\\lambda_l),$$或者，等价的，取逆图傅里叶变换，$$\\tag{13}f_{out}(i) = \\sum^{N-1}_{l=0} \\hat{f}_{in}(\\lambda_l) \\hat{h}(\\lambda_l) u_l(i).$$ 接用matrix functions[38]理论中的符号，我们可以将式12和式13写成$\\mathbf{f}_{out} = \\hat{h}(\\mathbf{L})\\mathbf{f}_{in}$，其中$$\\tag{14}\\hat{h}(\\mathbf{L}) := \\mathbf{U} \\begin{bmatrix}\\hat{h}(\\lambda_0) &amp; &amp; 0 \\\\ &amp; \\ddots &amp; \\\\ 0 &amp; &amp;\\hat{h}(\\lambda_{N-1})\\end{bmatrix}\\mathbf{U^T}$$ 基础的图谱滤波可以用来实现连续滤波技术的离散版，如高斯平滑，双边滤波，total variation filtering，anisotropic diffusion，non-local means filtering。特别地，这些滤波器中的很多成为了解决variational problems的方法，对ill-posed inverse problems进行正则化，这些问题如denoising，inpainting，super-resolution。举个例子，离散正则框架：$$\\tag{15}\\min_\\mathbf{f}\\lbrace \\Vert \\mathbf{f} - \\mathbf{y} \\Vert^2_2 + \\gamma S_p(\\mathbf{f}) \\rbrace,$$其中，$S_p(\\mathbf{f})$是式5的p-Dirichlet form。在Example 2中，我们举了个式15的$p = 2$时处理图像去噪的问题的例子。 2. Filtering in the Vertex Domain:在顶点域中filter一个信号，只要简单的将顶点$i$的输出$f_{out}(i)$写成一个顶点$i$的$K-hop$局部邻居上输入信号各分量的线性组合：$$\\tag{18}f_{out}(i) = b_{i, i} f_{in}(i) + \\sum_{j \\in \\mathcal{N}(i, K)} b_{i,j} f_{in}(j),$$$\\lbrace b_{i,j} \\rbrace_{i,j \\in \\mathcal{V}}$是常数。式18只说明了顶点域上的滤波是一个局部的线性变换。 我们现在简单地将图谱域上的滤波关联到了顶点域的滤波上。当式12中的频率滤波是$K$阶多项式$\\hat{h}(\\lambda_l) = \\sum^K_{k=0} a_k \\lambda^k_l$时，其中$\\lbrace a_k\\rbrace_{k=0,1,…K}$是常数，我们也可以将式12在顶点域中解释。由式13，我们得到：$$\\tag{19}\\begin{aligned}f_{out}(i) &amp; = \\sum^{N-1}_{l=0} \\hat{f}_{in}(\\lambda_l) \\hat{h}(\\lambda_l) u_l(i) \\\\&amp; = \\sum^N_{j=1} f_{in}(j) \\sum^K_{k=0} a_k \\sum^{N-1}_{l=0} \\lambda^k_l u^*_l(j) u_l(i) \\\\&amp; = \\sum^N_{j=1} f_{in}(j) \\sum^K_{k=0} a_k(\\mathbf{L}^k)_{i,j}.\\end{aligned}$$ 然而，在顶点$i$到顶点$j$之间的最短路径距离$d_\\mathcal{G}(i,j)$大于$k$时，$(\\mathbf{L}^k)_{i,j} = 0$。因此，我们可以将式19写成式18，常数定义为：$$b_{i,j} := \\sum^K_{k=d_\\mathcal{G}(i,j)} a_k (\\mathbf{L}^k)_{i,j}.$$所以当频率滤波是一个$K$阶多项式时，顶点$i$上频率滤波后的信号，$f_{out}(i)$，是顶点$i$的$K-hop$邻居上的输入信号的线性组合。这个性质在关联一个卷积核的平滑性与顶点域中滤波后信号的局部化之间很有用。 B. Convolution我们不能直接将卷积的定义（11）泛化到图上，因为$h(t - \\tau)$。然而，一种定义图上的卷积的方式是替换式10中的复指数为拉普拉斯矩阵的特征向量：$$\\tag{20}(f * h)(i) := \\sum^{N-1}_{l=0} \\hat{f}(\\lambda_l) \\hat{h}(\\lambda_l) u_l(i),$$这个使得在顶点域上的卷积等价于在图谱域的乘法。 C. Translation","link":"/blog/2018/08/03/the-emerging-field-of-signal-processing-on-graphs/"},{"title":"决策树为什么要引入随机数","text":"最近在使用scikit-learn的决策树的时候发现每次生成的树都不一样。发现决策树里面的有个random_state的参数，但是没想明白为什么会有这么个参数。最近给本科生的机器学习课程做助教，需要给他们出作业题，想做一个决策树相关的练习，生成了一批随机数据，然后画出决策树的decision boundary，结果发现，这个边界每次都不一样，然后就没想明白，决策树每次生成的不应该是一样的树吗，为什么边界会变化。 查了一下之后，发现如果决策树对连续性变量进行分类的时候，需要取一个中间值，这个中间值一般要加入随机因素，这样生成的树就不一样了。","link":"/blog/2018/09/01/决策树为什么要引入随机数/"},{"title":"在GitHub上部署hexo博客","text":"在GitHub Pages部署hexo博客 需要的工具 node.js git 安装及部署 安装完node.js和git后安装hexonpm install hexo -g安装后使用hexo -v查看版本号，看是否安装成功 创建hexo项目找个文件夹作为博客的目录在这个目录下使用hexo init初始化该目录 使用npm install安装需要的组件 使用npm install hexo-deployer-git --save安装插件 使用hexo generate或hexo g生成当前的博客 使用hexo server或hexo s启动服务器然后就可以打开浏览器访问localhost:4000在本地查看当前的博客 生成SSH密钥打开Git Bash，使用以下命令配置gitgit config --global user.name &quot;你的github用户的名字&quot;git config --global user.email &quot;你的github账户邮箱&quot;cd ~/.sshssh-keygen -t rsa -C &quot;你的github账户邮箱&quot;连续三个回车eval &quot;$(ssh-agent -s)&quot;，添加密钥到ssh-agentssh-add ~/.ssh/id_rsa，添加生成的SSH key到ssh-agentcat ~/.ssh/id_rsa.pub复制此时显示的内容，内容应该是以ssh-rsa开头 Ctrl+C退出后，在GitHub上新建一个新的仓库，仓库名随意，不过需要记录下来，我这里起名叫blog，最下面的Initialize this repository with a README要勾选上，然后保存即可。进入这个仓库后选择Settings，在左侧选项卡Options中翻到下面，GItHub Pages这项，Source选择master branch，选择save后，会在这部分的标题处写明这个仓库的url，这就是你博客的url了。还是页面的左侧的选项卡，Deploy 选择Add deploy key，添加密钥。Title随意，我设置为了blogKey粘贴我们刚才复制的那一段。最下面Allow write access要打勾.选择Add Key即可。然后在Git Bash中使用ssh -T git@github.com测试，如果看到Hi后面是你的用户名，就说明成功了。 修改hexo配置文件打开本地博客的根目录，找到_config.yml文件，在文件的开头处，第二部分，URL这部分改成如下内容： 123456# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: http://yoursite.com/blogroot: /blog/permalink: :year/:month/:day/:title/permalink_defaults: 这里的url和root这两项都需要修改。url在后面要加仓库名，我的仓库叫blog，所以写成了http://yoursite.com/仓库名，同理root要修改成/仓库名/。在文件的结尾处，Deployment这部分改成如下内容： 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: git@github.com:Davidham3/blog.git branch: master 需要注意的是，这里的repository这项，应该去GitHub里面你新建的那个叫blog的仓库里面找。进入仓库主页后，点击右侧绿色的按钮Clone or download，在新弹出的窗口右上角选择Use SSH，然后将下面的文字复制粘贴到此处。修改完配置文件后保存退出即可。 使用hexo clean清除缓存 使用hexo g生成博客 使用hexo deploy或hexo d将博客部署至GitHub上，打开刚才GitHub Pages设置里面给出的url，就可以进入你的博客了。以上两步也可以连写为hexo d -g。","link":"/blog/2018/02/20/在github上部署hexo/"},{"title":"复杂网络基础","text":"觉得最近应该把复杂网络的基础知识捡一捡，去年上课虽然学过，不过基本都忘了，什么betweeness，都不知道是什么了。最近要做复杂网络方面的研究，这些知识是必需的。打算看一下Newman的书”Networks an Introduction”。为什么我们对复杂网络感兴趣？ 很多系统都是由网络组成的。因特网，人类社会都是。有人研究个体组成，即顶点的性质，有人研究关系，即连接，还有人研究这些系统，也就是components和connections之间的pattern。 特殊的模式会在系统中产生很大影响。除非我们知道网络结构，否则很难完全理解整个系统如何工作。 网络是对一个系统的简单表示，只有基本的连接模式和其他的一点东西。顶点和边可以标记上额外的信息，来详细描述这个系统，但是在表示网络的时候通常会丢失信息。这就是它的缺点，但是也有优点。 这些年来，科学家们使用数学、计算机、统计学方法对复杂网络进行分析、建模、理解。很多工具从一个简单的网络表示开始，一组顶点和边，在适当的计算后会得到一些有用的信息：比如哪个是最好的顶点，或是一个顶点到另一个顶点的路径长度。其他工具使用可以做预测的网络的形式，比如互联网流量或传染病在社区内扩散的途径。因为这些工具是在抽象形式下工作，所以理论上可以应用到任何可以表示成网络的系统上。因此如果你感兴趣的系统可以表示成网络，那就有很多工具可以使用。当然不是所有的工具都能给出有效的结果，有些方法是用在特定问题上的。但是如果你有一个关于网络的well-posed question，大多数型框架，还是有一个可以解决这个问题的工具的。 一些网络的例子 最出名的且广泛研究的网络之一就是好论文。计算机组成的网络，计算机是顶点，物理数据连接是边，如光纤或电话线。图1.1展示了互联网的结构，2003年的一个snapshot，通过数据包的流通构建的网络。尽管互联网是人工制造的，而且是很细致的工程，但是我们并不知道它的结构是什么，因为它是由很多不同的人群构建出来的，大家并不了解其他人的行动，很少有中央控制。 有很多令人激动的实用原因让我们想研究因特网的结构。因特网的功能是在不同地方的计算机之间传播数据，他们会将数据分成片或包，然后再网络中的顶点间传送，直到这些包到达他们的目的地。网络的结构会影响他们完成这些任务的效率，而且如果我们直到网络的结构，我们可以解决很多实际问题。我们应该选择哪个路由来传输数据？最短路径总是最快的吗？如果不是，怎么才能找到最快的？我们怎么避免bottleneck使流量阻塞？当一个顶点或一条边挂掉时会发生什么？等等问题。 因特网结构的知识在研发新的通讯标准中是至关重要的。新的标准和协议还是为因特网所设计，老的会被翻新。协议的参数会为了网络的结构而优化。在网络的早些时候，不是网络结构的主要模型在调整过程中使用，而是那些更好的结构数据变得可获得后，我们可以更好的理解这些数据并且提升模型的性能。","link":"/blog/2018/07/21/复杂网络基础/"},{"title":"归并排序改进","text":"最近在看coursera上面普林斯顿大学的算法课，关于归并排序有三点改进方法。 当子序列的长度小于一定的阈值时，使用插入排序可以提升性能。 在归并两个子序列时，如果前一个子序列的最后一个值（最大值）小于后一个子序列的第一个值（最小值），说明当前的两个子序列拼起来已经是有序的了，可以跳过归并。 因为归并排序需要开辟一个新的临时空间，可以让这个临时空间和原来的空间交替使用，减少元素移动的开销。","link":"/blog/2019/11/08/归并排序改进/"},{"title":"vscode-remote-workspace","text":"使用vscode管理远程服务器上的文件与项目。 vscode-remote-workspace是一个vscode中的插件，可以管理远程存储上的文件、项目，还可以执行命令。支持的系统很多： Auzre Dropbox FTP FTPs S3 Buckets SFTP Slack WebDAV 以SFTP为例，只要写这么一个配置文件即可1234567{ &quot;folders&quot;: [{ &quot;uri&quot;: &quot;sftp://my-user:my-password@sftp.example.com/&quot;, &quot;name&quot;: &quot;My SFTP folder&quot; }], &quot;settings&quot;: {}} 举个例子：1234567{ &quot;folders&quot;: [{ &quot;uri&quot;: &quot;sftp://Davidham3:my-password@my-linux-server-ip/data/Davidham3&quot;, &quot;name&quot;: &quot;My SFTP folder&quot; }], &quot;settings&quot;: {}} 保存成名为my-linux-server.code-workspace的文件后，右键点击这个文件，使用vscode打开即可。或是打开vscode后，点击“文件”，选择“打开工作区”，然后选择这个文件即可。使用F1，然后输入execute remote command，然后就可以输入命令，直接在远程机器上运行。 安装方法打开vscode后，选择左侧第五个按钮，进入商店，然后查找vscode-remote-workspace，点击绿色的安装按钮安装即可，安装后点蓝色的“重新加载”按钮即可。 问题不过使用execute remote command的时候，如果程序可以正常运行，不报错，那这个工具是可以显示内容的，但是一旦程序出错了，就不会有任何错误信息显示。这点这个工具没法处理。所以解决方案就是，直接用下面的终端，ssh进去。最新版本的Windows10已经内置了OpenSSH，所以直接用ssh 用户名@hostname就可以连接到服务器，然后执行命令跑程序。","link":"/blog/2018/06/15/vscode-remote-workspace/"},{"title":"汉语分词最大匹配算法","text":"正向最大匹配，逆向最大匹配 汉语正向、逆向最大分词算法汉语分词最大匹配法(Maximum Matching)： 正向最大匹配算法(Forward MM) 逆向最大匹配算法(Backward MM) 算法假设句子：$S = c_1c_2···c_n$，某一词：$w_i = c_1c_2···c_m$，$m$为词典中最长词的字数。FMM 算法描述 令$i=0$，当前指针$p_i$指向输入字串的初始位置，执行下面的操作： 计算当前指针$p_i$到字串末端的字数（即未被切分字串的长度）$n$，如果$n=1$，转(4)，结束算法。否则，令$m=$词典中最长单词的字数，如果$n&lt;m$，令$m=n$。 从当前$p_i$起取$m$个汉字作为词$w_i$，判断：3.1. 如果$w_i$确实是词典中的词，则在$w_i$后添加一个切分标志，转(3.3);3.2. 如果$w_i$不是词典中的词且$w_i$的长度大于1，将$w_i$从右端去掉一个字，转(3.1)步；否则（$w_i$的长度等于1），则在$w_i$后添加一个切分标志，将$w_i$作为单字词添加到词典中，执行(3.3)步；3.3. 根据$w_i$的长度修改指针$p_i$的位置，如果$p_i$指向字串末端，转(4)，否则，$i=i+1$，返回(2)； 输出切分结果，结束分词程序。 逆向最大匹配算法同理。 数据人民日报语料，总共100344条样本。样例： ’/w ９９/m 昆明/ns 世博会/n 组委会/j 秘书长/n 、/w 云南省/ns 副/b 省长/n 刘/nr 京/nr 介绍/v 说/v ，/w ’/w ９９/m 世博会/j 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254# -*- coding:utf-8 -*-from collections import defaultdictimport numpy as npimport matplotlib.pyplot as pltimport seabornplt.style.use('fivethirtyeight')def readFile(filename): ''' read file return a generator, each element is one line Parameters ---------- filename: str, filename Returns: ---------- generator ''' with open(filename, 'r', encoding = 'utf-8') as f: data = f.readline().strip() while data: yield data try: data = f.readline().strip() except: print('read file failed in one line!') continuedef preprocess_text(text): ''' Parameters ---------- text: str, ’/w ９９/m 昆明/ns 世博会/n 组委会/j 秘书长/n 、/w 云南省/ns Returns ---------- str, ’９９昆明世博会组委会秘书长、云南省 ''' return ''.join(map(lambda x: x[: x.rfind('/')], text.split(' ')))def preprocess_text2(text): ''' Parameters ---------- text: str, ’/w ９９/m 昆明/ns 世博会/n 组委会/j 秘书长/n 、/w 云南省/ns Returns ---------- str, ’/９９/昆明/世博会/组委会/秘书长/、/云南省 ''' return ''.join(map(lambda x: x[:x.rfind('/')+1], text.split(' ')))def precision_recall_f1(output, target): ''' Parameters ---------- output, str target, str Returns ---------- precision, recall and f1, float ''' def extract_index_pair(text): o = [(0, 0)] index = 0 for i in text: if i != '/': index += 1 else: o.append((o[-1][-1], index)) else: o.append((o[-1][-1], index)) o = set(o) o.remove((0, 0)) return o o = extract_index_pair(output) t = extract_index_pair(target) def precision_score(o, t): count = 0 for i in t: if i in o: count += 1 return count / len(t) precision, recall = precision_score(o, t), precision_score(t, o) f1 = (2 * precision * recall) / (precision + recall) return precision, recall, f1def build_corpus_and_testing_text(filename, training_ratio = 0.7): ''' forward maximum matching Parameters ---------- filename: str training_ratio: float, ratio of training data Returns ---------- corpus: set testing_text: list, each element is a sentence that need to be preprocessed ''' corpus = set() num_of_lines = 0 for line in readFile(filename): num_of_lines += 1 all_index = np.arange(num_of_lines) np.random.shuffle(all_index) training_lines = set(all_index[:int(training_ratio * num_of_lines)].tolist()) testing_text = [] for index, line in enumerate(readFile(filename)): if index not in training_lines: testing_text.append(line) continue for temp in map(lambda x: x.split('/'), line.split(' ')): if len(temp) != 2: continue word, _ = temp if 'ｈｔｔｐ' in word or 'ｗｗｗ．' in word: continue corpus.add(word) return corpus, testing_textdef split_words(line, corpus_set): ''' forward maximum matching Parameters ---------- line: str, a chinese string corpus_set: set, corpus Returns ---------- str, results ''' n_line = len(line) start, end = 0, n_line result = [] while start &lt; n_line:# time.sleep(0.5)# print(result) n = n_line - start if n == 1: result.append(line[start:]) return '/'.join(result) + '/' current_word = line[start: end] if current_word in corpus_set: result.append(current_word) start = end end = n_line continue else: if len(current_word) == 1: corpus_set.add(current_word) result.append(current_word) start = end end = n_line continue end -= 1 continue start += 1 return '/'.join(result) + '/'def split_words_reverse(line, corpus_set): ''' backward maximum matching Parameters ---------- line: str, a chinese string corpus_set: set, corpus Returns ---------- str, results ''' n_line = len(line) start, end = 0, n_line result = [] while end &gt; 0:# time.sleep(0.5)# print(result) if (end - 0) == 1: result.append(line[start: end]) return '/'.join(reversed(result)) + '/' current_word = line[start: end] if current_word in corpus_set: result.append(current_word) end = start start = 0 continue else: if len(current_word) == 1: corpus_set.add(current_word) result.append(current_word) end = start start = 0 continue start += 1 continue end -= 1 return '/'.join(reversed(result)) + '/'def run(split_words_function, testing_text, corpus): p_r_f1 = [] results = [] for index, i in enumerate(testing_text): text = preprocess_text(i) target = preprocess_text2(i) output = split_words_function(text, corpus) p_r_f1.append(precision_recall_f1(output, target)) results.append(output) a, b, c = zip(*p_r_f1) average_precision = sum(a) / len(a) average_recall = sum(b) / len(b) average_f1 = sum(c) / len(c) print('average precision:', average_precision) print('average recall', average_recall) print('average F1-score', average_f1) plt.figure(figsize = (16, 4)) plt.subplot(121) plt.xticks(np.arange(0, 1.05, 0.1)) plt.hist(a, bins = np.arange(0, 1.05, 0.05)) plt.xlabel('precision') plt.ylabel('frequency') plt.title('precision') plt.subplot(122) plt.xticks(np.arange(0, 1.05, 0.1)) plt.hist(b, bins = np.arange(0, 1.05, 0.05)) plt.xlabel('recall') plt.ylabel('frequency') plt.title('recall') plt.show() return results, a, b, cif __name__ == '__main__': corpus_file = 'data/train_pd_utf8.txt' corpus, testing_text = build_corpus_and_testing_text(corpus_file) print('corpus size:', len(corpus)) print('testing data size:', len(testing_text)) results, a, b, c = run(split_words, testing_text, corpus) results, a, b, c = run(split_words_reverse, testing_text, corpus) 2.6 运行结果2.6.1 FMM运行结果 average-precision average-recall average-F1-score 0.9237 0.9198 0.9207 2.6.2 BMM运行结果 average-precision average-recall average-F1-score 0.9394 0.9269 0.9277","link":"/blog/2018/06/15/汉语分词最大匹配算法/"},{"title":"训练神经网络时归一化的目的","text":"在训练神经网络的时候，normalization是必不可少的，原因是如果不进行normalization，在更新参数的时候会出现zig zag的现象。在训练神经网络的时候，归一化是必不可少的。之前一直不理解为什么非要归一化，直到看了cs231n这门课才知道归一化的目的。事实上这个问题主要是针对激活函数来说，如果不归一化的话，那么激活函数在反向传播的时候就会出问题。 图1 左侧是原始数据，中间是中心化后的，右侧是归一化后的 图片来源于cs231n事实上归一化分为两个步骤，第一步是将数据变为以0为中心，第二部是缩小数据的范围。所以归一化的公式为：$$\\frac{X-\\bar{X}}{std(X)}$$其中，X为原始样本，$\\bar{X}$为样本均值，$std(X)$为样本标准差。在这里，真正影响反向传播的是第一步，zero-centered。如果没有将数据以0为中心中心化的话，就会影响反向传播的效果。以逻辑回归(Logistic Regression)为例，逻辑回归的模型可写为$$\\hat{y} = sigmoid(W \\cdot X+b)$$其中$W$和$b$是参数，X是样本，$sigmoid$表示sigmoid激活函数，设损失函数为$$L = Loss(y, \\hat{y})$$其中，$y$为样本的标签或标注值。在反向传播的时候，需要对$W$和$b$求偏导数，即求损失函数在当前样本点的梯度，这里我们设$Z = W \\cdot X + b$，则$$\\frac{\\partial{L}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{\\hat{y}}}\\frac{\\partial{\\hat{y}}}{\\partial{Z}}\\frac{\\partial{Z}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{\\hat{y}}}\\frac{\\partial{\\hat{y}}}{\\partial{Z}}X^T$$同理可以求出$b$的偏导数。在这里就可以看出问题，假设我们的输入是图像那样的样本，像素值都是大于0的，那这里$\\frac{\\partial{L}}{\\partial{W}}$就会大于0。使用梯度下降的更新规则来更新参数时$$W := W - \\alpha \\frac{\\partial{L}}{\\partial{W}}$$W就会一直减小，这显然是有问题的。 图2 右图展示了只有两个方向允许更新梯度后实际的参数更新路线(红线) 图片来源于cs231n如图2所示，可以发现如果我们的输入变成了要么都是大于0，要么都是小于0的数，那么允许梯度更新的两个方向在二维空间中就只能落在第一和第三象限中，扩展到高维空间中也是相对的两个卦限。这样在更新的过程中就会产生这种红线所示的路径zig zag path。以上是不进行中心化的后果。而不进行特征缩放的后果则是，如果每个特征的量级不同，假设一个特征是数值范围在$[-10, 10]$，另一个特征在$[-10^9, 10^9]$，那么在计算梯度后，使用梯度下降更新时，也会造成上面所述的zig zag现象。","link":"/blog/2018/02/21/训练神经网络时归一化的目的/"},{"title":"重装系统后hexo博客的恢复","text":"先重装一下node.js，然后把GitHub上面的密钥换一下好像就OK了。。。 重装node.js 更换GitHub上面仓库的密钥 随便写个文章push上去试试 不知道为什么好久之前的文章跑到首页来了。。。时间出现了问题？我折腾了一下，发现有两篇文章的时间hexo好像计算得不太对。。。明天把所有的插件删除再装一次试试，但是难点是我已经不太记得我装过哪些插件了。。。好像有个统计字数的，还有一个picture assets。","link":"/blog/2019/11/09/重装系统后hexo博客的恢复/"},{"title":"读研一周年随笔","text":"读研一周年总结今天有幸参加KDD CUP 2018的一个线下聚会，遇到了很多高手，学到了很多，主要是开拓了视野，优秀的人真的很多。 大四跨专业考研计算机成功。记得大四是6月30日毕业，正好周五，周末在家呆了两天马上就奔赴实验室了，7月3日周一，正式开始了我快(gou)乐(ri)的研究生之旅。 记得去年7月和8月，实验室新开了个交通数据挖掘方向，让我和我师姐一起去调研，然后就疯狂给甲方出方案，出文档，出PPT。当时反正也没学到什么，感受到的就是每天都在写文档，然后反复改，还得去甲方开会，感觉没啥意义，对我来说。那个暑假同时还负责研究hadoop HA的搭建，写了份初稿，然后发到简书上的博客去了，还被加入了什么专栏，前几天回头再看的时候感觉写的跟shi一样。暑假的时候第一次感受到了在北京挺辛苦的。当时没地方住，只能回家，回家坐公交车1个小时，就这样每天都得坐公交来回两个小时，晚上经常在车上睡着。当时项目上还有点开发任务，我记得当时用spark做了一些车辆轨迹的分析，当时学会了怎么写spark。然后就没啥了，那个暑假在coursera上面刷了一些机器学习的课，华盛顿大学开的，学了boosting啊，stacking啊那些东西。 然后不知不觉进入了9月份，开学的日子，开了学之后发现，卧槽老师一个比一个牛逼，讲课都好棒好棒的。等过了一段时间后，也就那么回事，可能是习惯了，就觉得还可以了。当时的机器学习和留学生一起上，还挺有意思的。周一到周五只要不上课，就在实验室呆着。上课的时候倒是还挺有意思，上人工智能的逻辑基础时，总是晚上上课，然后就困，虽说做第一排，但也会趴桌子睡着，最后考个第六，还可以。复杂网络的时候就一直玩手机了，复杂网络我们班同学选的还挺多，他们老在一起玩王者荣耀，挺有意思的，我现在还是挺羡慕他们上课一起玩游戏的，因为现在没课上了。。。复杂网络答辩前一个月，撸了好几篇论文，最后得了个第一，不过看的那几篇论文真是让我学到了很多东西，答辩时我说有一个算法只要调调参什么的，肯定最后效果很好，老师当时说不一定，当时我没有理解，后来我才理解了为什么不一定，后来随着看的论文多了，复现的多了，发现有些论文就是在扯淡，吹的挺好的，实际很渣。然后就是什么数学课啊，什么英语课，英语课最后也考了第一，主要是考研时英语打的底子有点深……感觉真的是终身受益，即便是读了研究生扇贝全家桶也一直在用。考的最渣的就是算法了，主要是后来都没怎么复习，觉得认真准备考试真的越来越没劲了，索性就看了看PPT，最后才考70多分，机器学习才80多。。。唉，就这样吧，毕竟考试分数代表不了任何东西，大家都在cramming，有什么意思呢。所以当时我就接着看论文，自己跑实验来着，那时候一直搞推荐系统来着，本科的时候就玩过，什么协同过滤啊那些玩意，当时机器学习老师给我们讲了矩阵分解的好几个算法，于是我就找了论文研读了一下，虽说都没有实现，但是确实懂了很多，在GitHub上还找到了一个开源实现的implicit feedback recommendation，那个代码真是牛逼，看都看不懂，cython的爆炸性能很强，自己写了很多爬虫，爬各种东西，抓取用户的行为数据，然后跑推荐。那个学期好像没什么印象深刻的事情，反正觉得实验室挺没劲的，项目很水，经常去清华科技园，给甲方运维去，找工作的时候都不好意思说这事。当时我们实验室有个很牛逼的师兄，推荐我去跟着李沐大神每周在斗鱼上的直播，学深度学习，然后我就去了，虽说没有每周都看直播，但还是经常看看录播的，学会了如何使用mxnet里面的gluon，然后就实现了一些网络，跑着玩来着。 寒假的时候，学校放假了，但我们实验室不放，就在实验室看论文来着，只不过老师让我看看群体个体出行轨迹什么的，反正就是研究用户出行规律的。当时我们学校交运学院的在nature communication上面发了论文，让我赶紧研究，我看了看，感觉和计算机研究的基本不相关，他们就是建模，然后用公式来做仿真那样的东西，也不是预测分类啊什么的。寒假在实验室水了水就过去了。然后放我们实验室的寒假了，17天，在家疯狂看cs231n。越看越爽，Justin讲的太好了，还有那个女博士，讲的很棒，学到了很多知识，为什么必须要均一化啊，常用的优化算法的参数啊，各种小trick，现在一看还真没什么大不了的，但那时候给我帮助很大。我就反复的跑mnist，发现accuracy一直在上升，每天都有成就感，寒假顺便还把我的博客搭建起来了，在GitHub上面。放完实验室的寒假，就回实验室接着干活了。忘了干什么，反正马上开学了。 开学之后还是，每周都往甲方跑，前几周好像还是运维，后面就做数据分析了。数据分析就更扯了……用pandas疯狂做各种数据的统计，各种指标全靠自己想，然后画图，写文档。异常坑爹加没劲，做完了也没有卵用，甲方看完连个反馈都tm没有。然后这个项目没弄完呢，又开辟新项目了，让我也参与那边的项目，那边要用的hbase我根本没怎么接触过，那整个4月份，是我最难受的一个月，有一次直接被气哭了，我来读研又不是来学这些的，当时直接不想念了，接着读也没有必要，觉得自己其实就是进错了实验室，天天干些low到不行的活儿，宝贵的时间就这么一分一秒的没了。后来老师也不让我接着去之前那个甲方出差了，那些low活也不让我干了，让我开始学习。4月份的时候看了不少论文，也复现了很多，两版ResNet，一版风格迁移，一版perceptual loss，当时就一直在看CV的论文，学到了不少东西，比如卷积核如何提取特征什么的，感觉那时看的几篇论文又一次拓宽了知识面，之后就是看了一些nlp方向的，机器翻译、NER一类的。5月份的时候还去北大听了MSRA周明博士的讲座，最后的感觉是：牛逼！知道了什么是深入浅出！那时候5月份的时候正好赶上院里面的活动，班级风采大赛，这是我研究生期间参加过的最有意思的活动，也是最难忘的。我们班同学都太给力了，真的很令人感动，演员们真是太用心了。记得当时想剧本，真是想破脑袋了，经常去开会，大家在一起讨论，经常就讨论不出来了，没有灵感了，不过最后总是能突然闪出一个想法，然后大家就在一起讨论。后来活动结束后，大家还一起出去玩，520那天，漂流的时候一个比一个惨哈哈哈哈，5月真的是我最快乐的一段时光，永生难忘。那之后，我就变了，我觉得过得快乐不快乐完全取决于自己是如何看待周围这些事情的，本科的时候没有那么多破事，或者说本科的时候能逃避的都逃避了，但是研究生期间，天天有破事，时间根本不是自己来把控，是boss，是周围的各种有的没的。之后我就把所有的事情都看淡了，快乐还是很重要的，如果自己不能让自己保持一个快乐的心情，那很有可能就是每天状态都非常不好，整天郁郁寡欢的。6月份过的很平静，每天写写作业，帮同学们解答问题，处理他们的虚拟机啊什么的。其他时间看看论文，逛逛GitHub，找一些有意思的工具，然后提提issue啥的。然后就是准备考试，因为一切都看淡了，所以考试成绩都挺惨的……高级D课直接挂了，挂了就挂了吧。平时专业课什么的不挂就行。考试周的时候，抽一部分时间看看考试，其他时间干自己的事情，复现复现论文什么的。 7月份，又迎来了暑假，暑假只有9天，我决定八月底放暑假，因为放的早就没盼头了。。。暑假项目也正式开工了，监督学妹做项目，我接着干自己的事情，搞研究。今天去KDD CUP 2018线下聚会，开拓了视野，我之前一直认为，搞什么数据挖掘，机器学习，没有牛逼的学历，没有牛逼的背景，还搞毛啊，今天发现，不是这样子的，我已经忘了我为什么当初会考计算机，当初考计算机的一个直接原因就是，这行不是靠实力吃饭的吗，但是读了研究生之后，看师兄们找工作的时候，说和北邮啊比不了什么的，潜意识中就逐渐变成了校名决定自己能去哪里，出身决定一切。读研这一年，总会把自己，拿去和那些特别牛逼的选手作比较，总会把自己，和清北那些学生作比较，总觉得平时干那些运维的活儿，就是浪费时间，时间一点一点的浪费着，和那些牛逼的选手越差越远，最后找了个普通工作，学习也变得越来越功利。今天的聚会里面，很多人都没有那么过硬的背景，有个大神，大学都没上，参加了自考，机器学习这些人家全都是自学的，现在马上就要被某银行破格录用，正在走程序。今天他的一个领导还去了那个活动，说想找那种觉得跑模型比打游戏有意思的同学去他们银行，那一刻我才真正的意识到，其实这世界上有很多人热爱计算机，热爱机器学习的，觉得跑模型比打游戏有意思的人多了去了，只不过我一直局限在自己的小圈子里，局限在自己的实验室里面，没有接触到这些人。我考试周之所以不怎么复习，这两次都是因为在跑模型，感觉跑模型上瘾啊，停不下来。 总的来说，读研一年了，实验室挺没意思的，当初是非常期望去实验室学习，早点进实验室，因为当时对实验室有着憧憬，进去之后一堆大神，像我这种跨专业的菜鸡可以学到很多东西。进了实验室后发现，实验室除了提供一个好的环境，有时候连氛围，都没有。打比赛连队友都找不到。。。后来看淡了这一切也就无所谓了，环境无法改变的话，那就不要让环境把自己改变，坚信自己认为对的事情就好了。暑假任务很明确，好好学习，好好看论文，今天主办方说以后好搞将论文，分享模型的活动，看看到时候能不能投投稿，赚赚稿费。然后想做些有意义的事情，争取明年找工作前能发一篇顶会，或者发个sci。然后找个比赛，认真打一下，多参加参加这种线下活动，和大神们多交流，学习，差不多这就是今年夏天到明年夏天的任务。 感觉这一年迷失了双眼。希望新的一年，目标更明确，能更努力的去提升自己，多做有意义的事情。","link":"/blog/2018/07/08/读研一周年随笔/"},{"title":"迁移CentOS7虚拟机mac和网卡名变换导致网络不通的问题","text":"vcenter迁移虚拟机的时候，迁移之后虚拟机网络不通。 参考：解决CentOS 7虚拟机克隆的网络问题 使用vcenter的迁移后，虚拟机出现了网络不通的现象，仔细观察可以发现vcenter给虚拟机分配了新的mac地址。因为Linux系统会记录mac地址与网卡名的关系，所以Linux系统在运行后，发现mac变了，于是会给当前这张网卡分配一个新的网卡名。解决方案就是： 修改网卡配置文件/etc/sysconfig/network-scripts/ifcfg-eno16884287 删除UUID这一行，因为每张网卡的mac地址是不一样的，所以UUID也是不一样的。 修改HWADDR为虚拟机克隆后的MAC地址 进入/etc/udev/rules.d/这个目录，将里面的.rules文件改名 mv 70-persistent-ipoib.rules 70-persistent-ipoib.rules.bak mv 90-eno-fix.rules 90-eno-fix.rules.bak 重启 reboot","link":"/blog/2018/06/12/迁移centos7虚拟机mac和网卡名变换导致网络不同的问题/"},{"title":"门控卷积网络语言建模","text":"ICML 2017，大体思路：卷积+一个线性门控单元，替代了传统的RNN进行language modeling，后来的Facebook将这个用于机器翻译，提出了卷积版的seq2seq模型。原文链接：Language Modeling with Gated Convolutional Networks 摘要当前流行的语言建模模型是基于RNN的。在这类任务上的成功经常和他们捕捉unbound context有关。这篇文章中我们提出了一个通过堆叠convolutions的finite context方法，卷积可以变得更有效因为他们可以在序列上并行。我们提出了一个新型的简单的门控机制，这个门控机制表现的要比Oord et al.(2016b)要好，我们也探究了关键架构决策的影响。我们提出的方法在WikiText-103上达到了最好的效果，even though it features long-term dependencies，在Google Billion Words上也达到了最好的效果。Our model reduces the latency to score a sentece by an order of magnitude compared to a recurrent baseline. 据我们所知，这是在大规模语言任务上第一次一个非循环结构的方法超越了强有力的循环模型。 引言统计语言模型估计一个单词序列的概率分布，通过给定当前的单词序列，对下一个单词的概率进行建模$$P(w_0, …, w_N) = P(w_0)\\prod^N_{i=1}P(w_i \\mid w_0, …, w_{i-1})$$其中$w_i$是单词表中的单词下标。语言模型对语音识别(Yu &amp; Deng, 2014)和机器翻译(Koehn, 2010)来说是很重要的一部分。最近，神经网络(Bengio et al., 2014; Mikolov et al., 2010; Jozefowicz et al., 2016)已经展示出了比传统n-gram模型(Kneser &amp; Ney, 1995; Chen &amp; Goodman, 1996)更好的语言模型。这些传统模型不能解决数据稀疏的问题，这个问题导致这些方法很难对大量的上下文进行表示，因此也不能回长范围的依赖进行表示。神经语言模型通过在连续空间中对词的嵌入解决了这个问题。当前最好的语言模型是基于LSTM（Hochreiter et al., 1997）的模型，LSTM理论上可以对任意长度的依赖进行建模。在这篇文章中，我们引入了新的门控卷积网络，并且用它进行语言建模。卷积网络可以被堆叠起来来表示大量的上下文并且在越来越长的有着抽象特征（LeCun &amp; Bengio, 1995）的上下文中提取层次特征。这使得这些模型可以通过上下文为$N$，卷积核宽度为$k$，$O(\\frac{N}{k})$的操作对长时间的依赖关系进行建模。相反，循环网络将输入看作是一个链式结构，因此需要一个线性时间$O(N)$的操作。层次的分析输入与传统的语法分析相似，传统的语法分析建立粒度增加的语法树结构，比如，包含名词短语和动词短语的句子，短语中又包含了更内在的结构（Manning &amp; Schutze, 1999; Steedman, 2002）。层次结构也会让学习变得更简单，因为对于一个给定的上下文大小，相比链式结构，非线性单元的数量会减少，因此减轻了梯度消失的问题（Glorot &amp; Bengio, 2010）。现代的硬件对高度并行的模型支持的很好。在循环神经网络中，下一个输出依赖于之前的隐藏状态，而之前的隐藏状态在序列中元素上是不能并行的。然而，卷积神经网络对这个计算流程支持的很好因为卷积是可以在输入元素上同时进行的。对于RNN来说想要达到很好的效果（Jozefowicz et al., 2016），门的作用很重要。我们的门控线性单元为深层的结构对梯度提供了一条线性的通道，同时又保留了非线性的特性，减少了梯度消失的现象。我们展示了门控卷积网络比其他的已经发表的语言模型都要好，比如在Google Billion Word Benchmark（Chelba et al., 2013）上的LSTM。我们也评估了我们的模型在处理长范围依赖关系WikiText-103上的能力，在这个数据集上，模型是以段落为条件进行输入的，而不是一个句子，我们在这个数据集（Merity et al., 2016）上获得了最好的效果。最后，我们展示了门控线性单元获得了更好的精度以及相比于Oord et al., 2016的LSTM门收敛的更快。 方法在这篇文章中我们引入了一种新的神经语言模型，这种模型使用门控时间卷积替代了使用在循环神经网络中使用的循环链接。神经语言模型（Bengio et al., 2003）提供了一种对每个单词$w_0, …, w_N$的上下文表示$H=[h_0, …, h_N]$用来预测下一个词的概率$P(w_i \\mid h_i)$。循环神经网络$f$通过一个循环函数$h_i = f(h_{i-1}, w_{i-1})$计算$H$，这个循环函数本质上是一种不能并行处理的序列操作。我们提出的方法使用函数$f$对输入进行卷积来获得$H = f \\ast w$并且因此没有时间上的依赖，所以它能更好的在句子中的单词上并行计算。这个过程将会把许多前面出现的单词作为一个函数进行计算。对比卷积神经网络，上下文的大小是有限的，但是我们展示出了有限的上下文大小不是必须的，而且我们的模型可以表示足够大的上下文并表现的很好。图1展示了模型的架构。词通过一个嵌入的向量进行表示，这些表示存储在lookup table$\\mathbf{D}^{\\vert \\mathcal{V} \\vert \\times e}$中，其中$\\vert \\mathcal{V} \\vert$是词库中单词的数量，$e$是嵌入的大小。我们模型的输入是一个词序列$w_0, …, w_N$，这个序列被词向量表示为$E = [D_{w_0}, …, D_{w_N}]$。我们将隐藏层$h_0, …, h_L$计算为$$h_l(X) = (X \\ast W + b) \\otimes \\sigma(X \\ast V + c)$$其中，$m$，$n$分别是输入和输出的feature map的数量，$k$是patch size，$X \\in \\mathbb{R}^{N \\times m}$是层$h_l$的输入（要么是词嵌入，要么是前一层的输出）,$W \\in \\mathbb{R}^{k \\times m \\times n}$，$b \\in \\mathbb{R}^n$，$V \\in \\mathbb{R}^{k \\times m \\times n}$，$c \\in \\mathbb{R}^n$是学习到的参数，$\\sigma$是sigmoid function，$\\otimes$是矩阵间的element-wise product。当卷积输入时，我们注意$h_i$不包含未来单词的信息。我们通过移动卷积输入来防止卷积核看到未来的上下文（Oord et al., 2016a）来解决这个问题。特别地，我们在序列的开始加入了$k-1$宽度的0作为padding补全，假设第一个输入的元素是序列的开始元素，起始的标记我们是不预测的，$k$是卷积核的宽度。每层的输出是一个线性变换$X \\ast W + b$通过门$\\sigma(X \\ast W + b)$调节。与LSTM相似的是，这些门乘以矩阵的每个元素$X \\ast W + b$，并且以层次的形式控制信息的通过。我们称这种门控机制为Gated Linear Units(GLU)。通过在输入$E$上堆叠多个这样的层，可以得到每个词$H = h_L \\circ … \\circ h_0(E)$的上下文表示。我们将卷积和门控线性单元放在了一个preactivation residual block，这个块将输入与输出相加（He et al., 2015a）。这个块有个bottleneck结构，可以使计算更高效并且每个块有5层。获得模型预测结果最简单的是使用softmax层，但是这个选择对于语料库很大和近似来说一般计算起来很慢，像noise contrastive estimation(Gutmann &amp; Hyvarinen)或层次softmax(Morin &amp; Bengio, 2005)一般更常用。我们选了后者的改良版adaptive softmax，这个算法将higher capacity分配给出现频率更高的单词，lower capacity分配给频率低的单词（Grave et al., 2016a）。这使得在训练和测试的时候内存占用更少且计算速度更快。 门控机制门控机制控制了网络中信息流通的路径，在循环神经网络中已经证明了是非常有效的手段（Hochreiter &amp; Schumidhuber, 1997）。LSTM通过输入和遗忘门控制分离的细胞使得LSTM获得长时间的记忆。这使得信息可以不受阻碍的流通多个时间步。没有这些门，信息会在通过时间步的转移时轻易地消失。与之相比，卷积神经网络不会遇到这样的梯度消失现象，我们通过实验发现卷积神经网络不需要遗忘门。因此，我们认为模型只需要输出门，这个门可以控制信息是否应该通过这些层。我们展示了这个模型对语言建模很有效，因为它可以使模型选择预测下一个单词的时候哪个单词是相关的。和我们同时进行研究的，Oord et al.(2016b)展示了LSTM风格的门控机制，$tanh(X \\ast W + b) \\otimes \\sigma(X \\ast V + c)$在对图像进行卷积建模的有效性。后来，Kalchbrenner et al. (2016)在翻译和字符级别的语言建模上使用额外的门扩展了这个机制。门控线性单元是一种简化的门控机制，基于Dauphin &amp; Grangier(2015)对non-deterministic gates的研究，这个门可以通过和门组合在一起的线性单元减少梯度消失的问题。这个门尽管允许梯度通过线性单元进行传播而不发生缩放的变化，但保持了层非线性的性质。我们称之为gated tanh unit(GTU)的LSTM风格的门的梯度是：$$\\nabla[tanh(X) \\otimes \\sigma(X)]=tanh’(X) \\nabla X \\otimes \\sigma(X) + \\sigma’(X) \\nabla X \\otimes tanh(X)$$注意到随着我们堆叠的层数的增加，它会渐渐地消失，因为$tanh’(X)$和$\\sigma’(X)$这两个因数的数值范围在减小。相对来说，门控线性单元的梯度：$$\\nabla [X \\otimes \\sigma(X)] = \\nabla X \\otimes \\sigma(X) + X \\otimes \\sigma’(X) \\nabla X$$有一条路径$\\nabla X \\otimes \\sigma(X)$对于在$\\sigma(X)$中的激活的门控单元没有减小的因数。这可以被理解为一个跳过乘法的连接帮助梯度传播过这些层。我们通过实验比较了不同的门策略后发现门控线性单元可以收敛地更快且困惑度的值更好。","link":"/blog/2018/05/23/门控卷积网络语言建模/"},{"title":"神经网络基础","text":"最近给本科生当机器学习课程的助教，给他们出的作业题需要看这些图，懒得放本地了，直接放博客里。发现jupyter导出markdown好方便，放到博客里面正好，改都不用改。 原来就想过一个问题，为什么我写出来的神经网络不收敛，loss会像火箭一样直接飞了。后来看了一些教程，发现有人在做梯度下降的时候，把梯度除以了梯度的二范数，我尝试之后发现还真好使了，在实验的时候发现是因为没有对数据集进行归一化，如果所有的数据都是很大的数，那么在反向传播的时候，计算出来的梯度的数量级会很大，这就导致更新得到的参数的数量级也很大，预测出的偏差就更大了，然后循环往复，如果给梯度除以一个梯度的二范数，其实就相当于把梯度的数量级降了，这样就可以训练了。但实际上还是将原始数据归一化比较好，对原始数据归一化还能让梯度下降的方向更多。如果数据都是正数，那下降方向会少很多，下降的时候会出现zig-zag现象。 第二题：神经网络：线性回归实验内容： 学会梯度下降的基本思想 学会使用梯度下降求解线性回归 了解归一化处理的作用 线性回归 我们来完成最简单的线性回归，上图是一个最简单的神经网络，一个输入层，一个输出层，没有激活函数。我们记输入为$X \\in \\mathbb{R}^{n \\times m}$，输出为$Z \\in \\mathbb{R}^{n}$。输入包含了$n$个样本，$m$个特征，输出是对这$n$个样本的预测值。输入层到输出层的权重和偏置，我们记为$W \\in \\mathbb{R}^{m}$和$b \\in \\mathbb{R}$。输出层没有激活函数，所以上面的神经网络的前向传播过程写为： $$Z = XW + b$$ 我们使用均方误差作为模型的损失函数 $$\\mathrm{loss}(y, \\hat{y}) = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat{y_i})^2$$ 我们通过调整参数$W$和$b$来降低均方误差，或者说是以降低均方误差为目标，学习参数$W$和参数$b$。当均方误差下降的时候，我们认为当前的模型的预测值$Z$与真值$y$越来越接近，也就是说模型正在学习如何让自己的预测值变得更准确。 在前面的课程中，我们已经学习了这种线性回归模型可以使用最小二乘法求解，最小二乘法在求解数据量较小的问题的时候很有效，但是最小二乘法的时间复杂度很高，一旦数据量变大，效率很低，实际应用中我们会使用梯度下降等基于梯度的优化算法来求解参数$W$和参数$b$。 梯度下降梯度下降是一种常用的优化算法，通俗来说就是计算出参数的梯度（损失函数对参数的偏导数的导数值），然后将参数减去参数的梯度乘以一个很小的数（下面的公式），来改变参数，然后重新计算损失函数，再次计算梯度，再次进行调整，通过一定次数的迭代，参数就会收敛到最优点附近。 在我们的这个线性回归问题中，我们的参数是$W$和$b$，使用以下的策略更新参数： $$W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}$$ $$b := b - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b}$$ 其中，$\\alpha$ 是学习率，一般设置为0.1，0.01等。 接下来我们会求解损失函数对参数的偏导数。 损失函数MSE记为： $$\\mathrm{loss}(y, Z) = \\frac{1}{n} \\sum^n_{i = 1} (y_i - Z_i)^2$$ 其中，$Z \\in \\mathbb{R}^{n}$是我们的预测值，也就是神经网络输出层的输出值。这里我们有$n$个样本，实际上是将$n$个样本的预测值与他们的真值相减，取平方后加和。 我们计算损失函数对参数$W$的偏导数，根据链式法则，可以将偏导数拆成两项，分别求解后相乘： 这里我们以矩阵的形式写出推导过程，感兴趣的同学可以尝试使用单个样本进行推到，然后推广到矩阵形式 $$\\begin{aligned}\\frac{\\partial \\mathrm{loss}}{\\partial W} &amp;= \\frac{\\partial \\mathrm{loss}}{\\partial Z} \\frac{\\partial Z}{\\partial W}\\\\&amp;= - \\frac{2}{n} X^\\mathrm{T} (y - Z)\\\\&amp;= \\frac{2}{n} X^\\mathrm{T} (Z - y)\\end{aligned}$$ 同理，求解损失函数对参数$b$的偏导数: $$\\begin{aligned}\\frac{\\partial \\mathrm{loss}}{\\partial b} &amp;= \\frac{\\partial \\mathrm{loss}}{\\partial Z} \\frac{\\partial Z}{\\partial b}\\\\&amp;= - \\frac{2}{n} \\sum^n_{i=1}(y_i - Z_i)\\\\&amp;= \\frac{2}{n} \\sum^n_{i=1}(Z_i - y_i)\\end{aligned}$$ 因为参数$b$对每个样本的损失值都有贡献，所以我们需要将所有样本的偏导数都加和。 其中，$\\frac{\\partial \\mathrm{loss}}{\\partial W} \\in \\mathbb{R}^{m}$，$\\frac{\\partial \\mathrm{loss}}{\\partial b} \\in \\mathbb{R}$，求解得到的梯度的维度与参数一致。 完成上式两个梯度的计算后，就可以使用梯度下降法对参数进行更新了。 训练神经网络的基本思路： 首先对参数进行初始化，对参数进行随机初始化（也就是取随机值） 将样本输入神经网络，计算神经网络预测值 $Z$ 计算损失值MSE 通过 $Z$ 和 $y$ ，以及 $X$ ，计算参数的梯度 使用梯度下降更新参数 循环1-5步，在反复迭代的过程中可以看到损失值不断减小的现象，如果没有下降说明出了问题 接下来我们来实现这个最简单的神经网络。 1. 导入数据使用kaggle房价数据，选3列作为特征 123456789101112import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inline# 读取数据data = pd.read_csv('data/kaggle_house_price_prediction/kaggle_hourse_price_train.csv')# 使用这3列作为特征features = ['LotArea', 'BsmtUnfSF', 'GarageArea']target = 'SalePrice'data = data[features + [target]] 2. 数据预处理40%做测试集，60%做训练集 12from sklearn.model_selection import train_test_splittrainX, testX, trainY, testY = train_test_split(data[features], data[target], test_size = 0.4, random_state = 32) 训练集876个样本，3个特征，测试集584个样本，3个特征 1trainX.shape, trainY.shape, testX.shape, testY.shape 3. 参数初始化这里，我们要初始化参数$W$和$b$，其中$W \\in \\mathbb{R}^m$，$b \\in \\mathbb{R}$，初始化的策略是将$W$初始化成一个随机数矩阵，参数$b$为0。 123456789101112131415161718192021222324def initialize(m): ''' 参数初始化，将W初始化成一个随机向量，b是一个长度为1的向量 Parameters ---------- m: int, 特征数 Returns ---------- W: np.ndarray, shape = (m, ), 参数W b: np.ndarray, shape = (1, ), 参数b ''' # 指定随机种子，这样生成的随机数就是固定的了，这样就可以与下面的测试样例进行比对 np.random.seed(32) W = np.random.normal(size = (m, )) * 0.01 b = np.zeros((1, )) return W, b 4. 前向传播这里，我们要完成输入矩阵$X$在神经网络中的计算，也就是完成 $Z = XW + b$ 的计算。 1234567891011121314151617181920212223def forward(X, W, b): ''' 前向传播，计算Z = XW + b Parameters ---------- X: np.ndarray, shape = (n, m)，输入的数据 W: np.ndarray, shape = (m, )，权重 b: np.ndarray, shape = (1, )，偏置 Returns ---------- Z: np.ndarray, shape = (n, )，线性组合后的值 ''' # 完成Z = XW + b的计算 # YOUR CODE HERE Z = np.dot(X, W) + b return Z 1234# 测试样例Wt, bt = initialize(trainX.shape[1])tmp = forward(trainX, Wt, bt)print(tmp.mean()) # -28.37377 5. 损失函数接下来编写损失函数，我们以均方误差(MSE)作为损失函数，需要大家实现MSE的计算： $$\\mathrm{loss}(y, Z) = \\frac{1}{n} \\sum^n_{i = 1} (y_i - Z_i)^2$$ 123456789101112131415161718192021def mse(y_true, y_pred): ''' MSE，均方误差 Parameters ---------- y_true: np.ndarray, shape = (n, )，真值 y_pred: np.ndarray, shape = (n, )，预测值 Returns ---------- loss: float，损失值 ''' # 计算MSE # YOUR CODE HERE loss = ((y_true - y_pred) ** 2).sum() / len(y_true) return loss 1234# 测试样例Wt, bt = initialize(trainX.shape[1])tmp = mse(trainY, forward(trainX, Wt, bt))print(tmp) # 39381033680.5 6. 反向传播这里我们要完成梯度的计算，也就是计算出损失函数对参数的偏导数的导数值： $$\\frac{\\partial \\mathrm{loss}}{\\partial W} = \\frac{2}{n} X^\\mathrm{T} (Z - y)$$ $$\\frac{\\partial \\mathrm{loss}}{\\partial b} = \\frac{2}{n} \\sum^n_{i=1}(Z_i - y_i)$$ 12345678910111213141516171819202122232425262728293031def compute_gradient(X, Z, y_true): ''' 计算梯度 Parameters ---------- X: np.ndarray, shape = (n, m)，输入的数据 Z: np.ndarray, shape = (n, )，线性组合后的值 y_true: np.ndarray, shape = (n, )，真值 Returns ---------- dW, np.ndarray, shape = (m, ), 参数W的梯度 db, np.ndarray, shape = (1, ), 参数b的梯度 ''' n = len(y_true) # 计算W的梯度 # YOUR CODE HERE dW = np.dot(X.T, (Z - y_true)) * 2 / n # 计算b的梯度 # YOUR CODE HERE db = (Z - y_true).sum() / n return dW, db 1234567# 测试样例Wt, bt = initialize(trainX.shape[1])Zt = forward(trainX, Wt, bt)dWt, dbt = compute_gradient(trainX, Zt, trainY)print(dWt.shape) # (3,)print(dWt.mean()) # -1532030241.25print(dbt.mean()) # -182154.277882 7. 梯度下降这部分需要实现梯度下降的函数$$W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}$$ $$b := b - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b}$$ 1234567891011121314151617181920212223def update(dW, db, W, b, learning_rate): ''' 梯度下降，参数更新，不需要返回值，W和b实际上是以引用的形式传入到函数内部， 函数内改变W和b会直接影响到它们本身 Parameters ---------- dW, np.ndarray, shape = (m, ), 参数W的梯度 db, np.ndarray, shape = (1, ), 参数b的梯度 W: np.ndarray, shape = (m, )，权重 b: np.ndarray, shape = (1, )，偏置 learning_rate, float，学习率 ''' # 更新W W -= learning_rate * dW # 更新b b -= learning_rate * db 123456789101112# 测试样例Wt, bt = initialize(trainX.shape[1])print(Wt.mean()) # 0.00405243937693print(bt.mean()) # 0.0Zt = forward(trainX, Wt, bt)dWt, dbt = compute_gradient(trainX, Zt, trainY)update(dWt, dbt, Wt, bt, 0.01)print(Wt.shape) # (3,)print(Wt.mean()) # 15320302.4166print(bt.mean()) # 1821.54277882 完成整个参数更新的过程，先计算梯度，再更新参数，将compute_gradient和update组装在一起。 1234567891011121314151617181920212223242526def backward(X, Z, y_true, W, b, learning_rate): ''' 使用compute_gradient和update函数，先计算梯度，再更新参数 Parameters ---------- X: np.ndarray, shape = (n, m)，输入的数据 Z: np.ndarray, shape = (n, )，线性组合后的值 y_true: np.ndarray, shape = (n, )，真值 W: np.ndarray, shape = (m, )，权重 b: np.ndarray, shape = (1, )，偏置 learning_rate, float，学习率 ''' # 计算参数的梯度 # YOUR CODE HERE dW, db = compute_gradient(X, Z, y_true) # 更新参数 # YOUR CODE HERE update(dW, db, W, b, learning_rate) 1234567891011# 测试样例Wt, bt = initialize(trainX.shape[1])print(Wt.mean()) # 0.00405243937693print(bt.mean()) # 0.0Zt = forward(trainX, Wt, bt)backward(trainX, Zt, trainY, Wt, bt, 0.01)print(Wt.shape) # (3,)print(Wt.mean()) # 15320302.4166print(bt.mean()) # 1821.54277882 8. 训练123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def train(trainX, trainY, testX, testY, W, b, epochs, learning_rate = 0.01, verbose = False): ''' 训练，我们要迭代epochs次，每次迭代的过程中，做一次前向传播和一次反向传播，更新参数 同时记录训练集和测试集上的损失值，后面画图用。然后循环往复，直到达到最大迭代次数epochs Parameters ---------- trainX: np.ndarray, shape = (n, m), 训练集 trainY: np.ndarray, shape = (n, ), 训练集标记 testX: np.ndarray, shape = (n_test, m)，测试集 testY: np.ndarray, shape = (n_test, )，测试集的标记 W: np.ndarray, shape = (m, )，参数W b: np.ndarray, shape = (1, )，参数b epochs: int, 要迭代的轮数 learning_rate: float, default 0.01，学习率 verbose: boolean, default False，是否打印损失值 Returns ---------- training_loss_list: list(float)，每迭代一次之后，训练集上的损失值 testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值 ''' training_loss_list = [] testing_loss_list = [] for epoch in range(epochs): # 这里我们要将神经网络的输出值保存起来，因为后面反向传播的时候需要这个值 Z = forward(trainX, W, b) # 计算训练集的损失值 training_loss = mse(trainY, Z) # 计算测试集的损失值 testing_loss = mse(testY, forward(testX, W, b)) # 将损失值存起来 training_loss_list.append(training_loss) testing_loss_list.append(testing_loss) # 打印损失值，debug用 if verbose: print('epoch %s training loss: %s'%(epoch+1, training_loss)) print('epoch %s testing loss: %s'%(epoch+1, testing_loss)) print() # 反向传播，参数更新 backward(trainX, Z, trainY, W, b, learning_rate) return training_loss_list, testing_loss_list 1234567891011# 测试样例Wt, bt = initialize(trainX.shape[1])print(Wt.mean()) # 0.00405243937693print(bt.mean()) # 0.0training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, Wt, bt, 2, learning_rate = 0.01, verbose = False)print(training_loss_list) # [39381033680.460075, 3.3902307664083424e+23]print(testing_loss_list) # [38555252685.093872, 4.1516070070405267e+23]print(Wt.mean()) # -5.70557904608e+13print(bt.mean()) # -4412133889.08 9. 检查编写一个绘制损失值变化曲线的函数 一般我们通过绘制损失函数的变化曲线来判断模型的拟合状态。 一般来说，随着迭代轮数的增加，训练集的loss在下降，而测试集的loss在上升，这说明我们正在不断地让模型在训练集上表现得越来越好，在测试集上表现得越来越糟糕，这就是过拟合的体现。 如果训练集loss和测试集loss共同下降，这就是我们想要的结果，说明模型正在很好的学习。 1234567891011121314151617def plot_loss_curve(training_loss_list, testing_loss_list): ''' 绘制损失值变化曲线 Parameters ---------- training_loss_list: list(float)，每迭代一次之后，训练集上的损失值 testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值 ''' plt.figure(figsize = (10, 6)) plt.plot(training_loss_list, label = 'training loss') plt.plot(testing_loss_list, label = 'testing loss') plt.xlabel('epoch') plt.ylabel('loss') plt.legend() 上面这些函数就是完成整个神经网络需要的函数了 函数名 功能 initialize 参数初始化 forward 给定数据，计算神经网络的输出值 mse 给定真值，计算神经网络的预测值与真值之间的差距 backward 计算参数的梯度，并实现参数的更新 compute_gradient 计算参数的梯度 update 参数的更新 backward 计算参数梯度，并且更新参数 train 训练神经网络 plot_loss_curve 绘制损失函数的变化曲线 我们使用参数初始化函数和训练函数，完成神经网络的训练。 12345678# 特征数mm = trainX.shape[1]# 参数初始化W, b = initialize(m)# 训练20轮，学习率为0.01training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, W, b, 20, learning_rate = 0.01, verbose = True) 绘制损失值的变化曲线 1plot_loss_curve(training_loss_list, testing_loss_list) 通过打印损失的信息我们可以看到损失值持续上升，这就说明哪里出了问题。但是如果所有的测试样例都通过了，就说明我们的实现是没有问题的。运行下面的测试样例，观察哪里出了问题。 123456789101112131415# 测试样例Wt, bt = initialize(trainX.shape[1])print('epoch 0, W:', Wt) # [-0.00348894 0.00983703 0.00580923]print('epoch 0, b:', bt) # [ 0.]print()Zt = forward(trainX, Wt, bt)dWt, dbt = compute_gradient(trainX, Zt, trainY)print('dWt:', dWt) # [ -4.18172940e+09 -2.19880296e+08 -1.94481031e+08]print('db:', dbt) # -182154.277882print()update(dWt, dbt, Wt, bt, 0.01)print('epoch 1, W:', Wt) # [ 41817293.96016914 2198802.97412493 1944810.31544994]print('epoch 1, b:', bt) # [ 1821.54277882] 可以看到，我们最开始的参数都是在 $10^{-3}$ 这个数量级上，而第一轮迭代时计算出的梯度的数量级在 $10^8$ 左右，这就导致使用梯度下降更新的时候，让参数变成了 $10^6$ 这个数量级左右（学习率为0.01）。产生这样的问题的主要原因是：我们的原始数据 $X$ 没有经过适当的处理，直接扔到了神经网络中进行训练，导致在计算梯度时，由于 $X$ 的数量级过大，导致梯度的数量级变大，在参数更新时使得参数的数量级不断上升，导致参数无法收敛。 解决的方法也很简单，对参数进行归一化处理，将其标准化，使均值为0，缩放到 $[-1, 1]$附近。 10. 标准化处理标准化处理和第一题一样 1234from sklearn.preprocessing import StandardScalerstand = StandardScaler()trainX_normalized = stand.fit_transform(trainX)testX_normalized = stand.transform(testX) 重新训练模型，这次我们迭代40轮，学习率设置为0.1 123m = trainX.shape[1]W, b = initialize(m)training_loss_list, testing_loss_list = train(trainX_normalized, trainY, testX_normalized, testY, W, b, 40, learning_rate = 0.1, verbose = False) 打印损失值变化曲线 1plot_loss_curve(training_loss_list, testing_loss_list) 计算测试集上的MSE 12prediction = forward(testX_normalized, W, b)mse(testY, prediction) ** 0.5 第三题：神经网络：对数几率回归实验内容： 完成对数几率回归 使用梯度下降求解模型参数 绘制模型损失值的变化曲线 调整学习率和迭代轮数，观察损失值曲线的变化 按照给定的学习率和迭代轮数，初始化新的参数，绘制其训练集和测试集损失值的变化曲线，完成表格内精度的填写 对数几率回归，二分类问题的分类算法，属于线性模型中的一种，我们可以将其抽象为最简单的神经网络。 只有一个输入层和一个输出层，还有一个激活函数，$\\rm sigmoid$，简记为$\\sigma$。我们设输入为$X \\in \\mathbb{R}^{n \\times m}$，输入层到输出层的权重为$W \\in \\mathbb{R}^{m}$，偏置$b \\in \\mathbb{R}$。 激活函数$$\\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}$$ 这个激活函数，会将输出层的神经元的输出值转换为一个 $(0, 1)$ 区间内的数。 因为是二分类问题，我们设类别为0和1，我们将输出值大于0.5的样本分为1类，输出值小于0.5的类分为0类。 前向传播$$Z = XW + b\\\\\\hat{y} = \\sigma(Z)$$ 其中，$O \\in \\mathbb{R}^{n}$为输出层的结果，$\\sigma$为$\\rm sigmoid$激活函数。 注意：这里我们其实是做了广播，将$b$复制了$n-1$份后拼接成了维数为$n$的向量。 所以对数几率回归就可以写为： $$\\hat{y} = \\frac{1}{1 + e^{-XW + b}}$$ 损失函数使用对数损失函数，因为对数损失函数较其他损失函数有更好的性质，感兴趣的同学可以去查相关的资料。 针对二分类问题的对数损失函数： $$\\mathrm{loss}(y, \\hat{y}) = - y \\log{\\hat{y}} - (1 - y) \\log{(1 - \\hat{y})}$$ 在这个对数几率回归中，我们的损失函数对所有样本取个平均值： $$\\mathrm{loss}(y, \\hat{y}) = - \\frac{1}{n} \\sum^n_{i = 1}[y_i \\log{\\hat{y_i}} + (1 - y_i) \\log{(1 - \\hat{y_i})}]$$ 注意，这里我们的提到的$\\log$均为$\\ln$，在numpy中为np.log。 因为我们的类别只有0和1，所以在这个对数损失函数中，要么前一项为0，要么后一项为0。 如果当前样本的类别为0，那么前一项就为0，损失函数变为 $- \\log{(1 - \\hat{y})}$ ，因为我们的预测值 $0 &lt; \\hat{y} &lt; 1$ ，所以 $0 &lt; 1 - \\hat{y} &lt; 1$ ，$- \\log{(1 - \\hat{y})} &gt; 0$ ，为了降低损失值，模型需要让预测值 $\\hat{y}$不断地趋于0。 同理，如果当前样本的类别为1，那么降低损失值就可以使模型的预测值趋于1。 参数更新求得损失函数对参数的偏导数后，我们就可以使用梯度下降进行参数更新： $$W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}\\\\b := b - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b}$$ 其中，$\\alpha$ 是学习率，一般设置为0.1，0.01等。 经过一定次数的迭代后，参数会收敛至最优点。这种基于梯度的优化算法很常用，训练神经网络主要使用这类优化算法。 反向传播我们使用梯度下降更新参数$W$和$b$。为此需要求得损失函数对参数$W$和$b$的偏导数，根据链式法则有： $$\\begin{aligned}\\frac{\\partial \\mathrm{loss}}{\\partial W} &amp;= \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial Z} \\frac{\\partial Z}{\\partial W}\\end{aligned}$$ 这里我们一项一项求，先求第一项： $$\\begin{aligned}\\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} = - \\frac{1}{n} \\sum^n_{i = 1} [\\frac{y}{\\hat{y}} - \\frac{1 - y}{1 - \\hat{y}}]\\end{aligned}$$ 第二项： $$\\begin{aligned}\\frac{\\partial \\hat{y}}{\\partial Z} &amp; = \\frac{\\partial (\\frac{1}{1 + e^{-Z}})}{\\partial Z}\\\\&amp; = \\frac{e^{-Z}}{(1 + e^{-Z})^2}\\\\&amp; = \\frac{e^{-Z}}{(1 + e^{-Z})} \\frac{1}{(1 + e^{-Z})}\\\\&amp; = \\frac{e^{-Z}}{(1 + e^{-Z})} (1 - \\frac{e^{-Z}}{(1 + e^{-Z})})\\\\&amp; = \\sigma(Z)(1 - \\sigma(Z))\\end{aligned}$$ 第三项： $$\\frac{\\partial Z}{\\partial W} = X^{\\mathrm{T}}$$ 综上： $$\\begin{aligned}\\frac{\\partial \\mathrm{loss}}{\\partial W} &amp;= \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial Z} \\frac{\\partial Z}{\\partial W}\\\\&amp;= - \\frac{1}{n} \\sum^n_{i = 1} [\\frac{y_i}{\\hat{y_i}} - \\frac{1 - y_i}{1 - \\hat{y_i}}] [\\sigma(Z_i)(1 - \\sigma(Z_i))] {X_i}^{\\mathrm{T}}\\\\&amp;= - \\frac{1}{n} \\sum^n_{i = 1} [\\frac{y_i}{\\hat{y_i}} - \\frac{1 - y_i}{1 - \\hat{y_i}}] [\\hat{y_i}(1 - \\hat{y_i})] {X_i}^{\\mathrm{T}}\\\\&amp;= - \\frac{1}{n} \\sum^n_{i = 1} [y_i(1 - \\hat{y_i}) - \\hat{y_i}(1 - y_i)] {X_i}^{\\mathrm{T}}\\\\&amp;= - \\frac{1}{n} \\sum^n_{i = 1} (y_i - y_i \\hat{y_i} - \\hat{y_i} + y_i \\hat{y_i}) {X_i}^{\\mathrm{T}}\\\\&amp;= - \\frac{1}{n} \\sum^n_{i = 1} (y_i - \\hat{y_i}) {X_i}^{\\mathrm{T}}\\\\&amp;= \\frac{1}{n} [X^{\\mathrm{T}}(\\hat{y} - y)]\\end{aligned}$$ 同理，求$\\rm loss$对$b$的偏导数： 注意，由于$b$是被广播成$n \\times K$的矩阵，因此实际上$b$对每个样本的损失都有贡献，因此对其求偏导时，要把$n$个样本对它的偏导数加和。 $$\\begin{aligned}\\frac{\\partial \\mathrm{loss}}{\\partial b} &amp;= \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial Z} \\frac{\\partial Z}{\\partial b}\\\\&amp;= - \\frac{1}{n} \\sum^n_{i = 1} [\\frac{y_i}{\\hat{y_i}} - \\frac{1 - y_i}{1 - \\hat{y_i}}] [\\sigma(Z_i)(1 - \\sigma(Z_i))]\\\\&amp;= - \\frac{1}{n} \\sum^n_{i = 1} [\\frac{y_i}{\\hat{y_i}} - \\frac{1 - y_i}{1 - \\hat{y_i}}] [\\hat{y_i}(1 - \\hat{y_i})]\\\\&amp;= - \\frac{1}{n} \\sum^n_{i = 1} [y_i(1 - \\hat{y_i}) - \\hat{y_i}(1 - y_i)]\\\\&amp;= - \\frac{1}{n} \\sum^n_{i = 1} (y_i - y_i \\hat{y_i} - \\hat{y_i} + y_i \\hat{y_i})\\\\&amp;= \\frac{1}{n} \\sum^n_{i = 1} (\\hat{y_i} - y_i)\\\\\\end{aligned}$$ 这样，我们就得到了损失函数对参数的偏导数，然后就可以使用梯度下降算法更新参数 1. 导入数据集123import numpy as npimport matplotlib.pyplot as plt%matplotlib inline 我们生成半月形数据 12from sklearn.datasets import make_moonsX, y = make_moons(n_samples = 2000, noise = 0.3, random_state=0) 选择40%的数据作为测试集，60%作为训练集 1234from sklearn.model_selection import train_test_splittrainX, testX, trainY, testY = train_test_split(X, y, test_size = 0.4, random_state = 32)trainY = trainYtestY = testY 1trainX.shape, trainY.shape, testX.shape, testY.shape 2. 数据预处理使用和第一题一样的预处理方式 1234from sklearn.preprocessing import StandardScalers = StandardScaler()trainX = s.fit_transform(trainX)testX = s.transform(testX) 3. 定义神经网络3.1 参数初始化我们需要对神经网络的参数进行初始化，这个网络中只有两个参数，一个$W \\in \\mathbb{R}^{m}$，一个$b \\in \\mathbb{R}$。初始化的时候，我们将参数W随机初始化，参数b初始化为0。为什么要对神经网络的参数进行随机初始化，感兴趣的同学可以去查相关的资料。 123456789101112131415def initialize(m): ''' 初始化参数W和参数b Returns ---------- W: np.ndarray, shape = (m, )，参数W b: np.ndarray, shape = (1, )，参数b ''' np.random.seed(32) W = np.random.normal(size = (m, )) * 0.01 b = np.zeros((1, )) return W, b 1234# 测试样例Wt, bt = initialize(trainX.shape[1])print(Wt.shape) # (2,)print(bt.shape) # (1,) 3.2 前向传播接下来我们要定义神经网络前向传播的过程。 首先计算$Z = XW + b$ 123456789101112131415161718192021def linear_combination(X, W, b): ''' 完成Z = XW + b的计算 Parameters ---------- X: np.ndarray, shape = (n, m)，输入的数据 W: np.ndarray, shape = (m, )，权重 b: np.ndarray, shape = (1, )，偏置 Returns ---------- Z: np.ndarray, shape = (n, )，线性组合后的值 ''' Z = np.dot(X, W) + b # YOUR CODE HERE return Z 123# 测试样例Wt, bt = initialize(trainX.shape[1])linear_combination(trainX, Wt, bt).shape #(1200,) 接下来实现激活函数$\\rm sigmoid$ 12345678910111213def my_sigmoid(x): ''' simgoid 1 / (1 + exp(-x)) Parameters ---------- X: np.ndarray, 待激活的值 ''' # YOUR CODE HERE activations = 1 / (1 + np.exp(-x)) return activations 1234# 测试样例Wt, bt = initialize(trainX.shape[1])Zt = linear_combination(trainX, Wt, bt)my_sigmoid(Zt).mean() # 0.49999 在实现$\\rm sigmoid$的时候，可能会遇到上溢(overflow)的问题，可以看到$\\rm sigmoid$中有一个指数运算$$\\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}$$当$x$很大的时候，我们使用numpy.exp(x)会直接溢出 1np.exp(1e56) 1my_sigmoid(np.array([-1e56])) 虽说程序没有报错，只是抛出了warning，但还是应该解决一下。 解决这种问题的方法有很多，比如，我们可以将$\\rm sigmoid$进行变换： $$\\begin{aligned}\\mathrm{sigmoid}(x) &amp;= \\frac{1}{1 + e^{-x}}\\\\&amp;= \\frac{e^x}{1 + e^x}\\\\&amp;= \\frac{1}{2} + \\frac{1}{2} \\mathrm{tanh}(\\frac{x}{2})\\end{aligned}$$ 其中，$\\mathrm{tanh}(x) = \\frac{\\mathrm{sinh}(x)}{\\mathrm{cosh}(x)} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ 转换成这种形式后，我们就可以直接利用numpy.tanh完成$\\rm sigmoid$的计算，就不会产生上溢的问题了。 除此以外，最好的解决方法是使用scipy中的expit函数，完成$\\rm sigmoid$的计算。我们现在做的都是神经网络底层相关的运算，很容易出现数值不稳定性相关的问题，最好的办法就是使用别人已经实现好的函数，这样就能减少我们很多的工作量，同时又快速地完成任务。 1from scipy.special import expit 12def sigmoid(X): return expit(X) 12# 测试样例sigmoid(np.array([-1e56])) 接下来完成整个前向传播的函数，也就是 $Z = XW+b$ 和 $\\hat{y} = \\mathrm{sigmoid}(Z)$ 123456789101112131415161718192021222324def forward(X, W, b): ''' 完成输入矩阵X到最后激活后的预测值y_pred的计算过程 Parameters ---------- X: np.ndarray, shape = (n, m)，数据，一行一个样本，一列一个特征 W: np.ndarray, shape = (m, )，权重 b: np.ndarray, shape = (1, )，偏置 Returns ---------- y_pred: np.ndarray, shape = (n, )，模型对每个样本的预测值 ''' # 求Z Z = linear_combination(X, W, b) # YOUR CODE HERE # 求激活后的预测值 y_pred = sigmoid(Z) # YOUR CODE HERE return y_pred 123# 测试样例Wt, bt = initialize(trainX.shape[1])forward(trainX, Wt, bt).mean() # 0.4999(没有四舍五入) 接下来完成损失函数的编写，我们使用的是对数损失，这里需要注意的一个问题是： $$\\mathrm{loss}(y, \\hat{y}) = - \\frac{1}{n}[ y \\log{\\hat{y}} + (1 - y) \\log{(1 - \\hat{y})}]$$ 在这个对数损失中，$\\hat{y}$中不能有$0$和$1$，如果有$0$，那么损失函数中的前半部分，$\\log{0}$就会出错，如果有$1$，那么后半部分$\\log{(1-1)}$就会出错。 所以我们要先将$\\hat{y}$中的$0$和$1$改变一下，把$0$变成一个比较小但是大于$0$的数，把$1$变成小于$1$但是足够大的数。使用numpy.clip函数就可以作到这点。 12345678910111213141516171819202122def logloss(y_true, y_pred): ''' 给定真值y，预测值y_hat，计算对数损失并返回 Parameters ---------- y_true: np.ndarray, shape = (n, ), 真值 y_pred: np.ndarray, shape = (n, )，预测值 Returns ---------- loss: float, 损失值 ''' # 下面这句话会把y_pred里面小于1e-10的数变成1e-10，大于1 - 1e-10的数变成1 - 1e-10 y_hat = np.clip(y_pred, 1e-10, 1 - 1e-10) # 求解对数损失 loss = - np.sum(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat)) / len(y_true) # YOUR CODE HERE return loss 123# 测试样例Wt, bt = initialize(trainX.shape[1])logloss(trainY, forward(trainX, Wt, bt)) # 0.69740 3.3 反向传播我们接下来要完成损失函数对参数的偏导数的计算 1234567891011121314151617181920212223242526def compute_gradient(y_true, y_pred, X): ''' 给定预测值y_pred，真值y_true，传入的输入数据X，计算损失函数对参数W的偏导数的导数值dW，以及对b的偏导数的导数值db Parameters ---------- y_true: np.ndarray, shape = (n, ), 真值 y_pred: np.ndarray, shape = (n, )，预测值 X: np.ndarray, shape = (n, m)，数据，一行一个样本，一列一个特征 Returns ---------- dW: np.ndarray, shape = (m, ), 损失函数对参数W的偏导数 db: float, 损失函数对参数b的偏导数 ''' # 求损失函数对参数W的偏导数的导数值 dW = np.dot(X.T, (y_pred - y_true)) / len(y_pred) # YOUR CODE HERE # 求损失函数对参数b的偏导数的导数值 db = np.sum(y_pred - y_true) / len(y_pred) # YOUR CODE HERE return dW, db 123456# 测试样例Wt, bt = initialize(trainX.shape[1])dWt, dbt = compute_gradient(trainY, forward(trainX, Wt, bt), trainX)print(dWt.shape) # (2, )print(dWt.sum()) # 0.04625print(dbt) # 0.00999 3.4 参数更新给定学习率，结合上一步求出的偏导数，完成梯度下降的更新公式 12345678910111213141516171819202122def update(W, b, dW, db, learning_rate): ''' 梯度下降，给定参数W，参数b，以及损失函数对他们的偏导数，使用梯度下降更新参数W和参数b Parameters ---------- W: np.ndarray, shape = (m, )，参数W b: np.ndarray, shape = (1, )，参数b dW: np.ndarray, shape = (m, ), 损失函数对参数W的偏导数 db: float, 损失函数对参数b的偏导数 learning_rate, float，学习率 ''' # 对参数W进行更新 W -= learning_rate * dW # 对参数b进行更新 b -= learning_rate * db # YOUR CODE HERE 1234567891011121314# 测试样例Wt, bt = initialize(trainX.shape[1])print(Wt) # [-0.00348894 0.00983703]print(bt) # [ 0.]print()dWt, dbt = compute_gradient(trainY, forward(trainX, Wt, bt), trainX)print(dWt) # [-0.28650366 0.33276308]print(dbt) # 0.00999999939463print()update(Wt, bt, dWt, dbt, 0.01)print(Wt) # [-0.00062391 0.0065094 ]print(bt) # [ -9.99999939e-05] 我们来完成整个反向传播和更新参数的函数 12345678910111213141516171819202122232425262728def backward(y_true, y_pred, X, W, b, learning_rate): ''' 反向传播，包含了计算损失函数对各个参数的偏导数的过程，以及梯度下降更新参数的过程 Parameters ---------- y_true: np.ndarray, shape = (n, ), 真值 y_pred: np.ndarray, shape = (n, )，预测值 X: np.ndarray, shape = (n, m)，数据，一行一个样本，一列一个特征 W: np.ndarray, shape = (m, )，参数W b: np.ndarray, shape = (1, )，参数b dW: np.ndarray, shape = (m, ), 损失函数对参数W的偏导数 db: float, 损失函数对参数b的偏导数 learning_rate, float，学习率 ''' # 求参数W和参数b的梯度 dW, db = compute_gradient(y_true, y_pred, X) # 梯度下降 update(W, b, dW, db, learning_rate) 1234567891011# 测试样例Wt, bt = initialize(trainX.shape[1])y_predt = forward(trainX, Wt, bt)loss_1 = logloss(trainY, y_predt)print(loss_1) # 0.697403529518backward(trainY, y_predt, trainX, Wt, bt, 0.01)y_predt = forward(trainX, Wt, bt)loss_2 = logloss(trainY, y_predt)print(loss_2) # 0.695477626714 4. 训练函数的编写我们已经实现了完成训练需要的子函数，接下来就是组装了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def train(trainX, trainY, testX, testY, W, b, epochs, learning_rate = 0.01, verbose = False): ''' 训练，我们要迭代epochs次，每次迭代的过程中，做一次前向传播和一次反向传播 同时记录训练集和测试集上的损失值，后面画图用 Parameters ---------- trainX: np.ndarray, shape = (n, m), 训练集 trainY: np.ndarray, shape = (n, ), 训练集标记 testX: np.ndarray, shape = (n_test, m)，测试集 testY: np.ndarray, shape = (n_test, )，测试集的标记 W: np.ndarray, shape = (m, )，参数W b: np.ndarray, shape = (1, )，参数b epochs: int, 要迭代的轮数 learning_rate: float, default 0.01，学习率 verbose: boolean, default False，是否打印损失值 Returns ---------- training_loss_list: list(float)，每迭代一次之后，训练集上的损失值 testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值 ''' training_loss_list = [] testing_loss_list = [] for i in range(epochs): # 计算训练集前向传播得到的预测值 # YOUR CODE HERE train_y_pred = forward(trainX, W, b) # 计算当前训练集的损失值 # YOUR CODE HERE training_loss = logloss(trainY, train_y_pred) # 计算测试集前向传播得到的预测值 # YOUR CODE HERE test_y_pred = forward(testX, W, b) # 计算当前测试集的损失值 testing_loss = logloss(testY, test_y_pred) if verbose == True: print('epoch %s, training loss:%s'%(i + 1, training_loss)) print('epoch %s, testing loss:%s'%(i + 1, testing_loss)) print() # 保存损失值 training_loss_list.append(training_loss) testing_loss_list.append(testing_loss) # 反向传播更新参数 # YOUR CODE HERE backward(trainY, train_y_pred, trainX, W, b, learning_rate) return training_loss_list, testing_loss_list 12345# 测试样例Wt, bt = initialize(trainX.shape[1])training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, Wt, bt, 2, 0.1)print(training_loss_list) # [0.69740352951773121, 0.67843729060725722]print(testing_loss_list) # [0.69743661286103986, 0.67880126235588389] 5. 绘制模型损失值变化曲线1234567891011121314151617def plot_loss_curve(training_loss_list, testing_loss_list): ''' 绘制损失值变化曲线 Parameters ---------- training_loss_list: list(float)，每迭代一次之后，训练集上的损失值 testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值 ''' plt.figure(figsize = (10, 6)) plt.plot(training_loss_list, label = 'training loss') plt.plot(testing_loss_list, label = 'testing loss') plt.xlabel('epoch') plt.ylabel('loss') plt.legend() 6. 预测接下来编写一个预测的函数，事实上，$\\rm sigmoid$输出的是当前这个样本为正例的概率，也就是说，这个输出值是一个0到1的值，一般我们将大于0.5的值变成1，小于0.5的值变成0，也就是说，如果当前输出的概率值大于0.5，那我们认为这个样本的类别就是1，否则就是0，这样输出的就是类标了。 12345678910111213141516171819def predict(X, W, b): ''' 预测，调用forward函数完成神经网络对输入X的计算，然后完成类别的划分，大于0.5的变为1，小于等于0.5的变为0 Parameters ---------- X: np.ndarray, shape = (n, m), 训练集 W: np.ndarray, shape = (m, 1)，参数W b: np.ndarray, shape = (1, )，参数b Returns ---------- prediction: np.ndarray, shape = (n, 1)，预测的标记 ''' prediction = (forward(testX, W, b) &gt; 0.5).astype('uint8') # YOUR CODE HERE return prediction 12345# 测试样例from sklearn.metrics import accuracy_scoreWt, bt = initialize(trainX.shape[1])predictiont = predict(testX, Wt, bt)accuracy_score(testY, predictiont) # 0.16250000000000001 7. 训练一个神经网络我们的学习率是0.01，迭代200轮 12W, b = initialize(trainX.shape[1])training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, W, b, 200, 0.01) 计算测试集精度 12prediction = predict(testX, W, b)accuracy_score(testY, prediction) # 0.83625000000000005 绘制损失值变化曲线 1plot_loss_curve(training_loss_list, testing_loss_list) test：初始化新的参数，学习率和迭代轮数按下表设置，绘制其训练集和测试集损失值的变化曲线，完成表格内精度的填写双击此处填写 学习率 迭代轮数 测试集精度 0.0001 200 0.3325 0.1 1000 0.84 123456# YOUR CODE HEREW2, b2 = initialize(trainX.shape[1])training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, W2, b2, 200, 0.0001)prediction = predict(testX, W2, b2)print(accuracy_score(prediction, testY))plot_loss_curve(training_loss_list, testing_loss_list) 123456# YOUR CODE HEREW2, b2 = initialize(trainX.shape[1])training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, W2, b2, 1000, 0.1)prediction = predict(testX, W2, b2)print(accuracy_score(prediction, testY))plot_loss_curve(training_loss_list, testing_loss_list) 第四题：神经网络：三层感知机实现内容： 实现一个三层感知机 对手写数字数据集进行分类 绘制损失值变化曲线 在这道题中，我们要实现一个三层感知机 前向传播我们实现一个最简单的三层感知机，一个输入层，一个隐藏层，一个输出层，隐藏层单元个数为$h$个，输出层有$K$个单元。 我们将第一层的输入，定义为$X \\in \\mathbb{R}^{n \\times m}$，n个样本，m个特征。 输入层到隐藏层之间的权重(weight)与偏置(bias)，分别为$W_1 \\in \\mathbb{R}^{m \\times h}$，$b_1 \\in \\mathbb{R}^{1 \\times h}$。 隐藏层到输出层的权重和偏置分为别$W_2 \\in \\mathbb{R}^{h \\times K}$，$b_2 \\in \\mathbb{R}^{1 \\times K}$。 隐藏层的激活函数选用ReLU $$\\mathrm{ReLU}(x) = \\max (0, x)$$ 我们用$H_1$表示第一个隐藏层的输出值，$O$表示输出层的输出值，这样，前向传播即可定义为 $$Z = XW_1 + b_1\\\\H_1 = \\mathrm{ReLU}(Z)\\\\O = H_1 W_2 + b_2$$ 其中，$H_1 \\in \\mathbb{R}^{n \\times h}$，$O \\in \\mathbb{R}^{n \\times K}$。 注意：这里我们其实是做了广播，将$b_1$复制了$n-1$份后拼接成了维数为$n \\times h$的矩阵，同理，$b_2$也做了广播，拼成了$n \\times K$的矩阵。 最后一层的输出，使用softmax函数激活，得到神经网络计算出的各类的概率值： $$\\begin{aligned}\\hat{y_i} &amp; = \\mathrm{softmax}(O_i)\\\\&amp; = \\frac{\\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}}\\end{aligned}$$ 其中，$\\hat{y_i}$表示第$i$类的概率值，也就是输出层第$i$个神经元经$\\mathrm{softmax}$激活后的值。 损失函数损失函数使用交叉熵损失函数：$$\\mathrm{cross_entropy}(y, \\hat{y}) = -\\sum^{K}_{k=1}y_k \\log{(\\hat{y_k})}$$ 这样，$n$个样本的平均损失为：$$\\mathrm{loss} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\log{(\\hat{y_k})}$$ 注意，这里我们的提到的$\\log$均为$\\ln$，在numpy中为np.log 反向传播我们使用梯度下降训练模型，求解方式就是求出损失函数对参数的偏导数，即参数的梯度，然后将参数减去梯度乘以学习率，进行参数的更新。$$W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}$$其中，$\\alpha$是学习率。 在这道题中，交叉熵损失函数的求导比较麻烦，我们先求神经网络的输出层的偏导数，写成链式法则的形式： $$\\frac{\\partial \\mathrm{loss}}{\\partial O_i} = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O_i}$$ 首先求解第一项：$$\\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\frac{1}{\\hat{y_k}}$$ 然后求解第二项，因为$\\hat{y_k}$的分母是$\\sum_k \\exp{(O_k)}$，里面包含$O_i$，所以每一个$\\hat{y_k}$的分母都包含$O_i$，这就要求反向传播的时候需要考虑这$K$项，将这$K$项的偏导数加在一起。 这$K$项分别为：$\\frac{\\exp{(O_1)}}{\\sum_k \\exp{(O_k)}}$，$\\frac{\\exp{(O_2)}}{\\sum_k \\exp{(O_k)}}$，…，$\\frac{\\exp{(O_i)}}{\\sum_k \\exp{(O_k)}}$，…，$\\frac{\\exp{(O_k)}}{\\sum_k \\exp{(O_k)}}$。 显然，这里只有分子带有$O_i$的这项与其他的项不同，因为分子和分母同时包含了$O_i$，而其他的项只有分母包含了$O_i$。 这就需要在求解$\\frac{\\partial \\hat{y}}{\\partial O_i}$的时候分两种情况讨论 分子带$O_i$ 分子不带$O_i$ 第一种情况，当分子含有$O_i$时： $$\\begin{aligned}\\frac{\\partial \\hat{y_i}}{\\partial O_i} &amp; = \\frac{\\partial \\hat{y_i}}{\\partial O_i}\\\\&amp; = \\frac{\\exp{(O_i)} (\\sum^{K}_{k=1} \\exp{(O_k)}) - (\\exp{(O_i)})^2 }{(\\sum^{K}_{k=1} \\exp{(O_k)})^2}\\\\&amp; = \\frac{\\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}} \\frac{\\sum^{K}_{k=1} \\exp{(O_k)} - \\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}}\\\\&amp; = \\hat{y_i} ( 1 - \\hat{y_i} )\\end{aligned}$$ 第二种情况，当分子不含$O_i$时，我们用$j$表示当前项的下标： $$\\begin{aligned}\\frac{\\partial \\hat{y_j}}{\\partial O_i} &amp; = \\frac{- \\exp{(O_j)} \\exp{(O_i)}}{(\\sum^{K}_{k=1} \\exp{(O_k)})^2}\\\\&amp; = - \\hat{y_j} \\hat{y_i}\\end{aligned}$$ 这样，$\\mathrm{loss}$对$O_i$的偏导数即为：$$\\begin{aligned}\\frac{\\partial \\mathrm{loss}}{\\partial O_i} &amp; = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O_i}\\\\&amp; = (- \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\frac{1}{\\hat{y_k}}) \\frac{\\partial \\hat{y}}{\\partial O_i}\\\\&amp; = - \\frac{1}{n} \\sum_n (y_i \\frac{1}{\\hat{y_i}} \\hat{y_i} ( 1 - \\hat{y_i} ) + \\sum^K_{k \\not= i} y_k \\frac{1}{\\hat{y_k}}( - \\hat{y_k} \\hat{y_i}))\\\\&amp; = - \\frac{1}{n} \\sum_n ( y_i - y_i \\hat{y_i} - \\sum^K_{k \\not= i} y_k \\hat{y_i})\\\\&amp; = - \\frac{1}{n} \\sum_n ( y_i - \\hat{y_i} \\sum^K_{k = 1} y_k )\\end{aligned}$$ 由于我们处理的多类分类任务，一个样本只对应一个标记，所以$\\sum^K_{k = 1} y_k = 1$，上式在这种问题中，即可化简为： $$\\begin{aligned}\\frac{\\partial \\mathrm{loss}}{\\partial O_i} &amp;= - \\frac{1}{n} \\sum_n ( y_i - \\hat{y_i})\\\\&amp; = \\frac{1}{n} \\sum_n (\\hat{y_i} - y_i)\\end{aligned}$$ 将其写成矩阵表达式： $$\\begin{aligned}\\frac{\\partial \\mathrm{loss}}{\\partial O} &amp;= \\frac{1}{n} (\\hat{y} - y)\\end{aligned}$$ 也就是说，我们的损失函数对输出层的$K$个神经单元的偏导数为$\\mathrm{softmax}$激活值减去真值。 接下来我们需要求损失函数对参数$W_2$和$b_2$的偏导数 $$\\begin{aligned}\\frac{\\partial loss}{\\partial W_2} &amp; = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\&amp; = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\&amp; = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial W_2}\\\\&amp; = \\frac{1}{n} [{H_1}^\\mathrm{T} (\\hat{y} - y)]\\end{aligned}$$ $$\\begin{aligned}\\frac{\\partial loss}{\\partial b_2} &amp; = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\&amp; = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\&amp; = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial b_2}\\\\&amp; = \\frac{1}{n} \\sum^n_{i=1} (\\hat{y_i} - y_i)\\end{aligned}$$ 其中，$\\frac{\\partial loss}{\\partial W_2} \\in \\mathbb{R}^{h \\times K}$，$\\frac{\\partial loss}{\\partial b_2} \\in \\mathbb{R}^{1 \\times K}$。注意，由于$b_2$是被广播成$n \\times K$的矩阵，因此实际上$b_2$对每个样本的损失都有贡献，因此对其求偏导时，要把$n$个样本对它的偏导数加和。 同理，我们可以求得$\\mathrm{loss}$对$W_1$和$b_1$的偏导数： $$\\begin{aligned}\\frac{\\partial loss}{\\partial W_1} &amp; = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\&amp; = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\&amp; = \\frac{1}{n} {X}^\\mathrm{T} [(\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}]\\\\\\end{aligned}$$ 由于我们使用的是$\\mathrm{ReLU}$激活函数，它的偏导数为： $$\\frac{\\partial \\mathrm{ReLU(x)}}{\\partial x} =\\begin{cases}0 &amp; \\text{if } x &lt; 0\\\\1 &amp; \\text{if } x \\geq 0\\end{cases}$$ 所以上式为： $$\\frac{\\partial loss}{\\partial {W_1}_{ij}} =\\begin{cases}0 &amp; \\text{if } {Z}_{ij} &lt; 0\\\\ \\frac{1}{n} {X}^\\mathrm{T} (\\hat{y} - y) {W_2}^\\mathrm{T} &amp; \\text{if } {Z}_{ij} \\geq 0\\end{cases}$$ 其中，${W_1}_{ij}$表示矩阵$W_1$第$i$行第$j$列的值，${Z}_{ij}$表示矩阵$Z$第$i$行第$j$列的值。同理： $$\\begin{aligned}\\frac{\\partial loss}{\\partial b_1} &amp; = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\&amp; = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\&amp; = \\frac{1}{n} (\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}\\\\&amp; = \\begin{cases}0 &amp;\\text{if } {Z}_{ij} &lt; 0\\\\\\frac{1}{n} \\sum_n (\\hat{y} - y) {W_2}^\\mathrm{T} &amp;\\text{if } {Z}_{ij} \\geq 0\\end{cases}\\end{aligned}$$ 其中，$\\frac{\\partial loss}{\\partial W_1} \\in \\mathbb{R}^{m \\times h}$，$\\frac{\\partial loss}{\\partial b_1} \\in \\mathbb{R}^{1 \\times h}$。 参数更新求得损失函数对四个参数的偏导数后，我们就可以使用梯度下降进行参数更新：$$W_2 := W_2 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W_2}\\\\b_2 := b_2 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b_2}\\\\W_1 := W_1 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W_1}\\\\b_1 := b_1 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b_1}\\\\$$其中，$\\alpha$是学习率 以上内容，就是一个三层感知机的前向传播与反向传播过程。 1. 导入数据使用第一题的手写数字数据集 12import matplotlib.pyplot as plt%matplotlib inline 1from time import time 123import numpy as npfrom sklearn.datasets import load_digitsfrom sklearn.model_selection import train_test_split 40%做测试集，60%做训练集 1trainX, testX, trainY, testY = train_test_split(load_digits()['data'], load_digits()['target'], test_size = 0.4, random_state = 32) 1trainX.shape, trainY.shape, testX.shape, testY.shape 2. 数据预处理使用和第一题一样的标准化处理方法 1234from sklearn.preprocessing import StandardScalers = StandardScaler()trainX = s.fit_transform(trainX)testX = s.transform(testX) 接下来还要处理输出。我们的神经网络是针对每个样本，输出其分别属于$K$类的概率，我们要找最大的那个概率，对应的是哪个类。我们当前的trainY和testY，每个样本都是一个类标，我们需要将其变成one_hot编码，也就是，假设当前样本的类别是3，我们需要把它变成一个长度为10的向量，其中第4个元素为1，其他元素都为0。得到的矩阵分别记为trainY_mat和testY_mat。这样，模型训练完成后，会针对每个样本输出十个数，分别代表这个样本属于$0,1,…,9$的概率，那我们只要取最大的那个数的下标，就知道模型认为这个样本是哪类了。 12345trainY_mat = np.zeros((len(trainY), 10))trainY_mat[np.arange(0, len(trainY), 1), trainY] = 1testY_mat = np.zeros((len(testY), 10))testY_mat[np.arange(0, len(testY), 1), testY] = 1 1trainY_mat.shape, testY_mat.shape 3. 参数初始化这题和上一题的区别是，我们把参数用dict存起来 1234567891011121314151617181920212223242526def initialize(h, K): ''' 参数初始化 Parameters ---------- h: int: 隐藏层单元个数 K: int: 输出层单元个数 Returns ---------- parameters: dict，参数，键是\"W1\", \"b1\", \"W2\", \"b2\" ''' np.random.seed(32) W_1 = np.random.normal(size = (trainX.shape[1], h)) * 0.01 b_1 = np.zeros((1, h)) np.random.seed(32) W_2 = np.random.normal(size = (h, K)) * 0.01 b_2 = np.zeros((1, K)) parameters = {'W1': W_1, 'b1': b_1, 'W2': W_2, 'b2': b_2} return parameters 123456# 测试样例parameterst = initialize(50, 10)print(parameterst['W1'].shape) # (64, 50)print(parameterst['b1'].shape) # (1, 50)print(parameterst['W2'].shape) # (50, 10)print(parameterst['b2'].shape) # (1, 10) 4. 前向传播完成Z的计算 12345678910111213141516171819202122def linear_combination(X, W, b): ''' 计算Z，Z = XW + b Parameters ---------- X: np.ndarray, shape = (n, m)，输入的数据 W: np.ndarray, shape = (m, h)，权重 b: np.ndarray, shape = (1, h)，偏置 Returns ---------- Z: np.ndarray, shape = (n, h)，线性组合后的值 ''' # Z = XW + b Z = np.dot(X, W) + b return Z 12345# 测试样例parameterst = initialize(50, 10)Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])print(Zt.shape) # (1078, 50)print(Zt.mean()) # -5.27304442123e-19 $\\rm ReLU$激活函数 12345678910111213141516def ReLU(X): ''' ReLU激活函数 Parameters ---------- X: np.ndarray，待激活的矩阵 Returns ---------- activations: np.ndarray, 激活后的矩阵 ''' activations = X.copy() activations[activations &lt; 0] = 0 return activations 123456789# 测试样例parameterst = initialize(50, 10)Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])Ht = ReLU(Zt)print(Ht.mean()) # 0.0304Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])print(Ot.shape) # (1078, 10)print(Ot.mean()) # 0.0006 $\\rm softmax$激活 $$\\mathrm{softmax}(O_i) = \\frac{\\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}}$$ 123456def my_softmax(O): ''' softmax激活 ''' denominator = np.exp(O).sum(axis = 1, keepdims = True) return np.exp(O) / denominator 12345# 测试样例test1 = np.array([[-1e32, -1e32, -1e32]])test2 = np.array([[1e32, 1e32, 1e32]])print(my_softmax(test1))print(my_softmax(test2)) 这里，其实是有数值计算上的问题的，假设，我们最后的输出有三个数，每个数都特别小，理论上来说，通过$\\rm softmax$激活后，三个值都是$\\frac{1}{3}$。但实际上就不是这样了，实际上会导致分母为0，除法就不能做了。如果每个数都特别大，会导致做指数运算的时候上溢。 我们需要用其他的方法来实现$\\rm softmax$。 我们将传入$\\rm softmax$的向量，每个元素减去他们中的最大值，即 $$\\mathrm{softmax}(O_i) = \\mathrm{softmax}(O_i - \\mathrm{max(O)})$$ 这个式子是成立的，感兴趣的同学可以证明一下上面的式子。 当我们做了这样的变换后，向量$O$中的最大值就变成了0，就不会上溢了，而分母中最少有一项为1，也不会出现下溢导致分母为0的问题了。 1234567891011121314151617181920def softmax(O): ''' softmax激活函数 Parameters ---------- O: np.ndarray，待激活的矩阵 Returns ---------- activations: np.ndarray, 激活后的矩阵 ''' # YOUR CODE HEER t = O - np.max(O, axis = 1, keepdims = True) denominator = np.exp(t).sum(axis = 1, keepdims = True) activations = np.exp(t) / denominator return activations 12345678910# 测试样例parameterst = initialize(50, 10)Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])Ht = ReLU(Zt)Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])y_pred = softmax(Ot)print(y_pred.shape) # (1078, 10)print(Ot.mean()) # 0.000600192658464print(y_pred.mean()) # 0.1 接下来是实现损失函数，交叉熵损失函数： $$\\mathrm{loss} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\log{(\\hat{y_k})}$$ 这里又会出一个问题，交叉熵损失函数中，我们需要对$\\rm softmax$的激活值取对数，也就是$\\log{\\hat{y}}$，这就要求我们的激活值全都是大于0的数，不能等于0，但是我们实现的$\\rm softmax$在有些时候确实会输出0，比如： 1softmax(np.array([[1e32, 0, -1e32]])) 这就使得在计算loss的时候会出现问题，解决这个问题的方法是$\\rm log \\ softmax$。所谓$\\rm log \\ softmax$，就是将交叉熵中的对数运算与$\\rm softmax$结合起来，避开为0的情况 $$\\begin{aligned}\\log{\\frac{\\exp{(O_i)}}{\\sum_K \\exp{(O_k)}}} &amp;= \\log{\\frac{\\exp{(O_i - \\mathrm{max}(O))}}{\\sum_K \\exp{(O_k - \\mathrm{max}(O))}}}\\\\&amp;= O_i - \\mathrm{max}(O) - \\log{\\sum_K \\exp{(O_k - \\mathrm{max}(O))}}\\end{aligned}$$ 这样我们再计算$\\rm loss$的时候就可以把输出层的输出直接放到$\\rm log \\ softmax$中计算，不用先激活，再取对数了。 我们先编写log_softmax 1234567891011121314151617181920212223242526def log_softmax(x): ''' log softmax Parameters ---------- x: np.ndarray，待激活的矩阵 Returns ---------- log_activations: np.ndarray, 激活后取了对数的矩阵 ''' # 获取每行的最大值 max_ = np.max(x, axis = 1, keepdims = True) # 指数运算 exp_x = np.exp(x - max_) # 每行求和 Z = np.sum(exp_x, axis = 1, keepdims = True) # 求log softmax log_activations = x - max_ - np.log(Z) return log_activations 12345678# 测试样例parameterst = initialize(50, 10)Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])Ht = ReLU(Zt)Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])t = log_softmax(Ot)print(t.shape) # (1078, 10)print(t.mean()) # -2.30259148717 然后编写cross_entropy_with_softmax 1234567891011121314151617181920def cross_entropy_with_softmax(y_true, O): ''' 求解交叉熵损失函数，这里需要使用log softmax，所以参数分别是真值和未经softmax激活的输出值 Parameters ---------- y_true: np.ndarray，shape = (n, K), 真值 O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值 Returns ---------- loss: float, 平均的交叉熵损失值 ''' # 平均交叉熵损失 loss = - np.sum(log_softmax(O) * y_true) / len(y_true) return loss 1234567# 测试样例parameterst = initialize(50, 10)Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])Ht = ReLU(Zt)Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])losst = cross_entropy_with_softmax(trainY_mat, Ot)print(losst.mean()) # 2.30266707958 12345678910111213141516171819202122232425def forward(X, parameters): ''' 前向传播，从输入一直到输出层softmax激活前的值 Parameters ---------- X: np.ndarray, shape = (n, m)，输入的数据 parameters: dict，参数 Returns ---------- O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值 ''' # 输入层到隐藏层 Z = linear_combination(X, parameters['W1'], parameters['b1']) # 隐藏层的激活 H = ReLU(Z) # 隐藏层到输出层 O = linear_combination(H, parameters['W2'], parameters['b2']) return O 1234# 测试样例parameterst = initialize(50, 10)Ot = forward(trainX, parameterst)print(Ot.mean()) # 0.000600192658464 5. 反向传播先计算梯度 1234567891011121314151617181920212223242526272829303132333435363738394041424344def compute_gradient(y_true, y_pred, H, Z, X, parameters): ''' 计算梯度 Parameters ---------- y_true: np.ndarray，shape = (n, K), 真值 y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值 H: np.ndarray, shape = (n, h)，隐藏层激活后的值 Z: np.ndarray, shape = (n, h), 隐藏层激活前的值 X: np.ndarray, shape = (n, m)，输入的原始数据 parameters: dict，参数 Returns ---------- grads: dict, 梯度 ''' # 计算W2的梯度 dW2 = np.dot(H.T, (y_pred - y_true)) / len(y_pred) # 计算b2的梯度 db2 = np.sum(y_pred - y_true, axis = 0) / len(y_pred) # 计算ReLU的梯度 relu_grad = Z.copy() relu_grad[relu_grad &lt; 0] = 0 relu_grad[relu_grad &gt;= 0] = 1 # 计算W1的梯度 dW1 = np.dot(X.T, np.dot(y_pred - y_true, parameters['W2'].T) * relu_grad) / len(y_pred) # 计算b1的梯度 db1 = np.sum((np.dot(y_pred - y_true, parameters['W2'].T) * relu_grad), axis = 0) / len(y_pred) grads = {'dW2': dW2, 'db2': db2, 'dW1': dW1, 'db1': db1} return grads 1234567891011121314# 测试样例parameterst = initialize(50, 10)Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])Ht = ReLU(Zt)Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])y_predt = softmax(Ot)gradst = compute_gradient(trainY_mat, y_predt, Ht, Zt, trainX, parameterst)print(gradst['dW1'].sum()) # 0.0429186117668print(gradst['db1'].sum()) # -5.05985151857e-05print(gradst['dW2'].sum()) # -2.16840434497e-18print(gradst['db2'].sum()) # -1.34441069388e-17 梯度下降，参数更新 1234567891011121314151617def update(parameters, grads, learning_rate): ''' 参数更新 Parameters ---------- parameters: dict，参数 grads: dict, 梯度 learning_rate: float, 学习率 ''' parameters['W2'] -= learning_rate * grads['dW2'] parameters['b2'] -= learning_rate * grads['db2'] parameters['W1'] -= learning_rate * grads['dW1'] parameters['b1'] -= learning_rate * grads['db1'] 反向传播，参数更新 1234567891011121314151617181920# 测试样例parameterst = initialize(50, 10)print(parameterst['W1'].sum()) # 0.583495454481print(parameterst['b1'].sum()) # 0.0print(parameterst['W2'].sum()) # 0.1888716431print(parameterst['b2'].sum()) # 0.0print()Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])Ht = ReLU(Zt)Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])y_predt = softmax(Ot)gradst = compute_gradient(trainY_mat, y_predt, Ht, Zt, trainX, parameterst)update(parameterst, gradst, 0.1)print(parameterst['W1'].sum()) # 0.579203593304print(parameterst['b1'].sum()) # 5.05985151857e-06print(parameterst['W2'].sum()) # 0.1888716431print(parameterst['b2'].sum()) # 1.24683249836e-18 1234567891011121314151617181920212223def backward(y_true, y_pred, H, Z, X, parameters, learning_rate): ''' 计算梯度，参数更新 Parameters ---------- y_true: np.ndarray，shape = (n, K), 真值 y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值 H: np.ndarray, shape = (n, h)，隐藏层激活后的值 Z: np.ndarray, shape = (n, h), 隐藏层激活前的值 X: np.ndarray, shape = (n, m)，输入的原始数据 parameters: dict，参数 learning_rate: float, 学习率 ''' grads = compute_gradient(y_true, y_pred, H, Z, X, parameters) update(parameters, grads, learning_rate) 12345678910111213141516171819# 测试样例parameterst = initialize(50, 10)print(parameterst['W1'].sum()) # 0.583495454481print(parameterst['b1'].sum()) # 0.0print(parameterst['W2'].sum()) # 0.1888716431print(parameterst['b2'].sum()) # 0.0print()Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])Ht = ReLU(Zt)Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])y_predt = softmax(Ot)backward(trainY_mat, y_predt, Ht, Zt, trainX, parameterst, 0.1)print(parameterst['W1'].sum()) # 0.579203593304print(parameterst['b1'].sum()) # 5.05985151857e-06print(parameterst['W2'].sum()) # 0.1888716431print(parameterst['b2'].sum()) # 1.24683249836e-18 6. 训练12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def train(trainX, trainY, testX, testY, parameters, epochs, learning_rate = 0.01, verbose = False): ''' 训练 Parameters ---------- Parameters ---------- trainX: np.ndarray, shape = (n, m), 训练集 trainY: np.ndarray, shape = (n, K), 训练集标记 testX: np.ndarray, shape = (n_test, m)，测试集 testY: np.ndarray, shape = (n_test, K)，测试集的标记 parameters: dict，参数 epochs: int, 要迭代的轮数 learning_rate: float, default 0.01，学习率 verbose: boolean, default False，是否打印损失值 Returns ---------- training_loss_list: list(float)，每迭代一次之后，训练集上的损失值 testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值 ''' # 存储损失值 training_loss_list = [] testing_loss_list = [] for i in range(epochs): # 这里要计算出Z和H，因为后面反向传播计算梯度的时候需要这两个矩阵 Z = linear_combination(trainX, parameters['W1'], parameters['b1']) H = ReLU(Z) train_O = linear_combination(H, parameters['W2'], parameters['b2']) train_y_pred = softmax(train_O) training_loss = cross_entropy_with_softmax(trainY, train_O) test_O = forward(testX, parameters) testing_loss = cross_entropy_with_softmax(testY, test_O) if verbose == True: print('epoch %s, training loss:%s'%(i + 1, training_loss)) print('epoch %s, testing loss:%s'%(i + 1, testing_loss)) print() training_loss_list.append(training_loss) testing_loss_list.append(testing_loss) backward(trainY, train_y_pred, H, Z, trainX, parameters, learning_rate) return training_loss_list, testing_loss_list 1234567891011121314# 测试样例parameterst = initialize(50, 10)print(parameterst['W1'].sum()) # 0.583495454481print(parameterst['b1'].sum()) # 0.0print(parameterst['W2'].sum()) # 0.1888716431print(parameterst['b2'].sum()) # 0.0print()training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameterst, 1, 0.1, False)print(parameterst['W1'].sum()) # 0.579203593304print(parameterst['b1'].sum()) # 5.05985151857e-06print(parameterst['W2'].sum()) # 0.1888716431print(parameterst['b2'].sum()) # 1.24683249836e-18 7. 绘制模型损失值变化曲线1234567891011121314151617def plot_loss_curve(training_loss_list, testing_loss_list): ''' 绘制损失值变化曲线 Parameters ---------- training_loss_list: list(float)，每迭代一次之后，训练集上的损失值 testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值 ''' plt.figure(figsize = (10, 6)) plt.plot(training_loss_list, label = 'training loss') plt.plot(testing_loss_list, label = 'testing loss') plt.xlabel('epoch') plt.ylabel('loss') plt.legend() 8. 预测模型训练完后，我们的就可以进行预测了，需要注意的是，我们的神经网络是针对每个样本，输出其分别属于$K$类的概率，我们要找最大的那个概率，对应的是哪个类。 12345678910111213141516171819202122232425def predict(X, parameters): ''' 预测，调用forward函数完成神经网络对输入X的计算，然后完成类别的划分，取每行最大的那个数的下标作为标记 Parameters ---------- X: np.ndarray, shape = (n, m), 训练集 parameters: dict，参数 Returns ---------- prediction: np.ndarray, shape = (n, 1)，预测的标记 ''' # 用forward函数得到softmax激活前的值 O = forward(X, parameters) # 计算softmax激活后的值 y_pred = softmax(O) # 取每行最大的元素对应的下标 prediction = np.argmax(y_pred, axis = 1) return prediction 12345678# 测试样例from sklearn.metrics import accuracy_scoreparameterst = initialize(50, 10)training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameterst, 1, 0.1, False)predictiont = predict(testX, parameterst)accuracy_score(predictiont, testY) # 0.15994436717663421 9. 训练一个三层感知机隐藏层单元数设置为50，输出层单元数为10，我们设置学习率为0.03，迭代轮数为1000轮 123456789start_time = time()h = 50K = 10parameters = initialize(h, K)training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameters, 1000, 0.03, False)end_time = time()print('training time: %s s'%(end_time - start_time)) 计算测试集精度 12prediction = predict(testX, parameters)accuracy_score(prediction, testY) 绘制损失值变化曲线 1plot_loss_curve(training_loss_list, testing_loss_list) 更换数据集我们换一个数据集，使用MNIST手写数字数据集，我们使用的是kaggle提供的MNIST手写数字识别比赛的训练集。这个数据集还是手写数字的图片，只不过像素变成了 $28 \\times 28$，图片的尺寸变大了，而且数据集的样本量也大了。我们取30%为测试集，70%为训练集。训练集样本数有29400个，测试集12600个。 12345678910111213import pandas as pddata = pd.read_csv('data/kaggle_mnist/mnist_train.csv')X = data.values[:, 1:].astype('float32')Y = data.values[:, 0]trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.3, random_state = 32)trainY_mat = np.zeros((len(trainY), 10))trainY_mat[np.arange(0, len(trainY), 1), trainY] = 1testY_mat = np.zeros((len(testY), 10))testY_mat[np.arange(0, len(testY), 1), testY] = 1 1trainX.shape, trainY.shape, trainY_mat.shape, testX.shape, testY.shape, testY_mat.shape 绘制训练集前10个图像 123456_, figs = plt.subplots(1, 10, figsize=(8, 4))for f, img, lbl in zip(figs, trainX[:10], trainY[:10]): f.imshow(img.reshape((28, 28)), cmap = 'gray') f.set_title(lbl) f.axes.get_xaxis().set_visible(False) f.axes.get_yaxis().set_visible(False) test：请你使用kaggle MNIST数据集，根据下表设定各个超参数，计算测试集上的精度，绘制损失值变化曲线，填写下表任务流程： 对数据集进行标准化处理 设定学习率和迭代轮数进行训练 计算测试集精度 绘制曲线 双击此处填写精度保留4位小数；训练时间单位为秒，保留两位小数。 隐藏层单元数 学习率 迭代轮数 测试集精度 训练时间(秒) 100 0.1 50 0.8071 50.47 100 0.1 100 0.8877 100.99 100 0.1 150 0.8998 147.81 100 0.1 500 0.9197 497.71 100 0.01 500 0.8125 484.04 123stand = StandardScaler()trainX_normalized = stand.fit_transform(trainX)testX_normalized = stand.fit_transform(testX) 123456789101112start_time = time()h = 100K = 10parameters = initialize(h, K)training_loss_list, testing_loss_list = train(trainX_normalized, trainY_mat, testX_normalized, testY_mat, parameters, 50, 0.1)prediction = predict(testX_normalized, parameters)print('testing accuracy:', accuracy_score(prediction, testY))plot_loss_curve(training_loss_list, testing_loss_list)end_time = time()print('training time: %s s'%(end_time - start_time)) 123456789101112start_time = time()h = 100K = 10parameters = initialize(h, K)training_loss_list, testing_loss_list = train(trainX_normalized, trainY_mat, testX_normalized, testY_mat, parameters, 100, 0.1)prediction = predict(testX_normalized, parameters)print('testing accuracy:', accuracy_score(prediction, testY))plot_loss_curve(training_loss_list, testing_loss_list)end_time = time()print('training time: %s s'%(end_time - start_time)) 123456789101112start_time = time()h = 100K = 10parameters = initialize(h, K)training_loss_list, testing_loss_list = train(trainX_normalized, trainY_mat, testX_normalized, testY_mat, parameters, 150, 0.1)prediction = predict(testX_normalized, parameters)print('testing accuracy:', accuracy_score(prediction, testY))plot_loss_curve(training_loss_list, testing_loss_list)end_time = time()print('training time: %s s'%(end_time - start_time)) 123456789101112start_time = time()h = 100K = 10parameters = initialize(h, K)training_loss_list, testing_loss_list = train(trainX_normalized, trainY_mat, testX_normalized, testY_mat, parameters, 500, 0.1)prediction = predict(testX_normalized, parameters)print('testing accuracy:', accuracy_score(prediction, testY))plot_loss_curve(training_loss_list, testing_loss_list)end_time = time()print('training time: %s s'%(end_time - start_time)) 123456789101112start_time = time()h = 100K = 10parameters = initialize(h, K)training_loss_list, testing_loss_list = train(trainX_normalized, trainY_mat, testX_normalized, testY_mat, parameters, 500, 0.01)prediction = predict(testX_normalized, parameters)print('testing accuracy:', accuracy_score(prediction, testY))plot_loss_curve(training_loss_list, testing_loss_list)end_time = time()print('training time: %s s'%(end_time - start_time))","link":"/blog/2018/09/11/logistic-regression/"},{"title":"决策树实现","text":"最近给本科生当助教，出了一道实现决策树的题，还有一个预剪枝的题，自己也顺便实现一下。 我实现的这个决策树主要是参照了C4.5算法。没加入剪枝。实现的其实很简单，只针对离散特征，做了一个二叉决策树。也就是将所有特征先做one-hot，这样所有的特征都变成0和1了，然后对其进行二分。 原理其实很简单，选择一种划分指标，遍历所有的特征，找到最优划分特征，然后分割训练集，从剩余特征中删除当前的最优特征，然后分左子树和右子树递归地继续创建结点即可。无非是递归的终止条件，递归的终止条件有三点： 如果当前结点内所有的样本同属一类，则直接做叶子结点 如果当前深度达到最大深度，直接做叶子结点 如果无剩余特征可供划分，直接做叶子节点 第三题：实现决策树实验内容：使用LendingClub Safe Loans数据集： 实现信息增益、信息增益率、基尼指数三种划分标准 使用给定的训练集完成三种决策树的训练过程 计算三种决策树在最大深度为10时在训练集和测试集上的精度，查准率，查全率，F1值 在这部分，我们会实现一个很简单的二叉决策树 1. 读取数据1234# 导入类库import pandas as pdimport numpy as npimport json 12# 导入数据loans = pd.read_csv('data/lendingclub/lending-club-data.csv', low_memory=False) 数据中有两列是我们想预测的指标，一项是safe_loans，一项是bad_loans，分别表示正例和负例，我们对其进行处理，将正例的safe_loans设为1，负例设为-1，删除bad_loans这列 123# 对数据进行预处理，将safe_loans作为标记loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)del loans['bad_loans'] 我们只使用grade, term, home_ownership, emp_length这四列作为特征，safe_loans作为标记，只保留loans中的这五列 1234567features = ['grade', # grade of the loan 'term', # the term of the loan 'home_ownership', # home_ownership status: own, mortgage or rent 'emp_length', # number of years of employment ]target = 'safe_loans'loans = loans[features + [target]] 2. 划分训练集和测试集123456from sklearn.utils import shuffleloans = shuffle(loans, random_state = 34)split_line = int(len(loans) * 0.6)train_data = loans.iloc[: split_line]test_data = loans.iloc[split_line:] 3. 特征预处理可以看到所有的特征都是离散类型的特征，需要对数据进行预处理，使用one-hot编码对其进行处理。 one-hot编码的思想就是将离散特征变成向量，假设特征$A$有三种取值$\\lbrace a, b, c\\rbrace$，这三种取值等价，如果我们使用1,2,3三个数字表示这三种取值，那么在计算时就会产生偏差，有一些涉及距离度量的算法会认为，2和1离得近，3和1离得远，但这三个值应该是等价的，这种表示方法会造成模型在判断上出现偏差。解决方案就是使用一个三维向量表示他们，用$[1, 0, 0]$表示a，$[0, 1, 0]$表示b，$[0, 0, 1]$表示c，这样三个向量之间的距离就都是相等的了，任意两个向量在欧式空间的距离都是$\\sqrt{2}$。这就是one-hot编码是思想。 pandas中使用get_dummies生成one-hot向量 12345678910111213141516171819202122def one_hot_encoding(data, features_categorical): ''' Parameter ---------- data: pd.DataFrame features_categorical: list(str) ''' # 对所有的离散特征遍历 for cat in features_categorical: # 对这列进行one-hot编码，前缀为这个变量名 one_encoding = pd.get_dummies(data[cat], prefix = cat) # 将生成的one-hot编码与之前的dataframe拼接起来 data = pd.concat([data, one_encoding],axis=1) # 删除掉原始的这列离散特征 del data[cat] return data 首先对训练集生成one-hot向量，然后对测试集生成one-hot向量，这里需要注意的是，如果训练集中，特征$A$的取值为$\\lbrace a, b, c\\rbrace$，这样我们生成的特征就有三列，分别为$A_a$, $A_b$, $A_c$，然后我们使用这个训练集训练模型，模型就只会考虑这三个特征，在测试集中如果有一个样本的特征$A$的值为$d$，那它的$A_a$，$A_b$，$A_c$就都为0，我们不去考虑$A_d$，因为这个特征在训练模型的时候是不存在的。 1train_data = one_hot_encoding(train_data, features) 获取所有特征的名字 123one_hot_features = train_data.columns.tolist()one_hot_features.remove(target)one_hot_features [&apos;grade_A&apos;, &apos;grade_B&apos;, &apos;grade_C&apos;, &apos;grade_D&apos;, &apos;grade_E&apos;, &apos;grade_F&apos;, &apos;grade_G&apos;, &apos;term_ 36 months&apos;, &apos;term_ 60 months&apos;, &apos;home_ownership_MORTGAGE&apos;, &apos;home_ownership_OTHER&apos;, &apos;home_ownership_OWN&apos;, &apos;home_ownership_RENT&apos;, &apos;emp_length_1 year&apos;, &apos;emp_length_10+ years&apos;, &apos;emp_length_2 years&apos;, &apos;emp_length_3 years&apos;, &apos;emp_length_4 years&apos;, &apos;emp_length_5 years&apos;, &apos;emp_length_6 years&apos;, &apos;emp_length_7 years&apos;, &apos;emp_length_8 years&apos;, &apos;emp_length_9 years&apos;, &apos;emp_length_&lt; 1 year&apos;] 接下来是对测试集进行one_hot编码，但只要保留出现在one_hot_features中的特征即可· 1test_data_tmp = one_hot_encoding(test_data, features) 123456789# 创建一个空的DataFrametest_data = pd.DataFrame(columns = train_data.columns)for feature in train_data.columns: # 如果训练集中当前特征在test_data_tmp中出现了，将其复制到test_data中 if feature in test_data_tmp.columns: test_data[feature] = test_data_tmp[feature].copy() else: # 否则就用全为0的列去替代 test_data[feature] = np.zeros(test_data_tmp.shape[0], dtype = 'uint8') 1train_data.shape (73564, 25) 1test_data.shape (49043, 25) 训练集有37224个样本，测试集有9284个样本，处理完后，所有的特征都是0和1，标记是1和-1，以上就是数据预处理流程 4. 实现3种特征划分准则决策树中有很多常用的特征划分方法，比如信息增益、信息增益率、基尼指数 我们需要实现一个函数，它的作用是，给定决策树的某个结点内的所有样本的标记，让它计算出对应划分指标的值是多少 接下来我们会实现上述三种划分指标 这里我们约定，将所有特征取值为0的样本，划分到左子树，特征取值为1的样本，划分到右子树 4.1 信息增益信息熵：$$\\mathrm{Ent}(D) = - \\sum^{\\vert \\mathcal{Y} \\vert}_{k = 1} p_k \\mathrm{log}_2 p_k$$ 信息增益：$$\\mathrm{Gain}(D, a) = \\mathrm{Ent}(D) - \\sum^{V}_{v=1} \\frac{\\vert D^v \\vert}{\\vert D \\vert} \\mathrm{Ent}(D^v)$$ 计算信息熵时约定：若$p = 0$，则$p \\log_2p = 0$ 12345678910111213141516171819202122232425262728293031323334353637383940414243def information_entropy(labels_in_node): ''' 求当前结点的信息熵 Parameter ---------- labels_in_node: np.ndarray, 如[-1, 1, -1, 1, 1] Returns ---------- float: information entropy ''' # 统计样本总个数 num_of_samples = labels_in_node.shape[0] if num_of_samples == 0: return 0 # 统计出标记为1的个数 num_of_positive = len(labels_in_node[labels_in_node == 1]) # 统计出标记为-1的个数 # YOUR CODE HERE num_of_negative = len(labels_in_node[labels_in_node == -1]) # 统计正例的概率 prob_positive = num_of_positive / num_of_samples # 统计负例的概率 # YOUR CODE HERE prob_negative = num_of_negative / num_of_samples if prob_positive == 0: positive_part = 0 else: positive_part = prob_positive * np.log2(prob_positive) if prob_negative == 0: negative_part = 0 else: negative_part = prob_negative * np.log2(prob_negative) return - ( positive_part + negative_part ) 下面是6个测试样例 1234567891011121314151617181920212223# 信息熵测试样例1example_labels = np.array([-1, -1, 1, 1, 1])print(information_entropy(example_labels)) # 0.97095# 信息熵测试样例2example_labels = np.array([-1, -1, 1, 1, 1, 1, 1])print(information_entropy(example_labels)) # 0.86312 # 信息熵测试样例3example_labels = np.array([-1, -1, -1, -1, -1, 1, 1])print(information_entropy(example_labels)) # 0.86312# 信息熵测试样例4example_labels = np.array([-1] * 9 + [1] * 8)print(information_entropy(example_labels)) # 0.99750# 信息熵测试样例5example_labels = np.array([1] * 8)print(information_entropy(example_labels)) # 0# 信息熵测试样例6example_labels = np.array([])print(information_entropy(example_labels)) # 0 0.970950594455 0.863120568567 0.863120568567 0.997502546369 -0.0 0 接下来完成计算所有特征的信息增益的函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def compute_information_gains(data, features, target, annotate = False): ''' 计算所有特征的信息增益 Parameter ---------- data: pd.DataFrame，传入的样本，带有特征和标记的dataframe features: list(str)，特征名组成的list target: str, 标记(label)的名字 annotate, boolean，是否打印所有特征的信息增益值，默认为False Returns ---------- information_gains: dict, key: str, 特征名 value: float，信息增益 ''' # 我们将使用每个特征划分的信息增益值存储在这个dict中 # 键是特征名，值是信息增益值 information_gains = dict() # 对所有的特征进行遍历，使用当前的划分方法对每个特征进行计算 for feature in features: # 左子树保证所有的样本的这个特征取值为0 left_split_target = data[data[feature] == 0][target] # 右子树保证所有的样本的这个特征取值为1 right_split_target = data[data[feature] == 1][target] # 计算左子树的信息熵 left_entropy = information_entropy(left_split_target) # 计算左子树的权重 left_weight = len(left_split_target) / (len(left_split_target) + len(right_split_target)) # 计算右子树的信息熵 # YOUR CODE HERE right_entropy = information_entropy(right_split_target) # 计算右子树的权重 # YOUR CODE HERE right_weight = len(right_split_target) / (len(left_split_target) + len(right_split_target)) # 计算当前结点的信息熵 current_entropy = information_entropy(data[target]) # 计算使用当前特征划分的信息增益 # YOUR CODE HERE gain = current_entropy - (left_weight * left_entropy + right_weight * right_entropy) # 将特征名与增益值以键值对的形式存储在information_gains中 information_gains[feature] = gain if annotate: print(\" \", feature, gain) return information_gains 12345678# 信息增益测试样例1print(compute_information_gains(train_data, one_hot_features, target)['grade_A']) # 0.01759# 信息增益测试样例2print(compute_information_gains(train_data, one_hot_features, target)['term_ 60 months']) # 0.01429# 信息增益测试样例3print(compute_information_gains(train_data, one_hot_features, target)['grade_B']) # 0.00370 0.0175919801789 0.0142918503294 0.00370492003453 4.2 信息增益率信息增益率： $$\\mathrm{Gain_ratio}(D, a) = \\frac{\\mathrm{Gain}(D, a)}{\\mathrm{IV}(a)}$$ 其中 $$\\mathrm{IV}(a) = - \\sum^V_{v=1} \\frac{\\vert D^v \\vert}{\\vert D \\vert} \\log_2 \\frac{\\vert D^v \\vert}{\\vert D \\vert}$$ 完成计算所有特征信息增益率的函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778def compute_information_gain_ratios(data, features, target, annotate = False): ''' 计算所有特征的信息增益率并保存起来 Parameter ---------- data: pd.DataFrame, 带有特征和标记的数据 features: list(str)，特征名组成的list target: str， 特征的名字 annotate: boolean, default False，是否打印注释 Returns ---------- gain_ratios: dict, key: str, 特征名 value: float，信息增益率 ''' gain_ratios = dict() # 对所有的特征进行遍历，使用当前的划分方法对每个特征进行计算 for feature in features: # 左子树保证所有的样本的这个特征取值为0 left_split_target = data[data[feature] == 0][target] # 右子树保证所有的样本的这个特征取值为1 right_split_target = data[data[feature] == 1][target] # 计算左子树的信息熵 left_entropy = information_entropy(left_split_target) # 计算左子树的权重 left_weight = len(left_split_target) / (len(left_split_target) + len(right_split_target)) # 计算右子树的信息熵 right_entropy = information_entropy(right_split_target) # 计算右子树的权重 right_weight = len(right_split_target) / (len(left_split_target) + len(right_split_target)) # 计算当前结点的信息熵 current_entropy = information_entropy(data[target]) # 计算当前结点的信息增益 # YOUR CODE HERE gain = current_entropy - (left_weight * left_entropy + right_weight * right_entropy) # 计算左子树的IV if left_weight == 0: left_IV = 0 else: # YOUR CODE HERE left_IV = left_weight * np.log2(left_weight) # 计算右子树的IV if right_weight == 0: right_IV = 0 else: # YOUR CODE HERE right_IV = right_weight * np.log2(right_weight) # IV 等于所有子树IV之和的相反数 IV = - (left_IV + right_IV) # 计算使用当前特征划分的信息增益率 # 这里为了防止IV是0，导致除法得到np.inf，在分母加了一个很小的小数 gain_ratio = gain / (IV + np.finfo(np.longdouble).eps) # 信息增益率的存储 gain_ratios[feature] = gain_ratio if annotate: print(\" \", feature, gain_ratio) return gain_ratios 12345678# 信息增益率测试样例1print(compute_information_gain_ratios(train_data, one_hot_features, target)['grade_A']) # 0.02573# 信息增益率测试样例2print(compute_information_gain_ratios(train_data, one_hot_features, target)['grade_B']) # 0.00417# 信息增益率测试样例3print(compute_information_gain_ratios(train_data, one_hot_features, target)['term_ 60 months']) # 0.01970 0.025734780668 0.00417549506943 0.0197093627186 4.3 基尼指数数据集$D$的基尼值： $$\\begin{aligned}\\mathrm{Gini}(D) &amp; = \\sum^{\\vert \\mathcal{Y} \\vert}_{k=1} \\sum_{k’ \\neq k} p_k p_{k’}\\\\&amp; = 1 - \\sum^{\\vert \\mathcal{Y} \\vert}_{k=1} p^2_k.\\end{aligned}$$ 属性$a$的基尼指数： $$\\mathrm{Gini_index}(D, a) = \\sum^V_{v = 1} \\frac{\\vert D^v \\vert}{\\vert D \\vert} \\mathrm{Gini}(D^v)$$ 完成数据集基尼值的计算 1234567891011121314151617181920212223242526272829303132333435363738def gini(labels_in_node): ''' 计算一个结点内样本的基尼指数 Paramters ---------- label_in_data: np.ndarray, 样本的标记，如[-1, -1, 1, 1, 1] Returns --------- gini: float，基尼指数 ''' # 统计样本总个数 num_of_samples = labels_in_node.shape[0] if num_of_samples == 0: return 0 # 统计出1的个数 num_of_positive = len(labels_in_node[labels_in_node == 1]) # 统计出-1的个数 # YOUR CODE HERE num_of_negative = len(labels_in_node[labels_in_node == -1]) # 统计正例的概率 prob_positive = num_of_positive / num_of_samples # 统计负例的概率 # YOUR CODE HERE prob_negative = num_of_negative / num_of_samples # 计算基尼值 # YOUR CODE HERE gini = 1 - (prob_positive ** 2 + prob_negative ** 2) return gini 1234567891011121314151617181920212223# 基尼值测试样例1example_labels = np.array([-1, -1, 1, 1, 1])print(gini(example_labels)) # 0.48# 基尼值测试样例2example_labels = np.array([-1, -1, 1, 1, 1, 1, 1])print(gini(example_labels)) # 0.40816 # 基尼值测试样例3example_labels = np.array([-1, -1, -1, -1, -1, 1, 1])print(gini(example_labels)) # 0.40816# 基尼值测试样例4example_labels = np.array([-1] * 9 + [1] * 8)print(gini(example_labels)) # 0.49827# 基尼值测试样例5example_labels = np.array([1] * 8)print(gini(example_labels)) # 0# 基尼值测试样例6example_labels = np.array([])print(gini(example_labels)) # 0 0.48 0.40816326530612246 0.40816326530612246 0.4982698961937716 0.0 0 然后计算所有特征的基尼指数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354def compute_gini_indices(data, features, target, annotate = False): ''' 计算使用各个特征进行划分时，各特征的基尼指数 Parameter ---------- data: pd.DataFrame, 带有特征和标记的数据 features: list(str)，特征名组成的list target: str， 特征的名字 annotate: boolean, default False，是否打印注释 Returns ---------- gini_indices: dict, key: str, 特征名 value: float，基尼指数 ''' gini_indices = dict() # 对所有的特征进行遍历，使用当前的划分方法对每个特征进行计算 for feature in features: # 左子树保证所有的样本的这个特征取值为0 left_split_target = data[data[feature] == 0][target] # 右子树保证所有的样本的这个特征取值为1 right_split_target = data[data[feature] == 1][target] # 计算左子树的基尼值 left_gini = gini(left_split_target) # 计算左子树的权重 left_weight = len(left_split_target) / (len(left_split_target) + len(right_split_target)) # 计算右子树的基尼值 # YOUR CODE HERE right_gini = gini(right_split_target) # 计算右子树的权重 # YOUR CODE HERE right_weight = len(right_split_target) / (len(left_split_target) + len(right_split_target)) # 计算当前结点的基尼指数 # YOUR CODE HERE gini_index = left_weight * left_gini + right_weight * right_gini # 存储 gini_indices[feature] = gini_index if annotate: print(\" \", feature, gini_index) return gini_indices 12345678# 基尼指数测试样例1print(compute_gini_indices(train_data, one_hot_features, target)['grade_A']) # 0.30095# 基尼指数测试样例2print(compute_gini_indices(train_data, one_hot_features, target)['grade_B']) # 0.30568# 基尼指数测试样例3print(compute_gini_indices(train_data, one_hot_features, target)['term_ 36 months']) # 0.30055 0.3009520964964362 0.3056855375882364 0.30055418611740065 5. 完成最优特征的选择到此，我们完成了三种划分策略的实现，接下来就是完成获取最优特征的函数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364def best_splitting_feature(data, features, target, criterion = 'gini', annotate = False): ''' 给定划分方法和数据，找到最优的划分特征 Parameters ---------- data: pd.DataFrame, 带有特征和标记的数据 features: list(str)，特征名组成的list target: str， 特征的名字 criterion: str, 使用哪种指标，三种选项: 'information_gain', 'gain_ratio', 'gini' annotate: boolean, default False，是否打印注释 Returns ---------- best_feature: str, 最佳的划分特征的名字 ''' if criterion == 'information_gain': if annotate: print('using information gain') # 得到当前所有特征的信息增益 information_gains = compute_information_gains(data, features, target, annotate) # information_gains是一个dict类型的对象，我们要找值最大的那个元素的键是谁 # 可以通过这种方式直接获取这个键 # 根据这些特征和他们的信息增益，找到最佳的划分特征 # YOUR CODE HERE best_feature = max(information_gains.items(), key = lambda x: x[1])[0] return best_feature elif criterion == 'gain_ratio': if annotate: print('using information gain ratio') # 得到当前所有特征的信息增益率 gain_ratios = compute_information_gain_ratios(data, features, target, annotate) # 根据这些特征和他们的信息增益率，找到最佳的划分特征 # YOUR CODE HERE best_feature = max(gain_ratios.items(), key = lambda x: x[1])[0] return best_feature elif criterion == 'gini': if annotate: print('using gini') # 得到当前所有特征的基尼指数 gini_indices = compute_gini_indices(data, features, target, annotate) # 根据这些特征和他们的基尼指数，找到最佳的划分特征 # YOUR CODE HERE best_feature = min(gini_indices.items(), key = lambda x: x[1])[0] return best_feature else: raise Exception(\"传入的criterion不合规!\", criterion) 6. 判断结点内样本的类别是否为同一类1234567891011121314151617181920212223242526def intermediate_node_num_mistakes(labels_in_node): ''' 求树的结点中，样本数少的那个类的样本有多少，比如输入是[1, 1, -1, -1, 1]，返回2 Parameter ---------- labels_in_node: np.ndarray, pd.Series Returns ---------- int：个数 ''' # 如果传入的array为空，返回0 if len(labels_in_node) == 0: return 0 # 统计1的个数 # YOUR CODE HERE num_of_one = len(labels_in_node[labels_in_node == 1]) # 统计-1的个数 # YOUR CODE HERE num_of_minus_one = len(labels_in_node[labels_in_node == -1]) return num_of_one if num_of_minus_one &gt; num_of_one else num_of_minus_one 12345678# 测试样例1print(intermediate_node_num_mistakes(np.array([1, 1, -1, -1, -1]))) # 2# 测试样例2print(intermediate_node_num_mistakes(np.array([]))) # 0# 测试样例3print(intermediate_node_num_mistakes(np.array([1]))) # 0 2 0 0 7. 创建叶子结点12345678910111213141516171819202122232425262728293031323334353637def create_leaf(target_values): ''' 计算出当前叶子结点的标记是什么，并且将叶子结点信息保存在一个dict中 Parameter: ---------- target_values: pd.Series, 当前叶子结点内样本的标记 Returns: ---------- leaf: dict，表示一个叶结点， leaf['splitting_features'], None，叶结点不需要划分特征 leaf['left'], None，叶结点没有左子树 leaf['right'], None，叶结点没有右子树 leaf['is_leaf'], True, 是否是叶子结点 leaf['prediction'], int, 表示该叶子结点的预测值 ''' # 创建叶子结点 leaf = {'splitting_feature' : None, 'left' : None, 'right' : None, 'is_leaf': True} # 数结点内-1和+1的个数 num_ones = len(target_values[target_values == +1]) num_minus_ones = len(target_values[target_values == -1]) # 叶子结点的标记使用少数服从多数的原则，为样本数多的那类的标记，保存在 leaf['prediction'] if num_ones &gt; num_minus_ones: # YOUR CODE HERE leaf['prediction'] = 1 else: # YOUR CODE HERE leaf['prediction'] = -1 # 返回叶子结点 return leaf 8. 递归地创建决策树递归的创建决策树递归算法终止的三个条件： 如果结点内所有的样本的标记都相同，该结点就不需要再继续划分，直接做叶子结点即可 如果结点所有的特征都已经在之前使用过了，在当前结点无剩余特征可供划分样本，该结点直接做叶子结点 如果当前结点的深度已经达到了我们限制的树的最大深度，直接做叶子结点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103def decision_tree_create(data, features, target, criterion = 'gini', current_depth = 0, max_depth = 10, annotate = False): ''' Parameter: ---------- data: pd.DataFrame, 数据 features: iterable, 特征组成的可迭代对象，比如一个list target: str, 标记的名字 criterion: 'str', 特征划分方法，只支持三种：'information_gain', 'gain_ratio', 'gini' current_depth: int, 当前深度，递归的时候需要记录 max_depth: int, 树的最大深度，我们设定的树的最大深度，达到最大深度需要终止递归 Returns: ---------- dict, dict['is_leaf'] : False, 当前顶点不是叶子结点 dict['prediction'] : None, 不是叶子结点就没有预测值 dict['splitting_feature']: splitting_feature, 当前结点是使用哪个特征进行划分的 dict['left'] : dict dict['right'] : dict ''' if criterion not in ['information_gain', 'gain_ratio', 'gini']: raise Exception(\"传入的criterion不合规!\", criterion) # 复制一份特征，存储起来，每使用一个特征进行划分，我们就删除一个 remaining_features = features[:] # 取出标记值 target_values = data[target] print(\"-\" * 50) print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))) # 终止条件1 # 如果当前结点内所有样本同属一类，即这个结点中，各类别样本数最小的那个等于0 # 使用前面写的intermediate_node_num_mistakes来完成这个判断 # YOUR CODE HERE if intermediate_node_num_mistakes(target_values) == 0: print(\"Stopping condition 1 reached.\") return create_leaf(target_values) # 创建叶子节点 # 终止条件2 # 如果已经没有剩余的特征可供分割，即remaining_features为空 # YOUR CODE HERE if len(remaining_features) == 0: print(\"Stopping condition 2 reached.\") return create_leaf(target_values) # 创建叶子节点 # 终止条件3 # 如果已经到达了我们要求的最大深度，即当前深度达到了最大深度 # YOUR CODE HERE if current_depth &gt;= max_depth: print(\"Reached maximum depth. Stopping for now.\") return create_leaf(target_values) # 创建叶子节点 # 找到最优划分特征 # 使用best_splitting_feature这个函数 ## YOUR CODE HERE splitting_feature = best_splitting_feature(data, features, target, criterion, annotate) # 使用我们找到的最优特征将数据划分成两份 # 左子树的数据 left_split = data[data[splitting_feature] == 0] # 右子树的数据 # YOUR CODE HERE right_split = data[data[splitting_feature] == 1] # 现在已经完成划分，我们要从剩余特征中删除掉当前这个特征 remaining_features.remove(splitting_feature) # 打印当前划分使用的特征，打印左子树样本个数，右子树样本个数 print(\"Split on feature %s. (%s, %s)\" % (\\ splitting_feature, len(left_split), len(right_split))) # 如果使用当前的特征，将所有的样本都划分到一棵子树中，那么就直接将这棵子树变成叶子节点 # 判断左子树是不是“完美”的 if len(left_split) == len(data): print(\"Creating leaf node.\") return create_leaf(left_split[target]) # 判断右子树是不是“完美”的 if len(right_split) == len(data): print(\"Creating right node.\") # YOUR CODE HERE return create_leaf(right_split[target]) # 递归地创建左子树 left_tree = decision_tree_create(left_split, remaining_features, target, criterion, current_depth + 1, max_depth, annotate) # 递归地创建右子树 ## YOUR CODE HERE right_tree = decision_tree_create(right_split, remaining_features, target, criterion, current_depth + 1, max_depth, annotate) # 返回树的非叶子结点 return {'is_leaf' : False, 'prediction' : None, 'splitting_feature': splitting_feature, 'left' : left_tree, 'right' : right_tree} 1my_decision_tree = decision_tree_create(train_data, one_hot_features, target, 'gini', max_depth = 6, annotate = False) -------------------------------------------------- Subtree, depth = 0 (73564 data points). Split on feature term_ 36 months. (14831, 58733) -------------------------------------------------- Subtree, depth = 1 (14831 data points). Split on feature grade_F. (13003, 1828) -------------------------------------------------- Subtree, depth = 2 (13003 data points). Split on feature grade_E. (9818, 3185) -------------------------------------------------- Subtree, depth = 3 (9818 data points). Split on feature home_ownership_RENT. (6796, 3022) -------------------------------------------------- Subtree, depth = 4 (6796 data points). Split on feature grade_G. (6507, 289) -------------------------------------------------- Subtree, depth = 5 (6507 data points). Split on feature grade_D. (4368, 2139) -------------------------------------------------- Subtree, depth = 6 (4368 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (2139 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (289 data points). Split on feature home_ownership_OWN. (249, 40) -------------------------------------------------- Subtree, depth = 6 (249 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (40 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 4 (3022 data points). Split on feature grade_G. (2827, 195) -------------------------------------------------- Subtree, depth = 5 (2827 data points). Split on feature grade_D. (1651, 1176) -------------------------------------------------- Subtree, depth = 6 (1651 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (1176 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (195 data points). Split on feature emp_length_2 years. (176, 19) -------------------------------------------------- Subtree, depth = 6 (176 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (19 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 3 (3185 data points). Split on feature home_ownership_RENT. (1980, 1205) -------------------------------------------------- Subtree, depth = 4 (1980 data points). Split on feature emp_length_3 years. (1828, 152) -------------------------------------------------- Subtree, depth = 5 (1828 data points). Split on feature emp_length_10+ years. (1057, 771) -------------------------------------------------- Subtree, depth = 6 (1057 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (771 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (152 data points). Split on feature home_ownership_OTHER. (151, 1) -------------------------------------------------- Subtree, depth = 6 (151 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (1 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 4 (1205 data points). Split on feature emp_length_1 year. (1124, 81) -------------------------------------------------- Subtree, depth = 5 (1124 data points). Split on feature emp_length_8 years. (1073, 51) -------------------------------------------------- Subtree, depth = 6 (1073 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (51 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (81 data points). Split on feature grade_A. (81, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 2 (1828 data points). Split on feature home_ownership_RENT. (1030, 798) -------------------------------------------------- Subtree, depth = 3 (1030 data points). Split on feature emp_length_3 years. (957, 73) -------------------------------------------------- Subtree, depth = 4 (957 data points). Split on feature emp_length_2 years. (886, 71) -------------------------------------------------- Subtree, depth = 5 (886 data points). Split on feature home_ownership_OTHER. (884, 2) -------------------------------------------------- Subtree, depth = 6 (884 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (2 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 5 (71 data points). Split on feature home_ownership_MORTGAGE. (12, 59) -------------------------------------------------- Subtree, depth = 6 (12 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (59 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 4 (73 data points). Split on feature home_ownership_MORTGAGE. (13, 60) -------------------------------------------------- Subtree, depth = 5 (13 data points). Split on feature grade_A. (13, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 5 (60 data points). Split on feature grade_A. (60, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 3 (798 data points). Split on feature emp_length_7 years. (740, 58) -------------------------------------------------- Subtree, depth = 4 (740 data points). Split on feature emp_length_3 years. (673, 67) -------------------------------------------------- Subtree, depth = 5 (673 data points). Split on feature emp_length_9 years. (646, 27) -------------------------------------------------- Subtree, depth = 6 (646 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (27 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (67 data points). Split on feature grade_A. (67, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 4 (58 data points). Split on feature grade_A. (58, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 1 (58733 data points). Split on feature grade_A. (45632, 13101) -------------------------------------------------- Subtree, depth = 2 (45632 data points). Split on feature grade_B. (25130, 20502) -------------------------------------------------- Subtree, depth = 3 (25130 data points). Split on feature grade_C. (11066, 14064) -------------------------------------------------- Subtree, depth = 4 (11066 data points). Split on feature home_ownership_MORTGAGE. (6987, 4079) -------------------------------------------------- Subtree, depth = 5 (6987 data points). Split on feature grade_F. (6650, 337) -------------------------------------------------- Subtree, depth = 6 (6650 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (337 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (4079 data points). Split on feature grade_G. (4003, 76) -------------------------------------------------- Subtree, depth = 6 (4003 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (76 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 4 (14064 data points). Split on feature home_ownership_MORTGAGE. (8209, 5855) -------------------------------------------------- Subtree, depth = 5 (8209 data points). Split on feature emp_length_2 years. (7209, 1000) -------------------------------------------------- Subtree, depth = 6 (7209 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (1000 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (5855 data points). Split on feature emp_length_10+ years. (3802, 2053) -------------------------------------------------- Subtree, depth = 6 (3802 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (2053 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 3 (20502 data points). Split on feature home_ownership_MORTGAGE. (10775, 9727) -------------------------------------------------- Subtree, depth = 4 (10775 data points). Split on feature home_ownership_OTHER. (10741, 34) -------------------------------------------------- Subtree, depth = 5 (10741 data points). Split on feature emp_length_1 year. (9754, 987) -------------------------------------------------- Subtree, depth = 6 (9754 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (987 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (34 data points). Split on feature emp_length_10+ years. (27, 7) -------------------------------------------------- Subtree, depth = 6 (27 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (7 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 4 (9727 data points). Split on feature emp_length_&lt; 1 year. (9186, 541) -------------------------------------------------- Subtree, depth = 5 (9186 data points). Split on feature emp_length_9 years. (8807, 379) -------------------------------------------------- Subtree, depth = 6 (8807 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (379 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (541 data points). Split on feature grade_C. (541, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 2 (13101 data points). Split on feature home_ownership_MORTGAGE. (5830, 7271) -------------------------------------------------- Subtree, depth = 3 (5830 data points). Split on feature emp_length_3 years. (5283, 547) -------------------------------------------------- Subtree, depth = 4 (5283 data points). Split on feature emp_length_1 year. (4705, 578) -------------------------------------------------- Subtree, depth = 5 (4705 data points). Split on feature emp_length_7 years. (4467, 238) -------------------------------------------------- Subtree, depth = 6 (4467 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (238 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (578 data points). Split on feature home_ownership_OTHER. (576, 2) -------------------------------------------------- Subtree, depth = 6 (576 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (2 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 4 (547 data points). Split on feature home_ownership_OTHER. (545, 2) -------------------------------------------------- Subtree, depth = 5 (545 data points). Split on feature home_ownership_OWN. (468, 77) -------------------------------------------------- Subtree, depth = 6 (468 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (77 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (2 data points). Split on feature grade_B. (2, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 3 (7271 data points). Split on feature emp_length_2 years. (6702, 569) -------------------------------------------------- Subtree, depth = 4 (6702 data points). Split on feature emp_length_4 years. (6234, 468) -------------------------------------------------- Subtree, depth = 5 (6234 data points). Split on feature emp_length_3 years. (5689, 545) -------------------------------------------------- Subtree, depth = 6 (5689 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (545 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (468 data points). Split on feature grade_B. (468, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 4 (569 data points). Split on feature grade_B. (569, 0) Creating leaf node. 现在，模型就训练好了 9. 预测接下来我们需要完成预测函数 123456789101112131415161718192021222324252627282930def classify(tree, x, annotate = False): ''' 递归的进行预测，一次只能预测一个样本 Parameters ---------- tree: dict x: pd.Series，样本 x: pd.DataFrame, 待预测的样本 annotate, boolean, 是否显示注释 Returns ---------- 返回预测的标记 ''' if tree['is_leaf']: if annotate: print (\"At leaf, predicting %s\" % tree['prediction']) return tree['prediction'] else: split_feature_value = x[tree['splitting_feature']] if annotate: print (\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value)) if split_feature_value == 0: return classify(tree['left'], x, annotate) else: return classify(tree['right'], x, annotate) 我们取测试集第一个样本来测试 12test_sample = test_data.iloc[0]print(test_sample) safe_loans 1 grade_A 0 grade_B 0 grade_C 0 grade_D 0 grade_E 1 grade_F 0 grade_G 0 term_ 36 months 1 term_ 60 months 0 home_ownership_MORTGAGE 1 home_ownership_OTHER 0 home_ownership_OWN 0 home_ownership_RENT 0 emp_length_1 year 0 emp_length_10+ years 0 emp_length_2 years 1 emp_length_3 years 0 emp_length_4 years 0 emp_length_5 years 0 emp_length_6 years 0 emp_length_7 years 0 emp_length_8 years 0 emp_length_9 years 0 emp_length_&lt; 1 year 0 Name: 37225, dtype: int64 12print('True class: %s ' % (test_sample['safe_loans']))print('Predicted class: %s ' % classify(my_decision_tree, test_sample)) True class: 1 Predicted class: 1 打印出使用决策树判断的过程 1classify(my_decision_tree, test_sample, annotate=True) Split on term_ 36 months = 1 Split on grade_A = 0 Split on grade_B = 0 Split on grade_C = 0 Split on home_ownership_MORTGAGE = 1 Split on grade_G = 0 At leaf, predicting 1 1 10. 在测试集上对我们的模型进行评估1234from sklearn.metrics import accuracy_scorefrom sklearn.metrics import precision_scorefrom sklearn.metrics import recall_scorefrom sklearn.metrics import f1_score 先来编写一个批量预测的函数，传入的是整个测试集那样的pd.DataFrame，这个函数返回一个np.ndarray，存储模型的预测结果 123456789101112131415161718192021def predict(tree, data): ''' 按行遍历data，对每个样本进行预测，将值存储起来，最后返回np.ndarray Parameter ---------- tree, dict, 模型 data, pd.DataFrame, 数据 Returns ---------- predictions, np.ndarray, 模型对这些样本的预测结果 ''' predictions = np.zeros(len(data)) # YOUR CODE HERE for i in range(len(data)): predictions[i] = classify(tree, data.iloc[i]) return predictions 11. 请你计算使用不同评价指标得到模型的四项指标的值，填写在下方表格内树的最大深度为6 1# YOUR CODE HERE 树的最大深度为6 双击此处编写 划分标准 精度 查准率 查全率 F1 信息增益 0.0 0.0 0.0 0.0 信息增益率 0.0 0.0 0.0 0.0 基尼指数 0.0 0.0 0.0 0.0 12. 扩展：使用Echarts绘制决策树我们可以使用echarts绘制出我们训练的决策树，这时候可以利用pyecharts这个库pyechartspyecharts可以与jupyter notebook无缝衔接，直接在notebook中绘制图表。提醒：pyecharts还未支持jupyter lab 12# 导入树形图from pyecharts import Tree echarts中的树形图要求我们提供一组这样的数据12345678910111213141516171819202122232425262728[ { value: 1212, # 数值 # 子节点 children: [ { # 子节点数值 value: 2323, # 子节点名 name: &apos;description of this node&apos;, children: [...], }, { value: 4545, name: &apos;description of this node&apos;, children: [ { value: 5656, name: &apos;description of this node&apos;, children: [...] }, ... ] } ] }, ...] 关于pyecharts中的树形图的文档地址:pyecharts Tree 其实和我们训练得到的树结构类似，只不过每个结点有个”name”属性，表示这个结点的名字，”value”表示它的值，”children”是一个list，里面还有这样的dict，我们可以写一个递归的函数完成这种数据的生成 12345678910111213141516171819202122def generate_echarts_data(tree): # 当前顶点的dict value = dict() # 如果传入的tree已经是叶子结点了 if tree['is_leaf'] == True: # 它的value就设置为预测的标记 value['value'] = tree['prediction'] # 它的名字就叫\"label: 标记\" value['name'] = 'label: %s'%(tree['prediction']) # 直接返回这个dict即可 return value # 如果传入的tree不是叶子结点，名字就叫当前这个顶点的划分特征，子树是一个list # 分别增加左子树和右子树到children中 value['name'] = tree['splitting_feature'] value['children'] = [generate_echarts_data(tree['left']), generate_echarts_data(tree['right'])] return value 1data = generate_echarts_data(my_decision_tree) 使用下面的代码进行绘制，绘制完成后，树的结点是可以点击的，点击后会展开它的子树 1234567891011tree = Tree(width=800, height=400)tree.add(\"\", [data], tree_collapse_interval=5, tree_top=\"15%\", tree_right=\"20%\", tree_symbol = 'rect', tree_symbol_size = 20, )tree.render()tree 第四题：预剪枝实验内容 实现使用信息增益率划分的预剪枝 我们会以信息增益率作为划分准则，构造带有预剪枝的二叉决策树使用的数据和第三题一样，剪枝需要使用验证集，所以数据划分策略会和第三题不同 1. 读取数据123# 导入类库import pandas as pdimport numpy as np 12# 导入数据loans = pd.read_csv('data/lendingclub/lending-club-data.csv', low_memory=False) 123# 对数据进行预处理，将safe_loans作为标记loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)del loans['bad_loans'] 1234567features = ['grade', # grade of the loan 'term', # the term of the loan 'home_ownership', # home_ownership status: own, mortgage or rent 'emp_length', # number of years of employment ]target = 'safe_loans'loans = loans[features + [target]] 2. 划分训练集和测试集1from sklearn.utils import shuffle 1loans = shuffle(loans, random_state = 34) 我们使用数据的60%做训练集，20%做验证集，20%做测试集 12345split_line1 = int(len(loans) * 0.6)split_line2 = int(len(loans) * 0.8)train_data = loans.iloc[: split_line1]validation_data = loans.iloc[split_line1: split_line2]test_data = loans.iloc[split_line2:] 3. 特征预处理12345678910111213141516171819202122def one_hot_encoding(data, features_categorical): ''' Parameter ---------- data: pd.DataFrame features_categorical: list(str) ''' # 对所有的离散特征遍历 for cat in features_categorical: # 对这列进行one-hot编码，前缀为这个变量名 one_encoding = pd.get_dummies(data[cat], prefix = cat) # 将生成的one-hot编码与之前的dataframe拼接起来 data = pd.concat([data, one_encoding],axis=1) # 删除掉原始的这列离散特征 del data[cat] return data 1train_data = one_hot_encoding(train_data, features) 12one_hot_features = train_data.columns.tolist()one_hot_features.remove(target) 1234567validation_tmp = one_hot_encoding(validation_data, features)validation_data = pd.DataFrame(columns = train_data.columns)for feature in train_data.columns: if feature in validation_tmp: validation_data[feature] = validation_tmp[feature].copy() else: validation_data[feature] = np.zeros(len(validation_tmp), dtype = 'uint8') 1234567test_data_tmp = one_hot_encoding(test_data, features)test_data = pd.DataFrame(columns = train_data.columns)for feature in train_data.columns: if feature in test_data_tmp.columns: test_data[feature] = test_data_tmp[feature].copy() else: test_data[feature] = np.zeros(test_data_tmp.shape[0], dtype = 'uint8') 打印一下3个数据集的shape 1print(train_data.shape, validation_data.shape, test_data.shape) (73564, 25) (24521, 25) (24522, 25) 4. 实现信息增益率的计算信息熵：$$\\mathrm{Ent}(D) = - \\sum^{\\vert \\mathcal{Y} \\vert}_{k = 1} p_k \\mathrm{log}_2 p_k$$ 信息增益：$$\\mathrm{Gain}(D, a) = \\mathrm{Ent}(D) - \\sum^{V}_{v=1} \\frac{\\vert D^v \\vert}{\\vert D \\vert} \\mathrm{Ent}(D^v)$$ 信息增益率： $$\\mathrm{Gain_ratio}(D, a) = \\frac{\\mathrm{Gain}(D, a)}{\\mathrm{IV}(a)}$$ 其中 $$\\mathrm{IV}(a) = - \\sum^V_{v=1} \\frac{\\vert D^v \\vert}{\\vert D \\vert} \\log_2 \\frac{\\vert D^v \\vert}{\\vert D \\vert}$$ 计算信息熵时约定：若$p = 0$，则$p \\log_2p = 0$ 1234567891011121314151617181920212223242526272829303132333435363738394041424344def information_entropy(labels_in_node): ''' 求当前结点的信息熵 Parameter ---------- labels_in_node: np.ndarray, 如[-1, 1, -1, 1, 1] Returns ---------- float: information entropy ''' # 统计样本总个数 num_of_samples = labels_in_node.shape[0] if num_of_samples == 0: return 0 # 统计出标记为1的个数 num_of_positive = len(labels_in_node[labels_in_node == 1]) # 统计出标记为-1的个数 # YOUR CODE HERE num_of_negative = len(labels_in_node[labels_in_node == -1]) # 统计正例的概率 prob_positive = num_of_positive / num_of_samples # 统计负例的概率 # YOUR CODE HERE prob_negative = num_of_negative / num_of_samples if prob_positive == 0: positive_part = 0 else: positive_part = prob_positive * np.log2(prob_positive) if prob_negative == 0: negative_part = 0 else: negative_part = prob_negative * np.log2(prob_negative) return - ( positive_part + negative_part ) 1234567891011121314151617181920212223# 信息熵测试样例1example_labels = np.array([-1, -1, 1, 1, 1])print(information_entropy(example_labels)) # 0.97095# 信息熵测试样例2example_labels = np.array([-1, -1, 1, 1, 1, 1, 1])print(information_entropy(example_labels)) # 0.86312 # 信息熵测试样例3example_labels = np.array([-1, -1, -1, -1, -1, 1, 1])print(information_entropy(example_labels)) # 0.86312# 信息熵测试样例4example_labels = np.array([-1] * 9 + [1] * 8)print(information_entropy(example_labels)) # 0.99750# 信息熵测试样例5example_labels = np.array([1] * 8)print(information_entropy(example_labels)) # 0# 信息熵测试样例6example_labels = np.array([])print(information_entropy(example_labels)) # 0 0.970950594455 0.863120568567 0.863120568567 0.997502546369 -0.0 0 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778def compute_information_gain_ratios(data, features, target, annotate = False): ''' 计算所有特征的信息增益率并保存起来 Parameter ---------- data: pd.DataFrame, 带有特征和标记的数据 features: list(str)，特征名组成的list target: str， 特征的名字 annotate: boolean, default False，是否打印注释 Returns ---------- gain_ratios: dict, key: str, 特征名 value: float，信息增益率 ''' gain_ratios = dict() # 对所有的特征进行遍历，使用当前的划分方法对每个特征进行计算 for feature in features: # 左子树保证所有的样本的这个特征取值为0 left_split_target = data[data[feature] == 0][target] # 右子树保证所有的样本的这个特征取值为1 right_split_target = data[data[feature] == 1][target] # 计算左子树的信息熵 left_entropy = information_entropy(left_split_target) # 计算左子树的权重 left_weight = len(left_split_target) / (len(left_split_target) + len(right_split_target)) # 计算右子树的信息熵 right_entropy = information_entropy(right_split_target) # 计算右子树的权重 right_weight = len(right_split_target) / (len(left_split_target) + len(right_split_target)) # 计算当前结点的信息熵 current_entropy = information_entropy(data[target]) # 计算当前结点的信息增益 # YOUR CODE HERE gain = current_entropy - (left_weight * left_entropy + right_weight * right_entropy) # 计算左子树的IV if left_weight == 0: left_IV = 0 else: # YOUR CODE HERE left_IV = left_weight * np.log2(left_weight) # 计算右子树的IV if right_weight == 0: right_IV = 0 else: # YOUR CODE HERE right_IV = right_weight * np.log2(right_weight) # IV 等于所有子树IV之和的相反数 IV = - (left_IV + right_IV) # 计算使用当前特征划分的信息增益率 # 这里为了防止IV是0，导致除法得到np.inf，在分母加了一个很小的小数 gain_ratio = gain / (IV + np.finfo(np.longdouble).eps) # 信息增益率的存储 gain_ratios[feature] = gain_ratio if annotate: print(\" \", feature, gain_ratio) return gain_ratios 12345678# 信息增益率测试样例1print(compute_information_gain_ratios(train_data, one_hot_features, target)['grade_A']) # 0.02573# 信息增益率测试样例2print(compute_information_gain_ratios(train_data, one_hot_features, target)['grade_B']) # 0.00418# 信息增益率测试样例3print(compute_information_gain_ratios(train_data, one_hot_features, target)['term_ 60 months']) # 0.01971 0.025734780668 0.00417549506943 0.0197093627186 5. 完成最优特征的选择这里我们没有实现信息增益和基尼指数的最优特征求解，感兴趣的同学可以按上一题实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344def best_splitting_feature(data, features, target, criterion = 'gain_ratios', annotate = False): ''' 给定划分方法和数据，找到最优的划分特征 Parameters ---------- data: pd.DataFrame, 带有特征和标记的数据 features: list(str)，特征名组成的list target: str， 特征的名字 criterion: str, 使用哪种指标，三种选项: 'information_gain', 'gain_ratio', 'gini' annotate: boolean, default False，是否打印注释 Returns ---------- best_feature: str, 最佳的划分特征的名字 ''' if criterion == 'information_gain': if annotate: print('using information gain') return None elif criterion == 'gain_ratio': if annotate: print('using information gain ratio') # 得到当前所有特征的信息增益率 gain_ratios = compute_information_gain_ratios(data, features, target, annotate) # 根据这些特征和他们的信息增益率，找到最佳的划分特征 best_feature = max(gain_ratios.items(), key = lambda x: x[1])[0] return best_feature elif criterion == 'gini': if annotate: print('using gini') return None else: raise Exception(\"传入的criterion不合规!\", criterion) 6. 判断结点内样本的类别是否为同一类1234567891011121314151617181920212223242526def intermediate_node_num_mistakes(labels_in_node): ''' 求树的结点中，样本数少的那个类的样本有多少，比如输入是[1, 1, -1, -1, 1]，返回2 Parameter ---------- labels_in_node: np.ndarray, pd.Series Returns ---------- int：个数 ''' # 如果传入的array为空，返回0 if len(labels_in_node) == 0: return 0 # 统计1的个数 # YOUR CODE HERE num_of_one = len(labels_in_node[labels_in_node == 1]) # 统计-1的个数 # YOUR CODE HERE num_of_minus_one = len(labels_in_node[labels_in_node == -1]) return num_of_one if num_of_minus_one &gt; num_of_one else num_of_minus_one 7. 创建叶子结点先编写一个辅助函数majority_class，求树的结点中，样本数多的那个类是什么 1234567891011121314151617def majority_class(labels_in_node): ''' 求树的结点中，样本数多的那个类是什么 ''' # 如果传入的array为空，返回0 if len(labels_in_node) == 0: return 0 # 统计1的个数 # YOUR CODE HERE num_of_one = len(labels_in_node[labels_in_node == 1]) # 统计-1的个数 # YOUR CODE HERE num_of_minus_one = len(labels_in_node[labels_in_node == -1]) return 1 if num_of_minus_one &lt; num_of_one else -1 1234567891011121314151617181920212223242526272829303132def create_leaf(target_values): ''' 计算出当前叶子结点的标记是什么，并且将叶子结点信息保存在一个dict中 Parameter: ---------- target_values: pd.Series, 当前叶子结点内样本的标记 Returns: ---------- leaf: dict，表示一个叶结点， leaf['splitting_features'], None，叶结点不需要划分特征 leaf['left'], None，叶结点没有左子树 leaf['right'], None，叶结点没有右子树 leaf['is_leaf'], True, 是否是叶子结点 leaf['prediction'], int, 表示该叶子结点的预测值 ''' # 创建叶子结点 leaf = {'splitting_feature' : None, 'left' : None, 'right' : None, 'is_leaf': True} # 数结点内-1和+1的个数 num_ones = len(target_values[target_values == +1]) num_minus_ones = len(target_values[target_values == -1]) # 叶子结点的标记使用少数服从多数的原则，为样本数多的那类的标记，保存在 leaf['prediction'] leaf['prediction'] = majority_class(target_values) # 返回叶子结点 return leaf 8. 递归地创建决策树递归的创建决策树决策树终止的三个条件： 如果结点内所有的样本的标记都相同，该结点就不需要再继续划分，直接做叶子结点即可 如果结点所有的特征都已经在之前使用过了，在当前结点无剩余特征可供划分样本，该结点直接做叶子结点 如果当前结点的深度已经达到了我们限制的树的最大深度，直接做叶子结点 对于预剪枝来说，实质上是增加了第四个终止条件： 如果当前结点划分后，模型的泛化能力没有提升，则不进行划分 如何判断泛化能力有没有提升？我们需要使用验证集就像使用训练集递归地划分数据一样，我们在递归地构造决策树时，也需要递归地将验证集进行划分，计算决策树在验证集上的精度 因为我们是递归地对决策树进行划分，所以每次计算验证集上精度是否提升时，也只是针对当前结点内的样本，因为是否对当前结点内的样本进行划分，不会影响它的兄弟结点及兄弟结点的子结点的精度 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139def decision_tree_create(training_data, validation_data, features, target, criterion = 'gain_ratios', pre_pruning = False, current_depth = 0, max_depth = 10, annotate = False): ''' Parameter: ---------- trianing_data: pd.DataFrame, 数据 features: iterable, 特征组成的可迭代对象 target: str, 标记的名字 criterion: 'str', 特征划分方法 current_depth: int, 当前深度 max_depth: int, 树的最大深度 Returns: ---------- dict, dict['is_leaf'] : False, 当前顶点不是叶子结点 dict['prediction'] : None, 不是叶子结点就没有预测值 dict['splitting_feature']: splitting_feature, 当前结点是使用哪个特征进行划分的 dict['left'] : dict dict['right'] : dict ''' if criterion not in ['information_gain', 'gain_ratio', 'gini']: raise Exception(\"传入的criterion不合规!\", criterion) # 复制一份特征，存储起来，每使用一个特征进行划分，我们就删除一个 remaining_features = features[:] # 取出标记值 target_values = training_data[target] validation_values = validation_data[target] print(\"-\" * 50) print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))) # 终止条件1 # 如果当前结点内所有样本同属一类，即这个结点中，各类别样本数最小的那个等于0 # 使用前面写的intermediate_node_num_mistakes来完成这个判断 # YOUR CODE HERE if intermediate_node_num_mistakes(target_values) == 0: print(\"Stopping condition 1 reached.\") return create_leaf(target_values) # 创建叶子结点 # 终止条件2 # 如果已经没有剩余的特征可供分割 # YOUR CODE HERE if len(remaining_features) == 0: print(\"Stopping condition 2 reached.\") return create_leaf(target_values) # 创建叶子结点 # 终止条件3 # 如果已经到达了我们要求的最大深度 # YOUR CODE HERE if current_depth &gt;= max_depth: print(\"Reached maximum depth. Stopping for now.\") return create_leaf(target_values) # 创建叶子结点 # 找到最优划分特征 # 使用best_splitting_feature这个函数 # YOUR CODE HERE splitting_feature = best_splitting_feature(training_data, features, target, criterion, annotate) # 使用我们找到的最优特征将数据划分成两份 # 左子树的数据 left_split = training_data[training_data[splitting_feature] == 0] # 右子树的数据 # YOUR CODE HERE right_split = training_data[training_data[splitting_feature] == 1] # 使用这个最优特征对验证集进行划分 validation_left_split = validation_data[validation_data[splitting_feature] == 0] validation_right_split = validation_data[validation_data[splitting_feature] == 1] # 如果使用预剪枝，需要判断在验证集上的精度是否提升了 if pre_pruning: # 首先计算不划分的时候的在验证集上的精度，也就是当前结点为叶子结点 # 统计当前结点样本中，样本数多的那个类(majority class) true_class = majority_class(target_values) # 判断验证集在不划分时的精度，分母加eps是因为，有可能在划分的时候，验证集的样本数为0 acc_without_splitting = len(validation_values[validation_values == true_class]) / (len(validation_values) + np.finfo(np.longdouble).eps) # 对当前结点进行划分，统计划分后，左子树的majority class left_true_class = majority_class(left_split[target]) # 对当前结点进行划分，统计右子树的majority class right_true_class = majority_class(right_split[target]) # 统计验证集左子树中有多少样本是左子树的majority class vali_left_num_of_majority = len(validation_left_split[validation_left_split[target] == left_true_class]) # 统计验证集右子树中有多少样本是右子树的majority class vali_right_num_of_majority = len(validation_right_split[validation_right_split[target] == right_true_class]) # 计算划分后的精度 acc_with_splitting = (vali_left_num_of_majority + vali_right_num_of_majority) / (len(validation_data) + np.finfo(np.longdouble).eps) if annotate == True: print('acc before splitting: %.3f'%(acc_without_splitting)) print('acc after splitting: %.3f'%(acc_with_splitting)) # 如果划分后的精度小于等于划分前的精度，那就不划分，当前结点直接变成叶子结点 # 否则继续划分，创建左右子树 if acc_with_splitting &lt; acc_without_splitting: print('Pre-Pruning') return create_leaf(target_values) # 创建叶子结点 # 从剩余特征中删除掉当前这个特征 remaining_features.remove(splitting_feature) print(\"Split on feature %s. (%s, %s)\" % (\\ splitting_feature, len(left_split), len(right_split))) # 如果使用当前的特征，将所有的样本都划分到一棵子树中，那么就直接将这棵子树变成叶子结点 # 判断左子树是不是“完美”的 if len(left_split) == len(training_data): print(\"Creating leaf node.\") return create_leaf(left_split[target]) # 判断右子树是不是“完美”的 if len(right_split) == len(training_data): print(\"Creating right node.\") # YOUR CODE HERE return create_leaf(right_split[target]) # 递归地创建左子树，需要传入验证集的左子树 left_tree = decision_tree_create(left_split, validation_left_split, remaining_features, target, criterion, pre_pruning, current_depth + 1, max_depth, annotate) # 递归地创建右子树，需要传入验证集的右子树 ## YOUR CODE HERE right_tree = decision_tree_create(right_split, validation_right_split, remaining_features, target, criterion, pre_pruning, current_depth + 1, max_depth, annotate) return {'is_leaf' : False, 'prediction' : None, 'splitting_feature': splitting_feature, 'left' : left_tree, 'right' : right_tree} 1tree_without_pre_pruning = decision_tree_create(train_data, validation_data, one_hot_features, target, criterion = 'gain_ratio', pre_pruning = False, current_depth = 0, max_depth = 6, annotate = False) -------------------------------------------------- Subtree, depth = 0 (73564 data points). Split on feature grade_F. (71229, 2335) -------------------------------------------------- Subtree, depth = 1 (71229 data points). Split on feature grade_A. (57869, 13360) -------------------------------------------------- Subtree, depth = 2 (57869 data points). Split on feature grade_G. (57232, 637) -------------------------------------------------- Subtree, depth = 3 (57232 data points). Split on feature grade_E. (51828, 5404) -------------------------------------------------- Subtree, depth = 4 (51828 data points). Split on feature grade_D. (40326, 11502) -------------------------------------------------- Subtree, depth = 5 (40326 data points). Split on feature term_ 36 months. (5760, 34566) -------------------------------------------------- Subtree, depth = 6 (5760 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (34566 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (11502 data points). Split on feature term_ 36 months. (3315, 8187) -------------------------------------------------- Subtree, depth = 6 (3315 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (8187 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 4 (5404 data points). Split on feature term_ 36 months. (3185, 2219) -------------------------------------------------- Subtree, depth = 5 (3185 data points). Split on feature home_ownership_OTHER. (3184, 1) -------------------------------------------------- Subtree, depth = 6 (3184 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (1 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 5 (2219 data points). Split on feature emp_length_1 year. (2011, 208) -------------------------------------------------- Subtree, depth = 6 (2011 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (208 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 3 (637 data points). Split on feature emp_length_3 years. (590, 47) -------------------------------------------------- Subtree, depth = 4 (590 data points). Split on feature emp_length_2 years. (541, 49) -------------------------------------------------- Subtree, depth = 5 (541 data points). Split on feature home_ownership_OWN. (495, 46) -------------------------------------------------- Subtree, depth = 6 (495 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (46 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (49 data points). Split on feature term_ 36 months. (32, 17) -------------------------------------------------- Subtree, depth = 6 (32 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (17 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 4 (47 data points). Split on feature home_ownership_OTHER. (46, 1) -------------------------------------------------- Subtree, depth = 5 (46 data points). Split on feature home_ownership_OWN. (44, 2) -------------------------------------------------- Subtree, depth = 6 (44 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (2 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (1 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 2 (13360 data points). Split on feature term_ 36 months. (259, 13101) -------------------------------------------------- Subtree, depth = 3 (259 data points). Split on feature emp_length_9 years. (252, 7) -------------------------------------------------- Subtree, depth = 4 (252 data points). Split on feature home_ownership_RENT. (202, 50) -------------------------------------------------- Subtree, depth = 5 (202 data points). Split on feature emp_length_8 years. (192, 10) -------------------------------------------------- Subtree, depth = 6 (192 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (10 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 5 (50 data points). Split on feature emp_length_4 years. (48, 2) -------------------------------------------------- Subtree, depth = 6 (48 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (2 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 4 (7 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 3 (13101 data points). Split on feature home_ownership_MORTGAGE. (5830, 7271) -------------------------------------------------- Subtree, depth = 4 (5830 data points). Split on feature emp_length_7 years. (5592, 238) -------------------------------------------------- Subtree, depth = 5 (5592 data points). Split on feature emp_length_3 years. (5045, 547) -------------------------------------------------- Subtree, depth = 6 (5045 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (547 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (238 data points). Split on feature home_ownership_OWN. (184, 54) -------------------------------------------------- Subtree, depth = 6 (184 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (54 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 4 (7271 data points). Split on feature emp_length_2 years. (6702, 569) -------------------------------------------------- Subtree, depth = 5 (6702 data points). Split on feature emp_length_4 years. (6234, 468) -------------------------------------------------- Subtree, depth = 6 (6234 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (468 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (569 data points). Split on feature grade_B. (569, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 1 (2335 data points). Split on feature emp_length_7 years. (2197, 138) -------------------------------------------------- Subtree, depth = 2 (2197 data points). Split on feature term_ 36 months. (1719, 478) -------------------------------------------------- Subtree, depth = 3 (1719 data points). Split on feature home_ownership_OTHER. (1717, 2) -------------------------------------------------- Subtree, depth = 4 (1717 data points). Split on feature emp_length_3 years. (1577, 140) -------------------------------------------------- Subtree, depth = 5 (1577 data points). Split on feature home_ownership_RENT. (904, 673) -------------------------------------------------- Subtree, depth = 6 (904 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (673 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (140 data points). Split on feature home_ownership_RENT. (73, 67) -------------------------------------------------- Subtree, depth = 6 (73 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (67 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 4 (2 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 3 (478 data points). Split on feature emp_length_8 years. (460, 18) -------------------------------------------------- Subtree, depth = 4 (460 data points). Split on feature emp_length_4 years. (433, 27) -------------------------------------------------- Subtree, depth = 5 (433 data points). Split on feature home_ownership_MORTGAGE. (287, 146) -------------------------------------------------- Subtree, depth = 6 (287 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (146 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (27 data points). Split on feature home_ownership_OWN. (25, 2) -------------------------------------------------- Subtree, depth = 6 (25 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (2 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 4 (18 data points). Split on feature home_ownership_OWN. (17, 1) -------------------------------------------------- Subtree, depth = 5 (17 data points). Split on feature home_ownership_MORTGAGE. (11, 6) -------------------------------------------------- Subtree, depth = 6 (11 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (6 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (1 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 2 (138 data points). Split on feature term_ 36 months. (109, 29) -------------------------------------------------- Subtree, depth = 3 (109 data points). Split on feature home_ownership_RENT. (51, 58) -------------------------------------------------- Subtree, depth = 4 (51 data points). Split on feature home_ownership_MORTGAGE. (8, 43) -------------------------------------------------- Subtree, depth = 5 (8 data points). Split on feature grade_A. (8, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 5 (43 data points). Split on feature grade_A. (43, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 4 (58 data points). Split on feature grade_A. (58, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 3 (29 data points). Split on feature home_ownership_OWN. (25, 4) -------------------------------------------------- Subtree, depth = 4 (25 data points). Split on feature home_ownership_MORTGAGE. (13, 12) -------------------------------------------------- Subtree, depth = 5 (13 data points). Split on feature grade_A. (13, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 5 (12 data points). Split on feature grade_A. (12, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 4 (4 data points). Split on feature grade_A. (4, 0) Creating leaf node. 1tree_with_pre_pruning = decision_tree_create(train_data, validation_data, one_hot_features, target, criterion = \"gain_ratio\", pre_pruning = True, current_depth = 0, max_depth = 6, annotate = False) -------------------------------------------------- Subtree, depth = 0 (73564 data points). Split on feature grade_F. (71229, 2335) -------------------------------------------------- Subtree, depth = 1 (71229 data points). Split on feature grade_A. (57869, 13360) -------------------------------------------------- Subtree, depth = 2 (57869 data points). Split on feature grade_G. (57232, 637) -------------------------------------------------- Subtree, depth = 3 (57232 data points). Split on feature grade_E. (51828, 5404) -------------------------------------------------- Subtree, depth = 4 (51828 data points). Split on feature grade_D. (40326, 11502) -------------------------------------------------- Subtree, depth = 5 (40326 data points). Split on feature term_ 36 months. (5760, 34566) -------------------------------------------------- Subtree, depth = 6 (5760 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (34566 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (11502 data points). Split on feature term_ 36 months. (3315, 8187) -------------------------------------------------- Subtree, depth = 6 (3315 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (8187 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 4 (5404 data points). Split on feature term_ 36 months. (3185, 2219) -------------------------------------------------- Subtree, depth = 5 (3185 data points). Split on feature home_ownership_OTHER. (3184, 1) -------------------------------------------------- Subtree, depth = 6 (3184 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (1 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 5 (2219 data points). Split on feature emp_length_1 year. (2011, 208) -------------------------------------------------- Subtree, depth = 6 (2011 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (208 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 3 (637 data points). Split on feature emp_length_3 years. (590, 47) -------------------------------------------------- Subtree, depth = 4 (590 data points). Split on feature emp_length_2 years. (541, 49) -------------------------------------------------- Subtree, depth = 5 (541 data points). Pre-Pruning -------------------------------------------------- Subtree, depth = 5 (49 data points). Split on feature term_ 36 months. (32, 17) -------------------------------------------------- Subtree, depth = 6 (32 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (17 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 4 (47 data points). Split on feature home_ownership_OTHER. (46, 1) -------------------------------------------------- Subtree, depth = 5 (46 data points). Split on feature home_ownership_OWN. (44, 2) -------------------------------------------------- Subtree, depth = 6 (44 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (2 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (1 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 2 (13360 data points). Split on feature term_ 36 months. (259, 13101) -------------------------------------------------- Subtree, depth = 3 (259 data points). Split on feature emp_length_9 years. (252, 7) -------------------------------------------------- Subtree, depth = 4 (252 data points). Split on feature home_ownership_RENT. (202, 50) -------------------------------------------------- Subtree, depth = 5 (202 data points). Split on feature emp_length_8 years. (192, 10) -------------------------------------------------- Subtree, depth = 6 (192 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (10 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 5 (50 data points). Split on feature emp_length_4 years. (48, 2) -------------------------------------------------- Subtree, depth = 6 (48 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (2 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 4 (7 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 3 (13101 data points). Split on feature home_ownership_MORTGAGE. (5830, 7271) -------------------------------------------------- Subtree, depth = 4 (5830 data points). Split on feature emp_length_7 years. (5592, 238) -------------------------------------------------- Subtree, depth = 5 (5592 data points). Split on feature emp_length_3 years. (5045, 547) -------------------------------------------------- Subtree, depth = 6 (5045 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (547 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (238 data points). Split on feature home_ownership_OWN. (184, 54) -------------------------------------------------- Subtree, depth = 6 (184 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (54 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 4 (7271 data points). Split on feature emp_length_2 years. (6702, 569) -------------------------------------------------- Subtree, depth = 5 (6702 data points). Split on feature emp_length_4 years. (6234, 468) -------------------------------------------------- Subtree, depth = 6 (6234 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (468 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (569 data points). Split on feature grade_B. (569, 0) Creating leaf node. -------------------------------------------------- Subtree, depth = 1 (2335 data points). Split on feature emp_length_7 years. (2197, 138) -------------------------------------------------- Subtree, depth = 2 (2197 data points). Split on feature term_ 36 months. (1719, 478) -------------------------------------------------- Subtree, depth = 3 (1719 data points). Split on feature home_ownership_OTHER. (1717, 2) -------------------------------------------------- Subtree, depth = 4 (1717 data points). Split on feature emp_length_3 years. (1577, 140) -------------------------------------------------- Subtree, depth = 5 (1577 data points). Split on feature home_ownership_RENT. (904, 673) -------------------------------------------------- Subtree, depth = 6 (904 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (673 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (140 data points). Split on feature home_ownership_RENT. (73, 67) -------------------------------------------------- Subtree, depth = 6 (73 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (67 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 4 (2 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 3 (478 data points). Split on feature emp_length_8 years. (460, 18) -------------------------------------------------- Subtree, depth = 4 (460 data points). Pre-Pruning -------------------------------------------------- Subtree, depth = 4 (18 data points). Split on feature home_ownership_OWN. (17, 1) -------------------------------------------------- Subtree, depth = 5 (17 data points). Split on feature home_ownership_MORTGAGE. (11, 6) -------------------------------------------------- Subtree, depth = 6 (11 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 6 (6 data points). Reached maximum depth. Stopping for now. -------------------------------------------------- Subtree, depth = 5 (1 data points). Stopping condition 1 reached. -------------------------------------------------- Subtree, depth = 2 (138 data points). Pre-Pruning 123456789101112131415161718192021222324252627282930def classify(tree, x, annotate = False): ''' 递归的进行预测，一次只能预测一个样本 Parameters ---------- tree: dict x: pd.Series，样本 x: pd.DataFrame, 待预测的样本 annotate, boolean, 是否显示注释 Returns ---------- 返回预测的标记 ''' if tree['is_leaf']: if annotate: print (\"At leaf, predicting %s\" % tree['prediction']) return tree['prediction'] else: split_feature_value = x[tree['splitting_feature']] if annotate: print (\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value)) if split_feature_value == 0: return classify(tree['left'], x, annotate) else: return classify(tree['right'], x, annotate) 123456789101112131415161718192021def predict(tree, data): ''' 按行遍历data，对每个样本进行预测，将值存储起来，最后返回np.ndarray Parameter ---------- tree, dict, 模型 data, pd.DataFrame, 数据 Returns ---------- predictions, np.ndarray, 模型对这些样本的预测结果 ''' predictions = np.zeros(len(data)) # YOUR CODE HERE for i in range(len(data)): predictions[i] = classify(tree, data.iloc[i]) return predictions 1from sklearn.metrics import accuracy_score 不预剪枝的决策树在测试集上的精度 1accuracy_score(predict(tree_without_pre_pruning, test_data), test_data[target]) 0.80882472881494172 预剪枝的决策树在测试集上的精度 1accuracy_score(predict(tree_with_pre_pruning, test_data), test_data[target]) 0.80939564472718373 test 在下方计算出带有预剪枝和不带预剪枝的决策树的精度，查准率，查全率和F1值(最大深度为6，使用信息增益率)，保留4位小数，四舍五入12345678910# YOUR CODE HEREfrom sklearn.metrics import precision_scorefrom sklearn.metrics import recall_scorefrom sklearn.metrics import f1_scoreprint(precision_score(predict(tree_without_pre_pruning, test_data), test_data[target]))print(precision_score(predict(tree_with_pre_pruning, test_data), test_data[target]))print(recall_score(predict(tree_without_pre_pruning, test_data), test_data[target]))print(recall_score(predict(tree_with_pre_pruning, test_data), test_data[target]))print(f1_score(predict(tree_without_pre_pruning, test_data), test_data[target]))print(f1_score(predict(tree_with_pre_pruning, test_data), test_data[target])) 0.998438365825 0.999848874112 0.809739755689 0.809494677597 0.894242916441 0.894658553076 双击此处编辑 模型 精度 查准率 查全率 F1 有预剪枝 0.8094 0.9998 0.8095 0.8947 无预剪枝 0.8088 0.9984 0.8097 0.8942 1from pyecharts import Tree 12345678910111213141516171819202122def generate_echarts_data(tree): # 当前顶点的dict value = dict() # 如果传入的tree已经是叶子结点了 if tree['is_leaf'] == True: # 它的value就设置为预测的标记 value['value'] = tree['prediction'] # 它的名字就叫\"label: 标记\" value['name'] = 'label: %s'%(tree['prediction']) # 直接返回这个dict即可 return value # 如果传入的tree不是叶子结点，名字就叫当前这个顶点的划分特征，子树是一个list # 分别增加左子树和右子树到children中 value['name'] = tree['splitting_feature'] value['children'] = [generate_echarts_data(tree['left']), generate_echarts_data(tree['right'])] return value 1data1 = generate_echarts_data(tree_without_pre_pruning) 1234567891011tree = Tree(width=800, height=800)tree.add(\"\", [data1], tree_collapse_interval=5, tree_top=\"15%\", tree_right=\"20%\", tree_symbol = 'rect', tree_symbol_size = 20, )tree.render()tree 1data2 = generate_echarts_data(tree_with_pre_pruning) 1234567891011tree = Tree(width=800, height=800)tree.add(\"\", [data2], tree_collapse_interval=5, tree_top=\"15%\", tree_right=\"20%\", tree_symbol = 'rect', tree_symbol_size = 20, )tree.render()tree","link":"/blog/2018/09/13/决策树实现/"}],"tags":[{"name":"deep learning","slug":"deep-learning","link":"/blog/tags/deep-learning/"},{"name":"Graph","slug":"graph","link":"/blog/tags/graph/"},{"name":"graph convolutional network","slug":"graph-convolutional-network","link":"/blog/tags/graph-convolutional-network/"},{"name":"machine learning","slug":"machine-learning","link":"/blog/tags/machine-learning/"},{"name":"recommender system","slug":"recommender-system","link":"/blog/tags/recommender-system/"},{"name":"implicit feedback","slug":"implicit-feedback","link":"/blog/tags/implicit-feedback/"},{"name":"seq2seq","slug":"seq2seq","link":"/blog/tags/seq2seq/"},{"name":"machine translation","slug":"machine-translation","link":"/blog/tags/machine-translation/"},{"name":"Spatial-temporal","slug":"spatial-temporal","link":"/blog/tags/spatial-temporal/"},{"name":"ResNet","slug":"resnet","link":"/blog/tags/resnet/"},{"name":"Time Series","slug":"time-series","link":"/blog/tags/time-series/"},{"name":"graph","slug":"graph","link":"/blog/tags/graph/"},{"name":"已复现","slug":"已复现","link":"/blog/tags/已复现/"},{"name":"dataset","slug":"dataset","link":"/blog/tags/dataset/"},{"name":"reinforcement learning","slug":"reinforcement-learning","link":"/blog/tags/reinforcement-learning/"},{"name":"software","slug":"software","link":"/blog/tags/software/"},{"name":"Attention","slug":"attention","link":"/blog/tags/attention/"},{"name":"large-scale learning","slug":"large-scale-learning","link":"/blog/tags/large-scale-learning/"},{"name":"Hadoop","slug":"hadoop","link":"/blog/tags/hadoop/"},{"name":"computer vision","slug":"computer-vision","link":"/blog/tags/computer-vision/"},{"name":"image style transfer","slug":"image-style-transfer","link":"/blog/tags/image-style-transfer/"},{"name":"Kafka","slug":"kafka","link":"/blog/tags/kafka/"},{"name":"super resolution","slug":"super-resolution","link":"/blog/tags/super-resolution/"},{"name":"algorithms","slug":"algorithms","link":"/blog/tags/algorithms/"},{"name":"natural language processing","slug":"natural-language-processing","link":"/blog/tags/natural-language-processing/"},{"name":"NER","slug":"ner","link":"/blog/tags/ner/"},{"name":"Spark","slug":"spark","link":"/blog/tags/spark/"},{"name":"Sequence","slug":"sequence","link":"/blog/tags/sequence/"},{"name":"vscode","slug":"vscode","link":"/blog/tags/vscode/"},{"name":"normalization","slug":"normalization","link":"/blog/tags/normalization/"},{"name":"随笔","slug":"随笔","link":"/blog/tags/随笔/"},{"name":"virtual machine","slug":"virtual-machine","link":"/blog/tags/virtual-machine/"},{"name":"language modeling","slug":"language-modeling","link":"/blog/tags/language-modeling/"}],"categories":[{"name":"论文阅读笔记","slug":"论文阅读笔记","link":"/blog/categories/论文阅读笔记/"},{"name":"机器学习","slug":"机器学习","link":"/blog/categories/机器学习/"},{"name":"dataset","slug":"dataset","link":"/blog/categories/dataset/"},{"name":"software","slug":"software","link":"/blog/categories/software/"},{"name":"分布式平台","slug":"分布式平台","link":"/blog/categories/分布式平台/"},{"name":"随笔","slug":"随笔","link":"/blog/categories/随笔/"},{"name":"algorithms","slug":"algorithms","link":"/blog/categories/algorithms/"},{"name":"自然语言处理","slug":"自然语言处理","link":"/blog/categories/自然语言处理/"}]}